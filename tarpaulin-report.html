<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <style>:root {
  --color: black;
  --bg: white;
  --head-bg: white;
  --link: #338;

  --blue: #ccf;
  --red: #fcc;
  --yellow: #ffc;
  --green: #cfc;
}

[data-theme='dark'] {
  --color: white;
  --bg: black;
  --head-bg: #333;
  --link: #aaf;

  --blue: #225;
  --red: #522;
  --yellow: #552;
  --green: #252;
}

html,
body {
  margin: 0;
  padding: 0;
  color: var(--color);
  background: var(--bg);
}

.app {
  margin: 10px;
  padding: 0;
}

.files-list {
  margin: 10px 0 0;
  width: 100%;
  border-collapse: collapse;
}
.files-list__head {
  border: 1px solid #999;
}
.files-list__head > tr > th {
  padding: 10px;
  border: 1px solid #999;
  text-align: left;
  font-weight: normal;
  background: var(--head-bg);
}
.files-list__body {
}
.files-list__file {
  cursor: pointer;
}
.files-list__file:hover {
  background: var(--blue);
}
.files-list__file > td {
  padding: 10px;
  border: 1px solid #999;
}
.files-list__file > td:first-child::before {
  content: '\01F4C4';
  margin-right: 1em;
}
.files-list__file_low {
  background: var(--red);
}
.files-list__file_medium {
  background: var(--yellow);
}
.files-list__file_high {
  background: var(--green);
}
.files-list__file_folder > td:first-child::before {
  content: '\01F4C1';
  margin-right: 1em;
}

.file-header {
  border: 1px solid #999;
  display: flex;
  justify-content: space-between;
  align-items: center;
  position: sticky;
  top: 0;
  background: var(--bg);
}

.file-header__back {
  margin: 10px;
  cursor: pointer;
  flex-shrink: 0;
  flex-grow: 0;
  text-decoration: underline;
  color: var(--link);
}

.file-header__name {
  margin: 10px;
  flex-shrink: 2;
  flex-grow: 2;
}

.file-header__stat {
  margin: 10px;
  flex-shrink: 0;
  flex-grow: 0;
}

.file-content {
  margin: 10px 0 0;
  border: 1px solid #999;
  padding: 10px;
  counter-reset: line;
  display: flex;
  flex-direction: column;
}

.code-line::before {
  content: counter(line);
  margin-right: 10px;
}
.code-line {
  margin: 0;
  padding: 0.3em;
  height: 1em;
  counter-increment: line;
}
.code-line_covered {
  background: var(--green);
}
.code-line_uncovered {
  background: var(--red);
}

#theme-toggle-label {
  margin-left: 1ch;
}
</style>
</head>
<body>
    <div id="root"></div>
    <script>
        var data = {"files":[{"path":["/","Users","blitz","my-oss","xla-rs","kernels","benches","ops_bench.rs"],"content":"use criterion::{Criterion, black_box, criterion_group, criterion_main};\nuse xla_rs_kernels::{cpu_matmul, cpu_transpose};\n\nfn benchmark_matmul(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"matmul\");\n    let sizes = [64, 128, 256, 512];\n\n    for \u0026size in \u0026sizes {\n        let m = size;\n        let k = size;\n        let n = size;\n        let lhs_shape = [m, k];\n        let rhs_shape = [k, n];\n        let lhs_data = vec![1.0f32; m * k];\n        let rhs_data = vec![1.0f32; k * n];\n\n        group.bench_function(format!(\"{}x{}\", size, size), |b| {\n            b.iter(|| {\n                cpu_matmul(\n                    black_box(\u0026lhs_data),\n                    black_box(\u0026rhs_data),\n                    black_box(\u0026lhs_shape),\n                    black_box(\u0026rhs_shape),\n                )\n                .unwrap()\n            })\n        });\n    }\n    group.finish();\n}\n\nfn benchmark_transpose(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"transpose\");\n    let sizes = [128, 512, 1024, 2048];\n\n    for \u0026size in \u0026sizes {\n        let m = size;\n        let n = size;\n        let shape = [m, n];\n        let data = vec![1.0f32; m * n];\n\n        group.bench_function(format!(\"{}x{}\", size, size), |b| {\n            b.iter(|| cpu_transpose(black_box(\u0026data), black_box(\u0026shape)).unwrap())\n        });\n    }\n    group.finish();\n}\n\ncriterion_group!(benches, benchmark_matmul, benchmark_transpose);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","cpu_matmul.rs"],"content":"use crate::{KernelElem, Result};\nuse rayon::prelude::*;\n\n/// CPU Implementation of Matrix Multiplication.\n///\n/// This function is the \"kernel\" that performs the actual computation.\n/// It is separated from the `Tensor` struct to allow for easy swapping with\n/// optimized libraries (like BLAS) in the future.\n///\n/// # SOTA Integration Guide\n///\n/// To integrate a SOTA library like `cblas` or `matrixmultiply`:\n/// 1. Replace the body of this function with a call to the library's `sgemm` or `dgemm`.\n/// 2. Ensure the memory layout matches (Row-Major vs Column-Major).\n///    - `xla-rs` uses Row-Major.\n///    - BLAS typically defaults to Column-Major but supports Row-Major via flags.\n/// 3. Handle batching:\n///    - If the library supports batched matmul, pass the batch count.\n///    - Otherwise, loop over the batch dimension here (parallelized with `rayon`).\npub fn cpu_matmul\u003cT, const RANK: usize\u003e(\n    lhs_data: \u0026[T],\n    rhs_data: \u0026[T],\n    lhs_shape: \u0026[usize; RANK],\n    rhs_shape: \u0026[usize; RANK],\n) -\u003e Result\u003cVec\u003cT\u003e\u003e\nwhere\n    T: KernelElem,\n{\n    let m = lhs_shape[RANK - 2];\n    let k = lhs_shape[RANK - 1];\n    let n = rhs_shape[RANK - 1];\n\n    if k != rhs_shape[RANK - 2] {\n        return Err(crate::KernelError::ShapeMismatch {\n            expected: vec![k],\n            got: vec![rhs_shape[RANK - 2]],\n        });\n    }\n\n    let mut out_shape = *lhs_shape;\n    out_shape[RANK - 2] = m;\n    out_shape[RANK - 1] = n;\n    let size: usize = out_shape.iter().product();\n    let mut out_data = vec![T::zero(); size];\n\n    // Optimization: Transpose rhs to allow sequential access (cache friendly)\n    // We need to transpose the RHS data. Since we don't have a Tensor object here,\n    // we call the kernel directly.\n    // rhs is [..., K, N], we want [..., N, K]\n    let rhs_t_data = super::cpu_transpose::cpu_transpose(rhs_data, rhs_shape)?;\n\n    // Parallelize over rows of the output matrices across all batches\n    // Output shape is [Batch..., M, N]\n    // We iterate over (Batch... * M) rows, each of size N\n\n    out_data\n        .as_mut_slice()\n        .par_chunks_mut(n)\n        .enumerate()\n        .for_each(|(global_row_idx, out_row)| {\n            let batch_idx = global_row_idx / m;\n            let row_in_matrix = global_row_idx % m;\n\n            // Calculate offsets for input tensors\n            let a_batch_offset = batch_idx * m * k;\n            // rhs_t has shape [..., N, K], so batch offset is batch_idx * N * K\n            let b_t_batch_offset = batch_idx * n * k;\n\n            let a_row_start = a_batch_offset + row_in_matrix * k;\n            let a_slice = \u0026lhs_data[a_row_start..a_row_start + k];\n\n            for (col_in_matrix, out_elem) in out_row.iter_mut().enumerate() {\n                // We want dot product of:\n                // A row: `row_in_matrix`\n                // B col: `col_in_matrix` -\u003e which is rhs_t row `col_in_matrix`\n\n                let b_t_row_start = b_t_batch_offset + col_in_matrix * k;\n                let b_t_slice = \u0026rhs_t_data[b_t_row_start..b_t_row_start + k];\n\n                let mut sum = T::zero();\n                // Vectorizable loop\n                for (\u0026val_a, \u0026val_b) in a_slice.iter().zip(b_t_slice.iter()) {\n                    sum += val_a * val_b;\n                }\n                *out_elem = sum;\n            }\n        });\n\n    Ok(out_data)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::KernelError;\n\n    #[test]\n    fn test_matmul_simple() {\n        let a = vec![1.0, 2.0, 3.0, 4.0]; // 2x2\n        let b = vec![5.0, 6.0, 7.0, 8.0]; // 2x2\n        let shape_a = [2, 2];\n        let shape_b = [2, 2];\n\n        let result = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b).unwrap();\n        // Expected:\n        // [1*5+2*7, 1*6+2*8] = [19, 22]\n        // [3*5+4*7, 3*6+4*8] = [43, 50]\n        assert_eq!(result, vec![19.0, 22.0, 43.0, 50.0]);\n    }\n\n    #[test]\n    fn test_matmul_batch() {\n        // Batch size 2, 2x2 matrices\n        let a = vec![\n            1.0, 0.0, 0.0, 1.0, // Identity\n            2.0, 0.0, 0.0, 2.0, // Scaled Identity\n        ];\n        let b = vec![\n            1.0, 2.0, 3.0, 4.0, // Matrix B1\n            5.0, 6.0, 7.0, 8.0, // Matrix B2\n        ];\n        let shape_a = [2, 2, 2];\n        let shape_b = [2, 2, 2];\n\n        let result = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b).unwrap();\n        // Expected:\n        // Batch 1: I * B1 = B1\n        // Batch 2: 2I * B2 = 2 * B2\n        let expected = vec![1.0, 2.0, 3.0, 4.0, 10.0, 12.0, 14.0, 16.0];\n        assert_eq!(result, expected);\n    }\n\n    #[test]\n    fn test_matmul_shape_mismatch() {\n        let a = vec![1.0; 4]; // 2x2\n        let b = vec![1.0; 6]; // 3x2\n        let shape_a = [2, 2];\n        let shape_b = [3, 2]; // Inner dim mismatch: 2 != 3\n\n        let err = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b);\n        assert!(matches!(err, Err(KernelError::ShapeMismatch { .. })));\n    }\n}\n","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":120109}},{"line":29,"address":[],"length":0,"stats":{"Line":240218}},{"line":30,"address":[],"length":0,"stats":{"Line":240218}},{"line":31,"address":[],"length":0,"stats":{"Line":240218}},{"line":33,"address":[],"length":0,"stats":{"Line":120109}},{"line":34,"address":[],"length":0,"stats":{"Line":3}},{"line":35,"address":[],"length":0,"stats":{"Line":9}},{"line":36,"address":[],"length":0,"stats":{"Line":3}},{"line":40,"address":[],"length":0,"stats":{"Line":240212}},{"line":41,"address":[],"length":0,"stats":{"Line":120106}},{"line":42,"address":[],"length":0,"stats":{"Line":120106}},{"line":43,"address":[],"length":0,"stats":{"Line":600530}},{"line":44,"address":[],"length":0,"stats":{"Line":480424}},{"line":50,"address":[],"length":0,"stats":{"Line":480424}},{"line":56,"address":[],"length":0,"stats":{"Line":120106}},{"line":58,"address":[],"length":0,"stats":{"Line":240212}},{"line":60,"address":[],"length":0,"stats":{"Line":320386}},{"line":61,"address":[],"length":0,"stats":{"Line":400560}},{"line":62,"address":[],"length":0,"stats":{"Line":400560}},{"line":65,"address":[],"length":0,"stats":{"Line":400560}},{"line":67,"address":[],"length":0,"stats":{"Line":400560}},{"line":69,"address":[],"length":0,"stats":{"Line":600840}},{"line":70,"address":[],"length":0,"stats":{"Line":801120}},{"line":72,"address":[],"length":0,"stats":{"Line":1529034}},{"line":77,"address":[],"length":0,"stats":{"Line":1392291}},{"line":78,"address":[],"length":0,"stats":{"Line":1856388}},{"line":80,"address":[],"length":0,"stats":{"Line":928194}},{"line":82,"address":[],"length":0,"stats":{"Line":5159469}},{"line":83,"address":[],"length":0,"stats":{"Line":1892656}},{"line":85,"address":[],"length":0,"stats":{"Line":464097}},{"line":89,"address":[],"length":0,"stats":{"Line":120106}}],"covered":31,"coverable":31},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","cpu_transpose.rs"],"content":"use crate::{KernelElem, Result};\nuse rayon::prelude::*;\n\n/// CPU Implementation of Transpose.\n///\n/// Swaps the last two dimensions of the input data.\n///\n/// # SOTA Integration Guide\n///\n/// Optimized transpose operations often use tiling (blocking) to improve cache usage.\n/// Libraries like `hptt` (High Performance Tensor Transpose) can be used here.\npub fn cpu_transpose\u003cT, const RANK: usize\u003e(data: \u0026[T], shape: \u0026[usize; RANK]) -\u003e Result\u003cVec\u003cT\u003e\u003e\nwhere\n    T: KernelElem,\n{\n    let m = shape[RANK - 2];\n    let n = shape[RANK - 1];\n\n    let mut new_shape = *shape;\n    new_shape.swap(RANK - 1, RANK - 2);\n    let size: usize = new_shape.iter().product();\n    let mut out_data = vec![T::zero(); size];\n\n    // We parallelize over the rows of the OUTPUT tensor.\n    // The output tensor has shape [Batch..., N, M].\n    // So we view it as `batch_size * N` rows, each of length `M`.\n    out_data\n        .as_mut_slice()\n        .par_chunks_mut(m)\n        .enumerate()\n        .for_each(|(i, out_row)| {\n            // `i` is the global row index in the flattened output [Batch * N, M]\n            let batch_idx = i / n;\n            let col_idx = i % n; // This corresponds to the column index in the input matrix\n\n            // Calculate the base offset for this batch in the input data\n            let input_batch_offset = batch_idx * m * n;\n\n            // Copy the column `col_idx` from the input matrix to `out_row`\n            for (r, out_elem) in out_row.iter_mut().enumerate() {\n                // Input is [M, N]. We want element at (r, col_idx).\n                // Index = input_batch_offset + r * N + col_idx\n                *out_elem = data[input_batch_offset + r * n + col_idx];\n            }\n        });\n\n    Ok(out_data)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_transpose_simple() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // 2x3\n        let shape = [2, 3];\n\n        let result = cpu_transpose(\u0026data, \u0026shape).unwrap();\n        // Expected 3x2:\n        // [1, 4]\n        // [2, 5]\n        // [3, 6]\n        assert_eq!(result, vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_batch() {\n        // Batch size 2, 2x2 matrices\n        let data = vec![\n            1.0, 2.0, 3.0, 4.0, // Matrix 1\n            5.0, 6.0, 7.0, 8.0, // Matrix 2\n        ];\n        let shape = [2, 2, 2];\n\n        let result = cpu_transpose(\u0026data, \u0026shape).unwrap();\n        // Expected:\n        // Batch 1: [1, 3, 2, 4]\n        // Batch 2: [5, 7, 6, 8]\n        let expected = vec![1.0, 3.0, 2.0, 4.0, 5.0, 7.0, 6.0, 8.0];\n        assert_eq!(result, expected);\n    }\n}\n","traces":[{"line":12,"address":[],"length":0,"stats":{"Line":200184}},{"line":16,"address":[],"length":0,"stats":{"Line":400368}},{"line":17,"address":[],"length":0,"stats":{"Line":400368}},{"line":19,"address":[],"length":0,"stats":{"Line":400368}},{"line":20,"address":[],"length":0,"stats":{"Line":800736}},{"line":21,"address":[],"length":0,"stats":{"Line":1000920}},{"line":22,"address":[],"length":0,"stats":{"Line":800736}},{"line":27,"address":[],"length":0,"stats":{"Line":200184}},{"line":29,"address":[],"length":0,"stats":{"Line":400368}},{"line":31,"address":[],"length":0,"stats":{"Line":744231}},{"line":33,"address":[],"length":0,"stats":{"Line":1088094}},{"line":34,"address":[],"length":0,"stats":{"Line":1088094}},{"line":37,"address":[],"length":0,"stats":{"Line":1088094}},{"line":40,"address":[],"length":0,"stats":{"Line":5129652}},{"line":43,"address":[],"length":0,"stats":{"Line":2331674}},{"line":47,"address":[],"length":0,"stats":{"Line":200184}}],"covered":16,"coverable":16},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","lib.rs"],"content":"use num_traits::{FromPrimitive, Num, NumAssign, ToPrimitive};\nuse std::fmt::Debug;\nuse thiserror::Error;\n\npub mod cpu_matmul;\npub mod cpu_transpose;\n\npub use cpu_matmul::cpu_matmul;\npub use cpu_transpose::cpu_transpose;\n\n#[derive(Error, Debug)]\npub enum KernelError {\n    #[error(\"Shape mismatch: expected {expected:?}, got {got:?}\")]\n    ShapeMismatch {\n        expected: Vec\u003cusize\u003e,\n        got: Vec\u003cusize\u003e,\n    },\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, KernelError\u003e;\n\n/// Trait bound for elements that can be processed by kernels.\n/// This mirrors `TensorElem` in the main crate to avoid circular dependencies.\npub trait KernelElem:\n    Num + NumAssign + Copy + Clone + Debug + Send + Sync + FromPrimitive + ToPrimitive + PartialOrd\n{\n}\n\nimpl\u003cT\u003e KernelElem for T where\n    T: Num\n        + NumAssign\n        + Copy\n        + Clone\n        + Debug\n        + Send\n        + Sync\n        + FromPrimitive\n        + ToPrimitive\n        + PartialOrd\n{\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","engine.rs"],"content":"use super::GraphNode;\nuse std::collections::HashSet;\nuse std::rc::Rc;\n\n/// Runs the backward pass starting from the given root node.\n///\n/// This function performs a topological sort of the computation graph to ensure\n/// that dependencies are processed before their consumers. It then calls\n/// `.backward()` on each node in reverse topological order.\n///\n/// # Parallelism Note\n///\n/// The backward pass is currently executed serially. While the computation graph\n/// theoretically allows for inter-op parallelism (processing independent nodes concurrently),\n/// this implementation prioritizes simplicity and relies on **intra-op parallelism**.\n///\n/// - **Intra-op parallelism**: Individual operations (like matrix multiplication) are\n///   parallelized internally (e.g., using BLAS or multi-threaded implementations).\n///   This usually yields better performance gains for deep learning workloads as\n///   operations are often heavy enough to saturate system resources.\n/// - **Inter-op parallelism**: Running multiple graph nodes simultaneously requires\n///   thread-safe graph structures (`Arc\u003cMutex\u003c...\u003e\u003e`) and complex scheduling,\n///   which adds significant overhead and complexity for often marginal gains\n///   compared to optimizing the operations themselves.\npub fn backward(root: Option\u003cRc\u003cdyn GraphNode\u003e\u003e) {\n    let Some(root) = root else { return };\n\n    let mut topo = Vec::new();\n    let mut visited = HashSet::new();\n\n    build_topo(root.clone(), \u0026mut topo, \u0026mut visited);\n\n    for node in topo.into_iter().rev() {\n        node.backward();\n    }\n}\n\n/// Recursively builds the topological sort of the graph.\nfn build_topo(\n    node: Rc\u003cdyn GraphNode\u003e,\n    topo: \u0026mut Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n    visited: \u0026mut HashSet\u003c*const ()\u003e,\n) {\n    let ptr = Rc::as_ptr(\u0026node) as *const ();\n    if visited.contains(\u0026ptr) {\n        return;\n    }\n    visited.insert(ptr);\n\n    for parent in node.parents() {\n        build_topo(parent, topo, visited);\n    }\n\n    topo.push(node);\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::cell::RefCell;\n\n    #[derive(Debug)]\n    struct MockNode {\n        id: usize,\n        parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n        visited_order: Rc\u003cRefCell\u003cVec\u003cusize\u003e\u003e\u003e,\n    }\n\n    impl GraphNode for MockNode {\n        fn backward(\u0026self) {\n            self.visited_order.borrow_mut().push(self.id);\n        }\n\n        fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n            self.parents.clone()\n        }\n    }\n\n    #[test]\n    fn test_topological_sort() {\n        let order = Rc::new(RefCell::new(Vec::new()));\n\n        // Create a diamond graph:\n        //   3\n        //  / \\\n        // 1   2\n        //  \\ /\n        //   0\n\n        let n0 = Rc::new(MockNode {\n            id: 0,\n            parents: vec![],\n            visited_order: order.clone(),\n        });\n        let n1 = Rc::new(MockNode {\n            id: 1,\n            parents: vec![n0.clone()],\n            visited_order: order.clone(),\n        });\n        let n2 = Rc::new(MockNode {\n            id: 2,\n            parents: vec![n0.clone()],\n            visited_order: order.clone(),\n        });\n        let n3 = Rc::new(MockNode {\n            id: 3,\n            parents: vec![n1.clone(), n2.clone()],\n            visited_order: order.clone(),\n        });\n\n        backward(Some(n3));\n\n        let result = order.borrow();\n        // Expected order: 3 -\u003e (1 or 2) -\u003e (2 or 1) -\u003e 0\n        assert_eq!(result.len(), 4);\n        assert_eq!(result[0], 3);\n        assert_eq!(result[3], 0);\n        assert!(result.contains(\u00261));\n        assert!(result.contains(\u00262));\n    }\n}\n","traces":[{"line":25,"address":[],"length":0,"stats":{"Line":20019}},{"line":26,"address":[],"length":0,"stats":{"Line":40038}},{"line":28,"address":[],"length":0,"stats":{"Line":40032}},{"line":29,"address":[],"length":0,"stats":{"Line":40032}},{"line":31,"address":[],"length":0,"stats":{"Line":80064}},{"line":33,"address":[],"length":0,"stats":{"Line":420104}},{"line":34,"address":[],"length":0,"stats":{"Line":180028}},{"line":39,"address":[],"length":0,"stats":{"Line":200031}},{"line":44,"address":[],"length":0,"stats":{"Line":400062}},{"line":45,"address":[],"length":0,"stats":{"Line":600093}},{"line":46,"address":[],"length":0,"stats":{"Line":20003}},{"line":48,"address":[],"length":0,"stats":{"Line":540084}},{"line":50,"address":[],"length":0,"stats":{"Line":720086}},{"line":51,"address":[],"length":0,"stats":{"Line":540045}},{"line":54,"address":[],"length":0,"stats":{"Line":540084}}],"covered":15,"coverable":15},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","functional.rs"],"content":"use crate::autograd::Variable;\nuse crate::tensor::{Cpu, Tensor, TensorElem};\n\n/// Computes the gradient of a function `f` with respect to its input.\n///\n/// Returns a function that takes a `Tensor` input and returns the gradient `Tensor`.\n///\n/// # Example\n/// ```ignore\n/// let grad_square = grad(|x| x.clone() * x.clone());\n/// let g = grad_square(Tensor::new(vec![3.0], []).unwrap());\n/// // g = 6.0\n/// ```\npub fn grad\u003cF, T, const RANK: usize\u003e(f: F) -\u003e impl Fn(Tensor\u003cT, RANK, Cpu\u003e) -\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    F: Fn(Variable\u003cT, RANK\u003e) -\u003e Variable\u003cT, RANK\u003e,\n    T: TensorElem + 'static,\n{\n    move |x| {\n        let x_var = Variable::new(x);\n        let y_var = f(x_var.clone());\n        y_var.backward();\n\n        // Extract gradient. If None (no dependency), return zeros.\n        let grad = x_var.grad.borrow();\n        if let Some(g) = grad.as_ref() {\n            g.clone()\n        } else {\n            Tensor::zeros(*x_var.data.shape())\n        }\n    }\n}\n\ntype CpuTensor\u003cT, const RANK: usize\u003e = Tensor\u003cT, RANK, Cpu\u003e;\n\n/// Computes the value and gradient of a function `f` with respect to its input.\n///\n/// Returns a function that takes a `Tensor` input and returns a tuple `(Value, Gradient)`.\npub fn value_and_grad\u003cF, T, const RANK: usize\u003e(\n    f: F,\n) -\u003e impl Fn(CpuTensor\u003cT, RANK\u003e) -\u003e (CpuTensor\u003cT, RANK\u003e, CpuTensor\u003cT, RANK\u003e)\nwhere\n    F: Fn(Variable\u003cT, RANK\u003e) -\u003e Variable\u003cT, RANK\u003e,\n    T: TensorElem + 'static,\n{\n    move |x| {\n        let x_var = Variable::new(x);\n        let y_var = f(x_var.clone());\n        y_var.backward();\n\n        let grad = x_var.grad.borrow();\n        let g = if let Some(g) = grad.as_ref() {\n            g.clone()\n        } else {\n            Tensor::zeros(*x_var.data.shape())\n        };\n\n        (y_var.data, g)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_grad_square() {\n        // f(x) = x^2\n        // f'(x) = 2x\n        let square = |x: Variable\u003cf32, 0\u003e| x.clone() * x.clone();\n        let grad_square = grad(square);\n\n        let x = Tensor::new(vec![3.0], []).unwrap();\n        let g = grad_square(x);\n\n        assert_eq!(g.data()[0], 6.0);\n    }\n\n    #[test]\n    fn test_value_and_grad_cubic() {\n        // f(x) = x^3 = x * x^2\n        // f'(x) = 3x^2\n        let cubic = |x: Variable\u003cf32, 0\u003e| x.clone() * x.clone() * x.clone();\n        let vag_cubic = value_and_grad(cubic);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let (val, g) = vag_cubic(x);\n\n        assert_eq!(val.data()[0], 8.0); // 2^3\n        assert_eq!(g.data()[0], 12.0); // 3 * 2^2\n    }\n\n    #[test]\n    fn test_grad_constant() {\n        // f(x) = 5.0\n        // f'(x) = 0.0\n        let constant = |_x: Variable\u003cf32, 0\u003e| Variable::new(Tensor::new(vec![5.0], []).unwrap());\n        let grad_constant = grad(constant);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let g = grad_constant(x);\n\n        assert_eq!(g.data()[0], 0.0);\n    }\n\n    #[test]\n    fn test_value_and_grad_constant() {\n        // f(x) = 5.0\n        let constant = |_x: Variable\u003cf32, 0\u003e| Variable::new(Tensor::new(vec![5.0], []).unwrap());\n        let vag_constant = value_and_grad(constant);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let (val, g) = vag_constant(x);\n\n        assert_eq!(val.data()[0], 5.0);\n        assert_eq!(g.data()[0], 0.0);\n    }\n}\n","traces":[{"line":14,"address":[],"length":0,"stats":{"Line":2}},{"line":19,"address":[],"length":0,"stats":{"Line":2}},{"line":20,"address":[],"length":0,"stats":{"Line":6}},{"line":21,"address":[],"length":0,"stats":{"Line":6}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":3}},{"line":27,"address":[],"length":0,"stats":{"Line":2}},{"line":29,"address":[],"length":0,"stats":{"Line":2}},{"line":39,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":2}},{"line":47,"address":[],"length":0,"stats":{"Line":6}},{"line":48,"address":[],"length":0,"stats":{"Line":6}},{"line":49,"address":[],"length":0,"stats":{"Line":4}},{"line":51,"address":[],"length":0,"stats":{"Line":4}},{"line":52,"address":[],"length":0,"stats":{"Line":5}},{"line":53,"address":[],"length":0,"stats":{"Line":2}},{"line":55,"address":[],"length":0,"stats":{"Line":2}},{"line":58,"address":[],"length":0,"stats":{"Line":2}}],"covered":19,"coverable":19},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","mod.rs"],"content":"//! Automatic Differentiation (Autograd) module.\n//!\n//! This module implements a \"Define-by-Run\" (Tape-based) automatic differentiation system,\n//! similar to PyTorch. It allows for automatic calculation of gradients for tensor operations,\n//! which is essential for training neural networks.\n//!\n//! # Key Components\n//!\n//! - [`Variable`]: The core struct that wraps a `Tensor` and tracks its gradient and computation history.\n//! - [`GraphNode`]: A trait representing an operation in the computation graph.\n//! - [`engine::backward`]: The engine that performs the backward pass (topological sort and gradient propagation).\n//! - [`functional`]: A submodule providing a JAX-style functional API (`grad`, `value_and_grad`).\n//!\n//! # Example\n//!\n//! ```rust\n//! # use xla_rs::tensor::Tensor;\n//! # use xla_rs::autograd::Variable;\n//! let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n//! let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n//!\n//! // c = a * b\n//! let c = a.clone() * b.clone();\n//!\n//! c.backward();\n//!\n//! // dc/da = b = 3.0\n//! assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n//! ```\n\nuse crate::tensor::{Cpu, Tensor, TensorElem};\nuse std::cell::RefCell;\nuse std::fmt::Debug;\nuse std::rc::Rc;\n\npub mod engine;\npub mod functional;\npub mod ops;\n\n/// A node in the computation graph.\n///\n/// This trait represents an operation that can be backpropagated through.\npub trait GraphNode: Debug {\n    /// Computes the gradient for this node and propagates it to its parents.\n    fn backward(\u0026self);\n    /// Returns the parent nodes of this node.\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e;\n}\n\n/// A variable in the computation graph.\n///\n/// Wraps a `Tensor` and tracks its gradient and the operation that created it.\n#[derive(Clone, Debug)]\npub struct Variable\u003cT, const RANK: usize\u003e\nwhere\n    T: TensorElem,\n{\n    /// The actual tensor data.\n    pub data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// The gradient of the loss with respect to this variable.\n    pub grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// The node in the computation graph that produced this variable.\n    pub node: Option\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT, const RANK: usize\u003e Variable\u003cT, RANK\u003e\nwhere\n    T: TensorElem + 'static,\n{\n    /// Creates a new leaf variable.\n    ///\n    /// Leaf variables are the inputs to the computation graph (e.g., weights, input data).\n    /// They do not have a parent node.\n    pub fn new(data: Tensor\u003cT, RANK, Cpu\u003e) -\u003e Self {\n        Self {\n            data,\n            grad: Rc::new(RefCell::new(None)),\n            node: None,\n        }\n    }\n\n    /// Creates a new variable with an associated graph node.\n    ///\n    /// This is typically used internally by operations to create output variables.\n    pub fn with_node(data: Tensor\u003cT, RANK, Cpu\u003e, node: Rc\u003cdyn GraphNode\u003e) -\u003e Self {\n        Self {\n            data,\n            grad: Rc::new(RefCell::new(None)),\n            node: Some(node),\n        }\n    }\n\n    /// Triggers the backward pass starting from this variable.\n    ///\n    /// This variable is typically the loss value (a scalar).\n    /// The gradient of this variable is seeded with 1.0.\n    pub fn backward(\u0026self) {\n        // Seed gradient\n        if self.grad.borrow().is_none() {\n            *self.grad.borrow_mut() = Some(Tensor::ones(*self.data.shape()));\n        }\n\n        crate::autograd::engine::backward(self.node.clone());\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_variable_creation() {\n        let data = Tensor::new(vec![1.0, 2.0], [2]).unwrap();\n        let var = Variable::new(data.clone());\n\n        assert_eq!(var.data.data(), data.data());\n        assert!(var.grad.borrow().is_none());\n        assert!(var.node.is_none());\n    }\n\n    #[test]\n    fn test_variable_backward_seed() {\n        let data = Tensor::new(vec![1.0], []).unwrap();\n        let var = Variable::new(data);\n\n        // Backward on leaf node should just seed the gradient\n        var.backward();\n\n        assert!(var.grad.borrow().is_some());\n        assert_eq!(var.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_variable_with_node() {\n        let data = Tensor::new(vec![10.0], []).unwrap();\n\n        // Create a mock node (using a simple struct that implements GraphNode)\n        #[derive(Debug)]\n        struct MockNode;\n        impl GraphNode for MockNode {\n            fn backward(\u0026self) {}\n            fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n                vec![]\n            }\n        }\n\n        let node = Rc::new(MockNode);\n        let var = Variable::with_node(data.clone(), node.clone());\n\n        assert_eq!(var.data.data(), data.data());\n        assert!(var.node.is_some());\n        assert!(var.grad.borrow().is_none());\n    }\n}\n","traces":[{"line":74,"address":[],"length":0,"stats":{"Line":60038}},{"line":77,"address":[],"length":0,"stats":{"Line":180114}},{"line":85,"address":[],"length":0,"stats":{"Line":1}},{"line":88,"address":[],"length":0,"stats":{"Line":3}},{"line":89,"address":[],"length":0,"stats":{"Line":1}},{"line":97,"address":[],"length":0,"stats":{"Line":20018}},{"line":99,"address":[],"length":0,"stats":{"Line":40033}},{"line":100,"address":[],"length":0,"stats":{"Line":60045}},{"line":103,"address":[],"length":0,"stats":{"Line":60054}}],"covered":9,"coverable":9},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","ops.rs"],"content":"//! Operations for the autograd system.\n//!\n//! This module defines the nodes in the computation graph for various operations\n//! (Add, Mul, MatMul) and implements the `backward` pass for each.\n\nuse super::{GraphNode, Variable};\nuse crate::tensor::{Cpu, Tensor, TensorElem};\nuse std::cell::RefCell;\nuse std::fmt::Debug;\nuse std::ops::{Add, Mul};\nuse std::rc::Rc;\n\n// --- Add Node ---\n/// A node representing element-wise addition in the computation graph.\n// --- Add Node ---\n/// A node representing element-wise addition in the computation graph.\n#[derive(Debug)]\nstruct AddNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output (received from the parent node).\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for AddNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // d(x+y)/dx = 1 * grad\n            // d(x+y)/dy = 1 * grad\n\n            // Accumulate gradient for lhs\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(grad)).unwrap();\n                } else {\n                    *lhs = Some(grad.clone());\n                }\n            }\n\n            // Accumulate gradient for rhs\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(grad)).unwrap();\n                } else {\n                    *rhs = Some(grad.clone());\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Add for Variable\u003cT, RANK\u003e {\n    type Output = Variable\u003cT, RANK\u003e;\n\n    /// Adds two variables element-wise.\n    ///\n    /// This operation creates a new node in the computation graph.\n    fn add(self, rhs: Self) -\u003e Self::Output {\n        let data = (\u0026self.data + \u0026rhs.data).unwrap();\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        // Even leaf nodes need to be part of the graph if we want to backprop to them?\n        // Actually, leaf variables usually don't have a `node` (creator).\n        // But the `AddNode` needs to update their `grad`.\n        // So `AddNode` holds references to their `grad` cells.\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(AddNode {\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents, // This is wrong. Parents should be the nodes that created lhs/rhs.\n                     // If lhs is leaf, it has no parent node.\n                     // But topological sort needs to traverse.\n                     // If leaf has no node, traversal stops there. Correct.\n        });\n\n        Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        }\n    }\n}\n\n// --- Mul Node ---\n/// A node representing element-wise multiplication in the computation graph.\n// --- Mul Node ---\n/// A node representing element-wise multiplication in the computation graph.\n#[derive(Debug)]\nstruct MulNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Data of the left-hand side operand (needed for gradient calculation).\n    lhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Data of the right-hand side operand (needed for gradient calculation).\n    rhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output (received from the parent node).\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for MulNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // d(x*y)/dx = y * grad\n            // d(x*y)/dy = x * grad\n\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                let dl_dx = (\u0026self.rhs_data * grad).unwrap();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(\u0026dl_dx)).unwrap();\n                } else {\n                    *lhs = Some(dl_dx);\n                }\n            }\n\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                let dr_dy = (\u0026self.lhs_data * grad).unwrap();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(\u0026dr_dy)).unwrap();\n                } else {\n                    *rhs = Some(dr_dy);\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Mul for Variable\u003cT, RANK\u003e {\n    type Output = Variable\u003cT, RANK\u003e;\n\n    /// Multiplies two variables element-wise.\n    ///\n    /// This operation creates a new node in the computation graph.\n    fn mul(self, rhs: Self) -\u003e Self::Output {\n        let data = (\u0026self.data * \u0026rhs.data).unwrap();\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(MulNode {\n            lhs_data: self.data.clone(),\n            rhs_data: rhs.data.clone(),\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents,\n        });\n\n        Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        }\n    }\n}\n\n// --- MatMul Node ---\n/// A node representing matrix multiplication in the computation graph.\n// --- MatMul Node ---\n/// A node representing matrix multiplication in the computation graph.\n#[derive(Debug)]\nstruct MatMulNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Data of the left-hand side operand.\n    lhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Data of the right-hand side operand.\n    rhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output.\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for MatMulNode\u003cT, RANK\u003e {\n    #[allow(clippy::collapsible_if)]\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // LHS Gradient\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                let rhs_t = self.rhs_data.transpose().unwrap();\n                let dl_da = grad.matmul(\u0026rhs_t).unwrap();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(\u0026dl_da)).unwrap();\n                } else {\n                    *lhs = Some(dl_da);\n                }\n            }\n\n            // RHS Gradient\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                let lhs_t = self.lhs_data.transpose().unwrap();\n                let dr_db = lhs_t.matmul(grad).unwrap();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(\u0026dr_db)).unwrap();\n                } else {\n                    *rhs = Some(dr_db);\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Variable\u003cT, RANK\u003e {\n    /// Performs matrix multiplication between two variables.\n    ///\n    /// This operation creates a new node in the computation graph.\n    pub fn matmul(\u0026self, rhs: \u0026Self) -\u003e crate::tensor::Result\u003cSelf\u003e {\n        let data = self.data.matmul(\u0026rhs.data)?;\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(MatMulNode {\n            lhs_data: self.data.clone(),\n            rhs_data: rhs.data.clone(),\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents,\n        });\n\n        Ok(Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_add_backward() {\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = a.clone() + b.clone();\n\n        c.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_mul_backward() {\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = a.clone() * b.clone();\n\n        c.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 2.0);\n    }\n\n    #[test]\n    fn test_chain_rule() {\n        // y = (a + b) * c\n        // a=2, b=3, c=4\n        // y = (2+3)*4 = 20\n        // dy/da = c = 4\n        // dy/db = c = 4\n        // dy/dc = a + b = 5\n\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = Variable::new(Tensor::new(vec![4.0], []).unwrap());\n\n        let sum = a.clone() + b.clone();\n        let y = sum * c.clone();\n\n        y.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 4.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 4.0);\n        assert_eq!(c.grad.borrow().as_ref().unwrap().data()[0], 5.0);\n    }\n\n    #[test]\n    fn test_matmul_backward() {\n        // A = [[1, 2], [3, 4]] (2x2)\n        // B = [[5, 6], [7, 8]] (2x2)\n        // C = A @ B\n        // C = [[19, 22], [43, 50]]\n\n        // Let Loss L = sum(C) = 19 + 22 + 43 + 50 = 134\n        // dL/dC = [[1, 1], [1, 1]]\n\n        // dL/dA = dL/dC @ B^T\n        //       = [[1, 1], [1, 1]] @ [[5, 7], [6, 8]]\n        //       = [[11, 15], [11, 15]]\n\n        // dL/dB = A^T @ dL/dC\n        //       = [[1, 3], [2, 4]] @ [[1, 1], [1, 1]]\n        //       = [[4, 4], [6, 6]]\n\n        let a_data = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], [2, 2]).unwrap();\n        let b_data = Tensor::new(vec![5.0, 6.0, 7.0, 8.0], [2, 2]).unwrap();\n\n        let a = Variable::new(a_data);\n        let b = Variable::new(b_data);\n\n        let c = a.matmul(\u0026b).unwrap();\n\n        // Manually seed gradient with ones (equivalent to sum(C))\n        *c.grad.borrow_mut() = Some(Tensor::ones([2, 2]));\n\n        // We need to manually trigger backward on the node because c is not a scalar\n        // and Variable::backward() assumes scalar and seeds with 1.0.\n        // But here we want to test the MatMulNode backward specifically.\n        // However, Variable::backward() calls engine::backward(self.node).\n        // If we seed grad manually, we can call c.backward() but we need to be careful\n        // that it doesn't overwrite our seed.\n        // Variable::backward() checks `if self.grad.borrow().is_none()`.\n        // So if we seed it first, it should be fine.\n\n        c.backward();\n\n        let a_grad = a.grad.borrow().as_ref().unwrap().clone();\n        let b_grad = b.grad.borrow().as_ref().unwrap().clone();\n\n        assert_eq!(a_grad.data(), \u0026[11.0, 15.0, 11.0, 15.0]);\n        assert_eq!(b_grad.data(), \u0026[4.0, 4.0, 6.0, 6.0]);\n    }\n\n    #[test]\n    fn test_matmul_chain_rule() {\n        // y = sum( (A @ x) * x )\n        // A = [[1, 2], [3, 4]]\n        // x = [1, 2]\n        // A@x = [5, 11]\n        // (A@x)*x = [5, 22]\n        // y = 27\n\n        // This is a bit complex to derive manually quickly.\n        // Let's try a simpler one: y = sum(A @ x)\n        // A = [[1, 2], [3, 4]]\n        // x = [1, 2]\n        // A@x = [5, 11]\n        // y = 16\n\n        // dy/dx = A^T @ ones\n        //       = [[1, 3], [2, 4]] @ [1, 1]\n        //       = [4, 6]\n\n        // dy/dA = ones @ x^T (outer product)\n        //       = [1, 1] @ [1, 2]\n        //       = [[1, 2], [1, 2]]\n\n        let a_data = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], [2, 2]).unwrap();\n        let x_data = Tensor::new(vec![1.0, 2.0], [2, 1]).unwrap(); // Column vector\n\n        let a = Variable::new(a_data);\n        let x = Variable::new(x_data);\n\n        let y_vec = a.matmul(\u0026x).unwrap();\n\n        // To make it a scalar for easy backward:\n        // We don't have a \"sum\" operation in autograd yet.\n        // But we can simulate \"sum\" by doing dot product with ones, or just seeding gradient with ones.\n        // Let's seed gradient of y_vec with ones.\n\n        *y_vec.grad.borrow_mut() = Some(Tensor::ones([2, 1]));\n        y_vec.backward();\n\n        let x_grad = x.grad.borrow().as_ref().unwrap().clone();\n        let a_grad = a.grad.borrow().as_ref().unwrap().clone();\n\n        assert_eq!(x_grad.data(), \u0026[4.0, 6.0]);\n        assert_eq!(a_grad.data(), \u0026[1.0, 2.0, 1.0, 2.0]);\n    }\n\n    #[test]\n    fn test_add_accumulation() {\n        // y = x + x + x\n        // x = 3\n        // y = 9\n        // dy/dx = 3\n\n        let x = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let y = x.clone() + x.clone() + x.clone();\n\n        y.backward();\n\n        assert_eq!(x.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n    }\n\n    #[test]\n    fn test_mul_accumulation() {\n        // y = x * x * x\n        // x = 3\n        // y = 27\n        // dy/dx = 3x^2 = 27\n\n        let x = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let y = x.clone() * x.clone() * x.clone();\n\n        y.backward();\n\n        assert_eq!(x.grad.borrow().as_ref().unwrap().data()[0], 27.0);\n    }\n\n    #[test]\n    fn test_matmul_accumulation() {\n        // Y = X @ X @ X\n        // X = [[1, 0], [0, 1]] (Identity)\n        // Y = I\n        // Loss = sum(Y) = 2\n        // dL/dX should be 3 * I ?\n        // Let's use scalar logic for intuition: y = x^3, dy/dx = 3x^2. If x=1, dy/dx=3.\n        // For matrix: d(X^3)/dX.\n        // If X = I, X^2 = I, X^3 = I.\n        // dL/dX = 3 * X^2 = 3 * I.\n\n        let x_data = Tensor::new(vec![1.0, 0.0, 0.0, 1.0], [2, 2]).unwrap();\n        let x = Variable::new(x_data);\n\n        let y = x.matmul(\u0026x).unwrap().matmul(\u0026x).unwrap();\n\n        *y.grad.borrow_mut() = Some(Tensor::ones([2, 2]));\n        y.backward();\n\n        let x_grad = x.grad.borrow().as_ref().unwrap().clone();\n        // Expected gradient is 3 * ones (since we seeded with ones and dY/dX is 3*I effectively distributed)\n        // Wait.\n        // Y = X^3. L = sum(Y).\n        // dL/dX = 3 * (X^T)^2 @ Ones?\n        // Let's just check the result.\n        // dL/dX = [[3, 3], [3, 3]]\n\n        assert_eq!(x_grad.data(), \u0026[3.0, 3.0, 3.0, 3.0]);\n    }\n\n    #[test]\n    fn test_non_leaf_operations() {\n        // Test operations where RHS has a node (is not a leaf)\n        // This ensures coverage for `if let Some(p) = \u0026rhs.node` branches\n\n        let x = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n\n        // a has a node\n        let a = x.clone() * x.clone(); // 4.0\n\n        // b = a + a. RHS a has node.\n        let b = a.clone() + a.clone(); // 8.0\n\n        // c = a * a. RHS a has node.\n        let c = a.clone() * a.clone(); // 16.0\n\n        // d = a @ a. RHS a has node. (Need rank 2 for matmul)\n        let m = Variable::new(Tensor::new(vec![2.0], [1, 1]).unwrap());\n        let n = m.matmul(\u0026m).unwrap(); // n has node\n        let _p = n.matmul(\u0026n).unwrap(); // RHS n has node\n\n        b.backward();\n        c.backward();\n        // We don't check gradients here, just ensuring the code paths run.\n    }\n}\n","traces":[{"line":30,"address":[],"length":0,"stats":{"Line":60006}},{"line":31,"address":[],"length":0,"stats":{"Line":120012}},{"line":37,"address":[],"length":0,"stats":{"Line":120012}},{"line":38,"address":[],"length":0,"stats":{"Line":60008}},{"line":39,"address":[],"length":0,"stats":{"Line":4}},{"line":41,"address":[],"length":0,"stats":{"Line":120010}},{"line":47,"address":[],"length":0,"stats":{"Line":120012}},{"line":48,"address":[],"length":0,"stats":{"Line":60010}},{"line":49,"address":[],"length":0,"stats":{"Line":8}},{"line":51,"address":[],"length":0,"stats":{"Line":120008}},{"line":57,"address":[],"length":0,"stats":{"Line":60006}},{"line":58,"address":[],"length":0,"stats":{"Line":120012}},{"line":68,"address":[],"length":0,"stats":{"Line":60014}},{"line":69,"address":[],"length":0,"stats":{"Line":180042}},{"line":71,"address":[],"length":0,"stats":{"Line":120028}},{"line":72,"address":[],"length":0,"stats":{"Line":180036}},{"line":73,"address":[],"length":0,"stats":{"Line":180033}},{"line":75,"address":[],"length":0,"stats":{"Line":100018}},{"line":76,"address":[],"length":0,"stats":{"Line":60006}},{"line":84,"address":[],"length":0,"stats":{"Line":240056}},{"line":86,"address":[],"length":0,"stats":{"Line":180042}},{"line":87,"address":[],"length":0,"stats":{"Line":180042}},{"line":88,"address":[],"length":0,"stats":{"Line":180042}},{"line":89,"address":[],"length":0,"stats":{"Line":120028}},{"line":90,"address":[],"length":0,"stats":{"Line":60014}},{"line":99,"address":[],"length":0,"stats":{"Line":60014}},{"line":125,"address":[],"length":0,"stats":{"Line":40013}},{"line":126,"address":[],"length":0,"stats":{"Line":80026}},{"line":131,"address":[],"length":0,"stats":{"Line":80026}},{"line":132,"address":[],"length":0,"stats":{"Line":120039}},{"line":133,"address":[],"length":0,"stats":{"Line":40023}},{"line":134,"address":[],"length":0,"stats":{"Line":20}},{"line":136,"address":[],"length":0,"stats":{"Line":40008}},{"line":141,"address":[],"length":0,"stats":{"Line":80026}},{"line":142,"address":[],"length":0,"stats":{"Line":120039}},{"line":143,"address":[],"length":0,"stats":{"Line":80027}},{"line":144,"address":[],"length":0,"stats":{"Line":80028}},{"line":146,"address":[],"length":0,"stats":{"Line":20006}},{"line":152,"address":[],"length":0,"stats":{"Line":40013}},{"line":153,"address":[],"length":0,"stats":{"Line":80026}},{"line":163,"address":[],"length":0,"stats":{"Line":40012}},{"line":164,"address":[],"length":0,"stats":{"Line":120036}},{"line":166,"address":[],"length":0,"stats":{"Line":80024}},{"line":167,"address":[],"length":0,"stats":{"Line":80020}},{"line":168,"address":[],"length":0,"stats":{"Line":60012}},{"line":170,"address":[],"length":0,"stats":{"Line":80014}},{"line":171,"address":[],"length":0,"stats":{"Line":60003}},{"line":174,"address":[],"length":0,"stats":{"Line":160048}},{"line":176,"address":[],"length":0,"stats":{"Line":120036}},{"line":177,"address":[],"length":0,"stats":{"Line":120036}},{"line":178,"address":[],"length":0,"stats":{"Line":120036}},{"line":179,"address":[],"length":0,"stats":{"Line":120036}},{"line":180,"address":[],"length":0,"stats":{"Line":120036}},{"line":181,"address":[],"length":0,"stats":{"Line":80024}},{"line":182,"address":[],"length":0,"stats":{"Line":40012}},{"line":188,"address":[],"length":0,"stats":{"Line":40012}},{"line":215,"address":[],"length":0,"stats":{"Line":40005}},{"line":216,"address":[],"length":0,"stats":{"Line":80010}},{"line":219,"address":[],"length":0,"stats":{"Line":80010}},{"line":220,"address":[],"length":0,"stats":{"Line":160020}},{"line":221,"address":[],"length":0,"stats":{"Line":200025}},{"line":222,"address":[],"length":0,"stats":{"Line":40007}},{"line":223,"address":[],"length":0,"stats":{"Line":4}},{"line":225,"address":[],"length":0,"stats":{"Line":40004}},{"line":231,"address":[],"length":0,"stats":{"Line":80010}},{"line":232,"address":[],"length":0,"stats":{"Line":160020}},{"line":233,"address":[],"length":0,"stats":{"Line":200025}},{"line":234,"address":[],"length":0,"stats":{"Line":40007}},{"line":235,"address":[],"length":0,"stats":{"Line":4}},{"line":237,"address":[],"length":0,"stats":{"Line":40004}},{"line":243,"address":[],"length":0,"stats":{"Line":40005}},{"line":244,"address":[],"length":0,"stats":{"Line":80010}},{"line":252,"address":[],"length":0,"stats":{"Line":40015}},{"line":253,"address":[],"length":0,"stats":{"Line":160060}},{"line":255,"address":[],"length":0,"stats":{"Line":80030}},{"line":256,"address":[],"length":0,"stats":{"Line":80027}},{"line":257,"address":[],"length":0,"stats":{"Line":60018}},{"line":259,"address":[],"length":0,"stats":{"Line":40017}},{"line":260,"address":[],"length":0,"stats":{"Line":3}},{"line":263,"address":[],"length":0,"stats":{"Line":160060}},{"line":265,"address":[],"length":0,"stats":{"Line":120045}},{"line":266,"address":[],"length":0,"stats":{"Line":120045}},{"line":267,"address":[],"length":0,"stats":{"Line":120045}},{"line":268,"address":[],"length":0,"stats":{"Line":120045}},{"line":269,"address":[],"length":0,"stats":{"Line":120045}},{"line":270,"address":[],"length":0,"stats":{"Line":80030}},{"line":271,"address":[],"length":0,"stats":{"Line":40015}},{"line":274,"address":[],"length":0,"stats":{"Line":40015}},{"line":275,"address":[],"length":0,"stats":{"Line":80030}},{"line":276,"address":[],"length":0,"stats":{"Line":40015}},{"line":277,"address":[],"length":0,"stats":{"Line":40015}}],"covered":91,"coverable":91},{"path":["/","Users","blitz","my-oss","xla-rs","src","lib.rs"],"content":"//! # xla-rs\n//!\n//! `xla-rs` is a pure Rust implementation of tensor operations and neural network building blocks,\n//! designed for educational purposes and understanding the internals of LLM inference.\n//!\n//! Despite the name, it currently runs on **CPU only** and does not yet integrate with the XLA compiler.\n//!\n//! ## Modules\n//!\n//! - [`mod@tensor`]: Core N-dimensional tensor implementation.\n//! - [`nn`]: Neural network layers (Linear, RMSNorm, MoE, etc.).\n//! - [`models`]: Model architectures (e.g., Gemma).\n//!\n//! ## Example\n//!\n//! ```rust\n//! use xla_rs::tensor::Tensor;\n//!\n//! let data = vec![1.0, 2.0, 3.0, 4.0];\n//! let tensor = Tensor::\u003cf32, 2\u003e::new(data, [2, 2]).unwrap();\n//! println!(\"{:?}\", tensor);\n//! ```\n\n/// Macro for creating a Tensor with compile-time shape checking.\n///\n/// # Examples\n///\n/// ```rust\n/// use xla_rs::tensor;\n/// use xla_rs::tensor::Tensor;\n///\n/// // Works\n/// let t = tensor!([1.0, 2.0, 3.0, 4.0], [2, 2]);\n///\n/// // Fails to compile:\n/// // let t = tensor!([1.0, 2.0, 3.0], [2, 2]);\n/// ```\n#[macro_export]\nmacro_rules! tensor {\n    ($data:expr, $shape:expr) =\u003e {{\n        // Constants to force compile-time evaluation\n        const DATA_LEN: usize = $data.len();\n        const SHAPE: [usize; $shape.len()] = $shape;\n        const EXPECTED_SIZE: usize = {\n            let mut size = 1;\n            let mut i = 0;\n            while i \u003c SHAPE.len() {\n                size *= SHAPE[i];\n                i += 1;\n            }\n            size\n        };\n\n        // This assertion triggers a compile-time error if false\n        const _: () = assert!(\n            DATA_LEN == EXPECTED_SIZE,\n            \"Shape mismatch: data length does not match shape product\"\n        );\n\n        // Safe to unwrap because we checked at compile time\n        $crate::tensor::Tensor::new($data.to_vec(), $shape).unwrap()\n    }};\n}\n\npub mod autograd;\npub mod models;\npub mod nn;\npub mod tensor;\n\npub use tensor::Tensor;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","attention.rs"],"content":"use super::rope::apply_rope;\nuse crate::nn::Linear;\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n#[derive(Debug)]\npub struct MultiHeadAttention\u003cT: TensorElem\u003e {\n    pub q_proj: Linear\u003cT\u003e,\n    pub k_proj: Linear\u003cT\u003e,\n    pub v_proj: Linear\u003cT\u003e,\n    pub o_proj: Linear\u003cT\u003e,\n\n    pub num_heads: usize,\n    pub num_kv_heads: usize,\n    pub head_dim: usize,\n    pub scaling: T,\n}\n\nimpl\u003cT: TensorElem + Float\u003e MultiHeadAttention\u003cT\u003e {\n    #[allow(clippy::too_many_arguments)]\n    pub fn new(\n        _dim: usize,\n        num_heads: usize,\n        num_kv_heads: usize,\n        head_dim: usize,\n        q_proj: Linear\u003cT\u003e,\n        k_proj: Linear\u003cT\u003e,\n        v_proj: Linear\u003cT\u003e,\n        o_proj: Linear\u003cT\u003e,\n    ) -\u003e Self {\n        Self {\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            scaling: T::one() / T::from_usize(head_dim).unwrap().sqrt(),\n        }\n    }\n\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let [b, s, _] = *x.shape();\n\n        let q = self.q_proj.forward(x)?;\n        let k = self.k_proj.forward(x)?;\n        let v = self.v_proj.forward(x)?;\n\n        // Reshape to [B, S, H, D]\n        let q = q.reshape([b, s, self.num_heads, self.head_dim])?;\n        let k = k.reshape([b, s, self.num_kv_heads, self.head_dim])?;\n        let v = v.reshape([b, s, self.num_kv_heads, self.head_dim])?;\n\n        // Permute to [B, H, S, D] using transpose_axes(1, 2)\n        let q = q.transpose_axes(1, 2)?;\n        let k = k.transpose_axes(1, 2)?;\n        let v = v.transpose_axes(1, 2)?;\n\n        // Apply RoPE (expects [B, H, S, D])\n        let q = apply_rope(\u0026q, freqs_cos, freqs_sin)?;\n        let k = apply_rope(\u0026k, freqs_cos, freqs_sin)?;\n\n        let (k, v) = if self.num_kv_heads != self.num_heads {\n            (self.repeat_kv(\u0026k)?, self.repeat_kv(\u0026v)?)\n        } else {\n            (k, v)\n        };\n\n        // Attention Score: q @ k.T\n        // q: [B, H, S, D]\n        // k: [B, H, S, D] -\u003e k.transpose() (swaps last two) -\u003e [B, H, D, S]\n        let k_t = k.transpose()?;\n\n        let q_flat = q.clone().reshape([b * self.num_heads, s, self.head_dim])?;\n        let k_t_flat = k_t.reshape([b * self.num_heads, self.head_dim, s])?;\n\n        let mut scores = q_flat.matmul(\u0026k_t_flat)?;\n\n        scores = scores.map(|val| val * self.scaling);\n\n        if let Some(m) = mask {\n            self.apply_mask(\u0026mut scores, m)?;\n        }\n\n        self.softmax_inplace(\u0026mut scores)?;\n\n        let v_flat = v.reshape([b * self.num_heads, s, self.head_dim])?;\n        let output = scores.matmul(\u0026v_flat)?;\n\n        // output: [B*H, S, D] -\u003e [B, H, S, D]\n        let output = output.reshape([b, self.num_heads, s, self.head_dim])?;\n\n        // We need [B, S, H, D].\n        // This is transpose_axes(1, 2) again on [B, H, S, D].\n        let output = output.transpose_axes(1, 2)?;\n\n        let output = output.reshape([b, s, self.num_heads * self.head_dim])?;\n\n        self.o_proj.forward(\u0026output)\n    }\n\n    fn repeat_kv(\u0026self, x: \u0026Tensor\u003cT, 4, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 4, Cpu\u003e\u003e {\n        let [b, n_kv, s, d] = *x.shape();\n        let n_rep = self.num_heads / n_kv;\n\n        if n_rep == 1 {\n            return Ok(x.clone());\n        }\n\n        let mut out = Tensor::zeros([b, self.num_heads, s, d]);\n        let src = x.data();\n        let dst = out.data_mut();\n\n        for batch in 0..b {\n            for h in 0..self.num_heads {\n                let src_h = h / n_rep;\n                let src_offset = (batch * n_kv + src_h) * s * d;\n                let dst_offset = (batch * self.num_heads + h) * s * d;\n\n                dst[dst_offset..dst_offset + s * d]\n                    .copy_from_slice(\u0026src[src_offset..src_offset + s * d]);\n            }\n        }\n        Ok(out)\n    }\n\n    fn softmax_inplace(\u0026self, x: \u0026mut Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003c()\u003e {\n        let [_, _, s] = *x.shape();\n        x.data_mut().par_chunks_mut(s).for_each(|row| {\n            let mut max_val = row[0];\n            for \u0026v in row.iter() {\n                if v \u003e max_val {\n                    max_val = v;\n                }\n            }\n\n            let mut sum_exp = T::zero();\n            for v in row.iter_mut() {\n                let exp_v = (*v - max_val).to_f32().unwrap().exp();\n                let exp_v_t = T::from_f32(exp_v).unwrap();\n                *v = exp_v_t;\n                sum_exp += exp_v_t;\n            }\n\n            let inv_sum = T::one() / sum_exp;\n            for v in row.iter_mut() {\n                *v *= inv_sum;\n            }\n        });\n        Ok(())\n    }\n\n    fn apply_mask(\u0026self, scores: \u0026mut Tensor\u003cT, 3, Cpu\u003e, mask: \u0026Tensor\u003cT, 2, Cpu\u003e) -\u003e Result\u003c()\u003e {\n        let [_, s, _] = *scores.shape();\n        let [ms1, ms2] = *mask.shape();\n\n        if s != ms1 || s != ms2 {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![s, s],\n                got: vec![ms1, ms2],\n            });\n        }\n\n        let mask_data = mask.data();\n\n        scores\n            .data_mut()\n            .par_chunks_mut(s * s)\n            .for_each(|score_matrix| {\n                for (i, val) in score_matrix.iter_mut().enumerate() {\n                    if mask_data[i] != T::one() {\n                        *val += mask_data[i];\n                    }\n                }\n            });\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::nn::Linear;\n\n    #[test]\n    fn test_mha_forward() {\n        // B=1, S=2, H=2, D=4 (Head Dim = 2)\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim; // 4\n\n        // Create dummy linear layers (identity weights for simplicity)\n        let weight_data = vec![\n            1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n        ]; // 4x4 Identity\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // Input [B, S, Hidden] -\u003e [1, 2, 4]\n        let input_data = vec![\n            1.0, 0.0, 1.0, 0.0, // Seq 1\n            0.0, 1.0, 0.0, 1.0, // Seq 2\n        ];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n\n        // RoPE cos/sin [S, HeadDim/2] -\u003e [2, 1] (since head_dim=2)\n        // Actually RoPE expects [S, HeadDim/2] for complex, but here implementation details might vary.\n        // Looking at rope.rs (not shown but inferred usage), usually [S, HeadDim/2] or [S, HeadDim].\n        // Let's assume [S, HeadDim/2] for complex rotation simulation or [S, HeadDim] for full rotation.\n        // The apply_rope signature is `freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e`.\n        // Let's use zeros/ones to be safe/no-op if possible or simple rotation.\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let output = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_mha_forward_with_mask() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        let weight_data = vec![1.0; 16]; // 4x4\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 8]; // 1x2x4\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        // Mask [S, S] -\u003e [2, 2]\n        let mask = Tensor::\u003cf32, 2\u003e::zeros([s, s]);\n\n        let output = mha\n            .forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, Some(\u0026mask))\n            .unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_mha_forward_gqa() {\n        // Grouped Query Attention: 4 heads, 2 KV heads\n        let b = 1;\n        let s = 2;\n        let num_heads = 4;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim; // 8\n        let kv_dim = num_kv_heads * head_dim; // 4\n\n        // Weights need to match dimensions\n        // Q: [Hidden, Hidden] -\u003e [8, 8]\n        // K, V: [Hidden, KV_Dim] -\u003e [8, 4] (Wait, Linear is [In, Out] or [Out, In]? Linear is usually x @ W.T + b.\n        // In xla-rs Linear, weight is [out_features, in_features].\n        // x is [B, S, Hidden].\n        // q_proj: [Hidden, Hidden] -\u003e Weight [8, 8]\n        // k_proj: [KV_Dim, Hidden] -\u003e Weight [4, 8]\n        // v_proj: [KV_Dim, Hidden] -\u003e Weight [4, 8]\n        // o_proj: [Hidden, Hidden] -\u003e Weight [8, 8]\n\n        let q_w = Tensor::new(vec![1.0; 64], [8, 8]).unwrap();\n        let k_w = Tensor::new(vec![1.0; 32], [4, 8]).unwrap();\n        let v_w = Tensor::new(vec![1.0; 32], [4, 8]).unwrap();\n        let o_w = Tensor::new(vec![1.0; 64], [8, 8]).unwrap();\n\n        let q_proj = Linear::new(q_w, None);\n        let k_proj = Linear::new(k_w, None);\n        let v_proj = Linear::new(v_w, None);\n        let o_proj = Linear::new(o_w, None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 16]; // 1x2x8\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let output = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_softmax_max_not_first() {\n        // Test case where max value is not at index 0 to cover \"v \u003e max_val\" branch\n        let b = 1;\n        let s = 2;\n        let num_heads = 1;\n        let num_kv_heads = 1;\n        let head_dim = 2;\n        let hidden_dim = 2;\n\n        // Identity weights\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0]; // 2x2\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // S=2.\n        // Input:\n        // Seq1: [0, 1]\n        // Seq2: [0, 2]\n\n        let input_data = vec![0.0, 1.0, 0.0, 2.0];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let _ = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n    }\n\n    #[test]\n    fn test_repeat_kv_no_rep() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        // Dummy linear layers\n        let weight_data = vec![1.0; 16];\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // Input [B, NumKV, S, D]\n        let input = Tensor::\u003cf32, 4\u003e::zeros([b, num_kv_heads, s, head_dim]);\n        let output = mha.repeat_kv(\u0026input).unwrap();\n\n        // Should be identical (clone)\n        assert_eq!(output.shape(), input.shape());\n    }\n\n    #[test]\n    fn test_mha_forward_mixed_mask() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        let weight_data = vec![1.0; 16];\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 8];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        // Mask [S, S] -\u003e [2, 2]\n        // [1.0, 0.0]\n        // [0.0, 1.0]\n        // 1.0 means \"keep\" (no change in additive mask logic if we assume 1.0 is identity? Wait)\n        // logic: if mask != 1.0 { val += mask }\n        // If mask is 1.0, we do nothing.\n        // If mask is 0.0, we add 0.0 (no change).\n        // Wait, usually mask is 0 for keep and -inf for mask out.\n        // Or 1 for keep and 0 for mask out (multiplicative).\n        // The code says: `if mask_data[i] != T::one() { *val += mask_data[i]; }`\n        // This implies if mask is 1.0, we do nothing.\n        // If mask is 0.0, we add 0.0.\n        // If mask is -1e9, we add -1e9.\n        // So to cover the `else` (do nothing), we need 1.0 in the mask.\n        let mask_data = vec![1.0, 0.0, 0.0, 1.0];\n        let mask = Tensor::\u003cf32, 2\u003e::new(mask_data, [s, s]).unwrap();\n\n        let output = mha\n            .forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, Some(\u0026mask))\n            .unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n}\n","traces":[{"line":22,"address":[],"length":0,"stats":{"Line":9}},{"line":40,"address":[],"length":0,"stats":{"Line":36}},{"line":44,"address":[],"length":0,"stats":{"Line":9}},{"line":51,"address":[],"length":0,"stats":{"Line":27}},{"line":53,"address":[],"length":0,"stats":{"Line":36}},{"line":54,"address":[],"length":0,"stats":{"Line":36}},{"line":55,"address":[],"length":0,"stats":{"Line":36}},{"line":58,"address":[],"length":0,"stats":{"Line":54}},{"line":59,"address":[],"length":0,"stats":{"Line":54}},{"line":60,"address":[],"length":0,"stats":{"Line":54}},{"line":63,"address":[],"length":0,"stats":{"Line":27}},{"line":64,"address":[],"length":0,"stats":{"Line":27}},{"line":65,"address":[],"length":0,"stats":{"Line":27}},{"line":68,"address":[],"length":0,"stats":{"Line":45}},{"line":69,"address":[],"length":0,"stats":{"Line":45}},{"line":71,"address":[],"length":0,"stats":{"Line":27}},{"line":72,"address":[],"length":0,"stats":{"Line":24}},{"line":74,"address":[],"length":0,"stats":{"Line":5}},{"line":80,"address":[],"length":0,"stats":{"Line":27}},{"line":82,"address":[],"length":0,"stats":{"Line":54}},{"line":83,"address":[],"length":0,"stats":{"Line":45}},{"line":85,"address":[],"length":0,"stats":{"Line":36}},{"line":87,"address":[],"length":0,"stats":{"Line":243}},{"line":89,"address":[],"length":0,"stats":{"Line":11}},{"line":90,"address":[],"length":0,"stats":{"Line":8}},{"line":93,"address":[],"length":0,"stats":{"Line":27}},{"line":95,"address":[],"length":0,"stats":{"Line":45}},{"line":96,"address":[],"length":0,"stats":{"Line":36}},{"line":99,"address":[],"length":0,"stats":{"Line":54}},{"line":103,"address":[],"length":0,"stats":{"Line":27}},{"line":105,"address":[],"length":0,"stats":{"Line":54}},{"line":107,"address":[],"length":0,"stats":{"Line":27}},{"line":110,"address":[],"length":0,"stats":{"Line":9}},{"line":111,"address":[],"length":0,"stats":{"Line":45}},{"line":112,"address":[],"length":0,"stats":{"Line":18}},{"line":114,"address":[],"length":0,"stats":{"Line":9}},{"line":115,"address":[],"length":0,"stats":{"Line":1}},{"line":118,"address":[],"length":0,"stats":{"Line":40}},{"line":119,"address":[],"length":0,"stats":{"Line":24}},{"line":120,"address":[],"length":0,"stats":{"Line":24}},{"line":122,"address":[],"length":0,"stats":{"Line":16}},{"line":123,"address":[],"length":0,"stats":{"Line":72}},{"line":124,"address":[],"length":0,"stats":{"Line":96}},{"line":125,"address":[],"length":0,"stats":{"Line":96}},{"line":126,"address":[],"length":0,"stats":{"Line":96}},{"line":128,"address":[],"length":0,"stats":{"Line":160}},{"line":129,"address":[],"length":0,"stats":{"Line":160}},{"line":132,"address":[],"length":0,"stats":{"Line":8}},{"line":135,"address":[],"length":0,"stats":{"Line":9}},{"line":136,"address":[],"length":0,"stats":{"Line":18}},{"line":137,"address":[],"length":0,"stats":{"Line":90}},{"line":138,"address":[],"length":0,"stats":{"Line":108}},{"line":139,"address":[],"length":0,"stats":{"Line":216}},{"line":140,"address":[],"length":0,"stats":{"Line":108}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":108}},{"line":146,"address":[],"length":0,"stats":{"Line":324}},{"line":147,"address":[],"length":0,"stats":{"Line":648}},{"line":148,"address":[],"length":0,"stats":{"Line":540}},{"line":149,"address":[],"length":0,"stats":{"Line":216}},{"line":150,"address":[],"length":0,"stats":{"Line":108}},{"line":153,"address":[],"length":0,"stats":{"Line":108}},{"line":154,"address":[],"length":0,"stats":{"Line":324}},{"line":155,"address":[],"length":0,"stats":{"Line":108}},{"line":158,"address":[],"length":0,"stats":{"Line":9}},{"line":161,"address":[],"length":0,"stats":{"Line":2}},{"line":162,"address":[],"length":0,"stats":{"Line":4}},{"line":163,"address":[],"length":0,"stats":{"Line":6}},{"line":165,"address":[],"length":0,"stats":{"Line":4}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":6}},{"line":174,"address":[],"length":0,"stats":{"Line":2}},{"line":176,"address":[],"length":0,"stats":{"Line":4}},{"line":177,"address":[],"length":0,"stats":{"Line":6}},{"line":178,"address":[],"length":0,"stats":{"Line":44}},{"line":179,"address":[],"length":0,"stats":{"Line":28}},{"line":180,"address":[],"length":0,"stats":{"Line":12}},{"line":184,"address":[],"length":0,"stats":{"Line":2}}],"covered":76,"coverable":80},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","mod.rs"],"content":"use crate::models::gemma::attention::MultiHeadAttention;\nuse crate::nn::{Activation, Linear, RMSNorm};\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse std::ops::Add;\n\npub mod attention;\npub mod rope;\n\n#[derive(Debug, Clone)]\npub struct GemmaConfig {\n    pub hidden_size: usize,\n    pub intermediate_size: usize,\n    pub num_hidden_layers: usize,\n    pub num_attention_heads: usize,\n    pub num_key_value_heads: usize,\n    pub head_dim: usize,\n    pub rms_norm_eps: f32,\n    pub vocab_size: usize,\n}\n\nimpl GemmaConfig {\n    pub fn gemma_70b() -\u003e Self {\n        Self {\n            hidden_size: 8192,\n            intermediate_size: 32768,\n            num_hidden_layers: 80,\n            num_attention_heads: 64,\n            num_key_value_heads: 8,\n            head_dim: 128,\n            rms_norm_eps: 1e-6,\n            vocab_size: 256000,\n        }\n    }\n\n    pub fn tiny_test() -\u003e Self {\n        Self {\n            hidden_size: 64,\n            intermediate_size: 128,\n            num_hidden_layers: 2,\n            num_attention_heads: 4,\n            num_key_value_heads: 2,\n            head_dim: 16,\n            rms_norm_eps: 1e-6,\n            vocab_size: 100,\n        }\n    }\n}\n\n#[derive(Debug)]\npub struct MLP\u003cT: TensorElem\u003e {\n    pub gate_proj: Linear\u003cT\u003e,\n    pub up_proj: Linear\u003cT\u003e,\n    pub down_proj: Linear\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e MLP\u003cT\u003e {\n    pub fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let gate = self.gate_proj.forward(x)?;\n        let up = self.up_proj.forward(x)?;\n\n        let gate = Activation::silu(\u0026gate);\n        let fused = (\u0026gate * \u0026up)?;\n\n        self.down_proj.forward(\u0026fused)\n    }\n}\n\n#[derive(Debug)]\npub struct GemmaBlock\u003cT: TensorElem\u003e {\n    pub self_attn: MultiHeadAttention\u003cT\u003e,\n    pub mlp: MLP\u003cT\u003e,\n    pub input_layernorm: RMSNorm\u003cT\u003e,\n    pub post_attention_layernorm: RMSNorm\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e GemmaBlock\u003cT\u003e {\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let residual = x;\n\n        let norm_x = self.input_layernorm.forward(x)?;\n        let attn_out = self\n            .self_attn\n            .forward(\u0026norm_x, freqs_cos, freqs_sin, mask)?;\n\n        let x = (residual.add(\u0026attn_out))?;\n\n        let residual = \u0026x;\n        let norm_x = self.post_attention_layernorm.forward(\u0026x)?;\n        let mlp_out = self.mlp.forward(\u0026norm_x)?;\n\n        let x = (residual.add(\u0026mlp_out))?;\n\n        Ok(x)\n    }\n}\n\n/// The full Gemma Model.\n///\n/// Consists of a stack of `GemmaBlock` layers followed by a final RMSNorm.\n/// Note: This struct represents the transformer body. The embedding layer and language model head\n/// are typically handled separately or wrapped in a `GemmaForCausalLM` struct (not yet implemented).\n#[derive(Debug)]\npub struct GemmaModel\u003cT: TensorElem\u003e {\n    pub layers: Vec\u003cGemmaBlock\u003cT\u003e\u003e,\n    pub norm: RMSNorm\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e GemmaModel\u003cT\u003e {\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let mut hidden = x.clone();\n\n        for layer in \u0026self.layers {\n            hidden = layer.forward(\u0026hidden, freqs_cos, freqs_sin, mask)?;\n        }\n\n        self.norm.forward(\u0026hidden)\n    }\n}\n","traces":[{"line":23,"address":[],"length":0,"stats":{"Line":0}},{"line":36,"address":[],"length":0,"stats":{"Line":1}},{"line":58,"address":[],"length":0,"stats":{"Line":3}},{"line":59,"address":[],"length":0,"stats":{"Line":12}},{"line":60,"address":[],"length":0,"stats":{"Line":12}},{"line":62,"address":[],"length":0,"stats":{"Line":9}},{"line":63,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":9}},{"line":78,"address":[],"length":0,"stats":{"Line":2}},{"line":85,"address":[],"length":0,"stats":{"Line":4}},{"line":87,"address":[],"length":0,"stats":{"Line":8}},{"line":88,"address":[],"length":0,"stats":{"Line":4}},{"line":89,"address":[],"length":0,"stats":{"Line":2}},{"line":90,"address":[],"length":0,"stats":{"Line":10}},{"line":92,"address":[],"length":0,"stats":{"Line":8}},{"line":94,"address":[],"length":0,"stats":{"Line":4}},{"line":95,"address":[],"length":0,"stats":{"Line":8}},{"line":96,"address":[],"length":0,"stats":{"Line":8}},{"line":98,"address":[],"length":0,"stats":{"Line":8}},{"line":100,"address":[],"length":0,"stats":{"Line":2}},{"line":116,"address":[],"length":0,"stats":{"Line":1}},{"line":123,"address":[],"length":0,"stats":{"Line":3}},{"line":125,"address":[],"length":0,"stats":{"Line":5}},{"line":126,"address":[],"length":0,"stats":{"Line":14}},{"line":129,"address":[],"length":0,"stats":{"Line":3}}],"covered":24,"coverable":25},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","rope.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\n\n/// Rotary Positional Embedding\n///\n/// Applies rotation to query and key tensors.\n/// x: [Batch, SeqLen, HeadDim] or [Batch, NumHeads, SeqLen, HeadDim]\n///\n/// Standard RoPE rotates adjacent pairs of elements.\npub fn apply_rope\u003cT: TensorElem\u003e(\n    x: \u0026Tensor\u003cT, 4, Cpu\u003e,\n    freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e, // [SeqLen, HeadDim/2]\n    freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n) -\u003e Result\u003cTensor\u003cT, 4, Cpu\u003e\u003e {\n    // x shape: [Batch, Heads, SeqLen, HeadDim]\n    let [b, h, s, d] = *x.shape();\n\n    // This implementation assumes `d` is even.\n\n    let mut out = Tensor::zeros([b, h, s, d]);\n\n    // Parallelize\n    use rayon::prelude::*;\n\n    // We iterate over the flattened buffer for efficiency if possible, but indices are tricky.\n    // Let's iterate over Batch, Head, SeqLen.\n\n    out.data_mut()\n        .par_chunks_mut(s * d) // Chunk by (SeqLen * HeadDim) -\u003e one head's worth of data\n        .enumerate()\n        .for_each(|(_bh_idx, head_data)| {\n            // _bh_idx tracks batch and head, but we don't need them for freq lookup usually,\n            // unless freq depends on head (it doesn't usually).\n\n            // head_data is [SeqLen, HeadDim] flat\n            for t in 0..s {\n                let offset = t * d;\n                let freqs_idx = t; // corresponds to position\n\n                for i in 0..(d / 2) {\n                    let cos = freqs_cos.data()[freqs_idx * (d / 2) + i];\n                    let sin = freqs_sin.data()[freqs_idx * (d / 2) + i];\n\n                    let x0 = head_data[offset + 2 * i];\n                    let x1 = head_data[offset + 2 * i + 1];\n\n                    // Rotate\n                    head_data[offset + 2 * i] = x0 * cos - x1 * sin;\n                    head_data[offset + 2 * i + 1] = x0 * sin + x1 * cos;\n                }\n            }\n        });\n\n    Ok(out)\n}\n\n/// Precompute frequency cis for RoPE\npub fn precompute_freqs_cis\u003cT: TensorElem + Float\u003e(\n    dim: usize,\n    max_seq_len: usize,\n    theta: T,\n) -\u003e (Tensor\u003cT, 2, Cpu\u003e, Tensor\u003cT, 2, Cpu\u003e) {\n    let half_dim = dim / 2;\n    let mut cos_out = Tensor::zeros([max_seq_len, half_dim]);\n    let mut sin_out = Tensor::zeros([max_seq_len, half_dim]);\n\n    let data_cos = cos_out.data_mut();\n    let data_sin = sin_out.data_mut();\n\n    for seq in 0..max_seq_len {\n        for i in 0..half_dim {\n            // freq = 1.0 / (theta ^ (2*i / dim))\n            let freq =\n                T::one() / theta.powf(T::from_usize(2 * i).unwrap() / T::from_usize(dim).unwrap());\n            let val = T::from_usize(seq).unwrap() * freq;\n\n            data_cos[seq * half_dim + i] = val.cos();\n            data_sin[seq * half_dim + i] = val.sin();\n        }\n    }\n\n    (cos_out, sin_out)\n}\n","traces":[{"line":10,"address":[],"length":0,"stats":{"Line":18}},{"line":16,"address":[],"length":0,"stats":{"Line":90}},{"line":20,"address":[],"length":0,"stats":{"Line":90}},{"line":28,"address":[],"length":0,"stats":{"Line":18}},{"line":29,"address":[],"length":0,"stats":{"Line":36}},{"line":31,"address":[],"length":0,"stats":{"Line":64}},{"line":36,"address":[],"length":0,"stats":{"Line":138}},{"line":37,"address":[],"length":0,"stats":{"Line":184}},{"line":38,"address":[],"length":0,"stats":{"Line":184}},{"line":40,"address":[],"length":0,"stats":{"Line":812}},{"line":41,"address":[],"length":0,"stats":{"Line":1800}},{"line":42,"address":[],"length":0,"stats":{"Line":1800}},{"line":44,"address":[],"length":0,"stats":{"Line":1080}},{"line":45,"address":[],"length":0,"stats":{"Line":1080}},{"line":48,"address":[],"length":0,"stats":{"Line":1440}},{"line":49,"address":[],"length":0,"stats":{"Line":1080}},{"line":54,"address":[],"length":0,"stats":{"Line":18}},{"line":58,"address":[],"length":0,"stats":{"Line":2}},{"line":63,"address":[],"length":0,"stats":{"Line":4}},{"line":64,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":6}},{"line":67,"address":[],"length":0,"stats":{"Line":6}},{"line":68,"address":[],"length":0,"stats":{"Line":6}},{"line":70,"address":[],"length":0,"stats":{"Line":22}},{"line":71,"address":[],"length":0,"stats":{"Line":220}},{"line":73,"address":[],"length":0,"stats":{"Line":200}},{"line":74,"address":[],"length":0,"stats":{"Line":900}},{"line":75,"address":[],"length":0,"stats":{"Line":500}},{"line":77,"address":[],"length":0,"stats":{"Line":300}},{"line":78,"address":[],"length":0,"stats":{"Line":200}},{"line":82,"address":[],"length":0,"stats":{"Line":2}}],"covered":31,"coverable":31},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","mod.rs"],"content":"pub mod gemma;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","activation.rs"],"content":"use crate::tensor::{Cpu, Tensor, TensorElem};\nuse num_traits::Float;\n\n/// Computes the SiLU (Sigmoid Linear Unit) activation function.\n///\n/// $$ \\text{SiLU}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}} $$\npub fn silu\u003cT: TensorElem + Float\u003e(x: T) -\u003e T {\n    let val = x.to_f32().unwrap();\n    let sig = 1.0 / (1.0 + (-val).exp());\n    T::from_f32(val * sig).unwrap()\n}\n\n/// Activation functions namespace.\npub struct Activation;\n\nimpl Activation {\n    /// Applies the SiLU activation function element-wise to a tensor.\n    pub fn silu\u003cconst RANK: usize, T: TensorElem + Float\u003e(\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Tensor\u003cT, RANK, Cpu\u003e {\n        x.map(silu)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_silu_value() {\n        let val = 2.0f32;\n        let res = silu(val);\n        // silu(2) = 2 * sigmoid(2) = 2 * (1 / (1 + exp(-2)))\n        // exp(-2) approx 0.135335\n        // 1 / 1.135335 approx 0.880797\n        // 2 * 0.880797 approx 1.76159\n        assert!((res - 1.76159).abs() \u003c 1e-4);\n    }\n\n    #[test]\n    fn test_activation_silu_tensor() {\n        let data = vec![0.0, 2.0];\n        let tensor = Tensor::\u003cf32, 1, Cpu\u003e::new(data, [2]).unwrap();\n        let res = Activation::silu(\u0026tensor);\n\n        let res_data = res.data();\n        assert!((res_data[0] - 0.0).abs() \u003c 1e-6); // 0 * sigmoid(0) = 0\n        assert!((res_data[1] - 1.76159).abs() \u003c 1e-4);\n    }\n}\n","traces":[{"line":7,"address":[],"length":0,"stats":{"Line":771}},{"line":8,"address":[],"length":0,"stats":{"Line":3084}},{"line":9,"address":[],"length":0,"stats":{"Line":1542}},{"line":10,"address":[],"length":0,"stats":{"Line":2313}},{"line":18,"address":[],"length":0,"stats":{"Line":4}},{"line":21,"address":[],"length":0,"stats":{"Line":8}}],"covered":6,"coverable":6},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","linear.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\n\nuse rayon::prelude::*;\n\n/// Constants for Linear Layer\nconst WEIGHT_RANK: usize = 2;\nconst BIAS_RANK: usize = 1;\n\n/// Trait to enforce allowed ranks for Linear layer forward pass.\npub trait AllowedLinearRank\u003cconst N: usize\u003e {}\nimpl AllowedLinearRank\u003c2\u003e for () {}\nimpl AllowedLinearRank\u003c3\u003e for () {}\n\n/// # Design Philosophy: Fixed Ranks\n///\n/// The `Linear` layer enforces `WEIGHT_RANK = 2` and `BIAS_RANK = 1`.\n/// This is a deliberate design choice to align with the mathematical definition of a Linear (or Fully Connected) layer:\n/// $$y = xA^T + b$$\n///\n/// - **Weights ($A$):** Must be a Matrix (Rank 2) mapping `in_features` $\\to$ `out_features`.\n/// - **Bias ($b$):** Must be a Vector (Rank 1) matching `out_features`.\n///\n/// While one might want to use a `Tensor\u003cT, 16, Cpu\u003e` as weights, doing so would strictly no longer be a\n/// standard \"Linear\" layer operation (it would be a Tensor Contraction or specialized convolution).\n/// By enforcing these ranks, we keep the `Linear` abstraction clean, predictable, and mathematically correct.\n/// If higher-dimensional weights are needed, they should be explicitly reshaped or flattened before being\n/// passed to a Linear layer.\n///\n/// Linear Layer: `y = xA^T + b`\n///\n/// Performs a linear transformation on the input data.\n/// This layer represents a collection of neurons where every input is connected to every output.\n///\n/// # Generics\n/// - `T`: The element type of the tensors (e.g., `f32`, `f64`).\n///\n/// # Examples\n/// ```rust\n/// use xla_rs::nn::Linear;\n/// use xla_rs::tensor::Tensor;\n/// // Create a layer with 10 inputs and 5 outputs\n/// let layer = Linear::\u003cf32\u003e::new(\n///     Tensor::zeros([5, 10]), // Weights: [out, in]\n///     Some(Tensor::zeros([5])) // Bias: [out]\n/// );\n/// ```\n#[derive(Debug)]\npub struct Linear\u003cT: TensorElem\u003e {\n    /// The learnable weights of the layer.\n    /// - Shape: `[out_features, in_features]`\n    pub weight: Tensor\u003cT, WEIGHT_RANK, Cpu\u003e,\n\n    /// The learnable bias of the layer.\n    /// - Shape: `[out_features]`\n    pub bias: Option\u003cTensor\u003cT, BIAS_RANK, Cpu\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem\u003e Linear\u003cT\u003e {\n    /// Creates a new Linear layer.\n    ///\n    /// # Arguments\n    ///\n    /// * `weight` - The weight tensor of shape `[out_features, in_features]`.\n    /// * `bias` - The optional bias tensor of shape `[out_features]`.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::nn::Linear;\n    /// use xla_rs::tensor::Tensor;\n    ///\n    /// let weight = Tensor::\u003cf32, 2, _\u003e::zeros([5, 10]);\n    /// let layer = Linear::new(weight, None);\n    /// ```\n    pub fn new(\n        weight: Tensor\u003cT, WEIGHT_RANK, Cpu\u003e,\n        bias: Option\u003cTensor\u003cT, BIAS_RANK, Cpu\u003e\u003e,\n    ) -\u003e Self {\n        Self { weight, bias }\n    }\n\n    /// Performs the forward pass of the Linear layer.\n    ///\n    /// Supports inputs of Rank 2 `[batch_size, in_features]` or Rank 3 `[batch_size, seq_len, in_features]`.\n    ///\n    /// # Arguments\n    ///\n    /// * `x` - The input tensor.\n    ///\n    /// # Returns\n    ///\n    /// The output tensor after applying the linear transformation.\n    pub fn forward\u003cconst RANK: usize\u003e(\n        \u0026self,\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\n    where\n        (): AllowedLinearRank\u003cRANK\u003e,\n    {\n        // Compile-time check (redundant with trait bound but satisfies request for safer check if desired,\n        // but trait bound `AllowedLinearRank` actually prevents compilation for other ranks, so strictly unnecessary\n        // to assert constant. However, we'll stick to the logic that handles 2 and 3).\n\n        let w_t = self.weight.transpose()?;\n\n        if RANK == 3 {\n            let [b, s, i] = x.shape()[0..3].try_into().unwrap();\n            // Explicitly specify type for flat_x\n            let flat_x: Tensor\u003cT, 2, Cpu\u003e = x.clone().reshape([b * s, i])?;\n            let out_flat = flat_x.matmul(\u0026w_t)?;\n\n            let out_biased = if let Some(b_bias) = \u0026self.bias {\n                Self::add_bias(\u0026out_flat, b_bias)?\n            } else {\n                out_flat\n            };\n\n            let out_features = self.weight.shape()[0];\n            let res = out_biased.reshape([b, s, out_features])?;\n\n            unsafe {\n                let res_ptr = \u0026res as *const Tensor\u003cT, 3, Cpu\u003e;\n                let ret = std::ptr::read(res_ptr as *const Tensor\u003cT, RANK, Cpu\u003e);\n                std::mem::forget(res);\n                Ok(ret)\n            }\n        } else {\n            // RANK == 2 guaranteed by AllowedLinearRank\u003cRANK\u003e if not 3\n\n            // We need to cast `w_t` (Rank 2) to `Tensor\u003cT, RANK\u003e` unsafely to call `matmul`\n            // because compiler sees generic RANK.\n\n            let w_t_cast: \u0026Tensor\u003cT, RANK, Cpu\u003e = unsafe { std::mem::transmute(\u0026w_t) };\n\n            let out = x.matmul(w_t_cast)?;\n            let out = if let Some(b) = \u0026self.bias {\n                let shape = out.shape();\n                let [_rows, cols] = [shape[0], shape[1]];\n\n                if b.shape()[0] != cols {\n                    return Err(crate::tensor::TensorError::ShapeMismatch {\n                        expected: vec![cols],\n                        got: vec![b.shape()[0]],\n                    });\n                }\n\n                let bias_data = b.data();\n                let mut out_mut = out;\n                let out_slice = out_mut.data_mut();\n\n                out_slice.par_chunks_mut(cols).for_each(|row| {\n                    for (r, bv) in row.iter_mut().zip(bias_data.iter()) {\n                        *r += *bv;\n                    }\n                });\n                out_mut\n            } else {\n                out\n            };\n            Ok(out)\n        }\n    }\n\n    /// Helper to add bias to a 2D tensor.\n    fn add_bias(x: \u0026Tensor\u003cT, 2, Cpu\u003e, bias: \u0026Tensor\u003cT, 1, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 2, Cpu\u003e\u003e {\n        let [_, cols] = *x.shape();\n        let [b_cols] = *bias.shape();\n\n        if cols != b_cols {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![cols],\n                got: vec![b_cols],\n            });\n        }\n\n        let mut out = x.clone();\n\n        out.data_mut().par_chunks_mut(cols).for_each(|row| {\n            for (r, b) in row.iter_mut().zip(bias.data().iter()) {\n                *r += *b;\n            }\n        });\n\n        Ok(out)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_linear_new() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([5, 10]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([5]);\n        let layer = Linear::new(weight, Some(bias));\n        assert!(layer.bias.is_some());\n    }\n\n    #[test]\n    fn test_linear_forward_rank2() {\n        // Input: [2, 3]\n        // Weight: [4, 3] (out=4, in=3)\n        // Bias: [4]\n        let input_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::new(input_data, [2, 3]).unwrap();\n\n        let weight_data = vec![\n            1.0, 0.0, 0.0, // 1st neuron\n            0.0, 1.0, 0.0, // 2nd neuron\n            0.0, 0.0, 1.0, // 3rd neuron\n            1.0, 1.0, 1.0, // 4th neuron\n        ];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [4, 3]).unwrap();\n\n        let bias_data = vec![0.1, 0.2, 0.3, 0.4];\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::new(bias_data, [4]).unwrap();\n\n        let layer = Linear::new(weight, Some(bias));\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[2, 4]);\n        // Row 1: [1, 2, 3]\n        // Out 1: 1*1 + 0.1 = 1.1\n        // Out 2: 2*1 + 0.2 = 2.2\n        // Out 3: 3*1 + 0.3 = 3.3\n        // Out 4: (1+2+3) + 0.4 = 6.4\n        let out_data = output.data();\n        assert!((out_data[0] - 1.1).abs() \u003c 1e-6);\n        assert!((out_data[1] - 2.2).abs() \u003c 1e-6);\n        assert!((out_data[2] - 3.3).abs() \u003c 1e-6);\n        assert!((out_data[3] - 6.4).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3() {\n        // Input: [1, 2, 3] (Batch=1, Seq=2, In=3)\n        let input_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 3]).unwrap();\n\n        let weight_data = vec![\n            1.0, 1.0, 1.0, // 1st neuron\n            2.0, 2.0, 2.0, // 2nd neuron\n        ];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 3]).unwrap();\n\n        let layer = Linear::new(weight, None);\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2, 2]);\n        // Row 1: [1, 2, 3] -\u003e Sum=6. Out1=6, Out2=12\n        // Row 2: [4, 5, 6] -\u003e Sum=15. Out1=15, Out2=30\n        let out_data = output.data();\n        assert!((out_data[0] - 6.0).abs() \u003c 1e-6);\n        assert!((out_data[1] - 12.0).abs() \u003c 1e-6);\n        assert!((out_data[2] - 15.0).abs() \u003c 1e-6);\n        assert!((out_data[3] - 30.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_shape_mismatch() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([5, 10]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([4]); // Wrong size\n        let layer = Linear::new(weight, Some(bias));\n\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 10]);\n        // This should fail inside forward when adding bias, or ideally we check in new?\n        // Current impl checks in add_bias which is called in forward.\n        // Actually, let's check add_bias directly via private access if possible or just run forward.\n        // Since we are in the same module (submodule tests), we can test private methods if we want,\n        // but forward is public.\n\n        let res = layer.forward(\u0026input);\n        assert!(res.is_err());\n    }\n\n    #[test]\n    fn test_linear_forward_rank2_no_bias() {\n        let input_data = vec![1.0, 2.0];\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::new(input_data, [1, 2]).unwrap();\n\n        // Weight: [2, 2]\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 2]).unwrap();\n\n        let layer = Linear::new(weight, None);\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2]);\n        let out_data = output.data();\n        assert!((out_data[0] - 1.0).abs() \u003c 1e-6);\n        assert!((out_data[1] - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3_with_bias() {\n        // Input: [1, 2, 2] (Batch=1, Seq=2, In=2)\n        let input_data = vec![1.0, 1.0, 2.0, 2.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 2]).unwrap();\n\n        // Weight: [2, 2] (Identity)\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 2]).unwrap();\n\n        // Bias: [2]\n        let bias_data = vec![0.5, 0.5];\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::new(bias_data, [2]).unwrap();\n\n        let layer = Linear::new(weight, Some(bias));\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2, 2]);\n        let out_data = output.data();\n        // Row 1: [1, 1] + [0.5, 0.5] = [1.5, 1.5]\n        // Row 2: [2, 2] + [0.5, 0.5] = [2.5, 2.5]\n        assert!((out_data[0] - 1.5).abs() \u003c 1e-6);\n        assert!((out_data[1] - 1.5).abs() \u003c 1e-6);\n        assert!((out_data[2] - 2.5).abs() \u003c 1e-6);\n        assert!((out_data[3] - 2.5).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3_mismatch() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 2]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([3]); // Wrong size\n        let layer = Linear::new(weight, Some(bias));\n\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::zeros([1, 2, 2]);\n        let res = layer.forward(\u0026input);\n        assert!(res.is_err());\n    }\n}\n","traces":[{"line":75,"address":[],"length":0,"stats":{"Line":51}},{"line":93,"address":[],"length":0,"stats":{"Line":53}},{"line":104,"address":[],"length":0,"stats":{"Line":159}},{"line":106,"address":[],"length":0,"stats":{"Line":53}},{"line":107,"address":[],"length":0,"stats":{"Line":245}},{"line":109,"address":[],"length":0,"stats":{"Line":294}},{"line":110,"address":[],"length":0,"stats":{"Line":196}},{"line":112,"address":[],"length":0,"stats":{"Line":99}},{"line":113,"address":[],"length":0,"stats":{"Line":7}},{"line":115,"address":[],"length":0,"stats":{"Line":47}},{"line":118,"address":[],"length":0,"stats":{"Line":144}},{"line":119,"address":[],"length":0,"stats":{"Line":240}},{"line":122,"address":[],"length":0,"stats":{"Line":96}},{"line":123,"address":[],"length":0,"stats":{"Line":144}},{"line":124,"address":[],"length":0,"stats":{"Line":96}},{"line":125,"address":[],"length":0,"stats":{"Line":48}},{"line":133,"address":[],"length":0,"stats":{"Line":16}},{"line":135,"address":[],"length":0,"stats":{"Line":16}},{"line":136,"address":[],"length":0,"stats":{"Line":10}},{"line":137,"address":[],"length":0,"stats":{"Line":9}},{"line":138,"address":[],"length":0,"stats":{"Line":9}},{"line":140,"address":[],"length":0,"stats":{"Line":6}},{"line":141,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":3}},{"line":143,"address":[],"length":0,"stats":{"Line":3}},{"line":147,"address":[],"length":0,"stats":{"Line":6}},{"line":148,"address":[],"length":0,"stats":{"Line":4}},{"line":149,"address":[],"length":0,"stats":{"Line":6}},{"line":151,"address":[],"length":0,"stats":{"Line":11}},{"line":152,"address":[],"length":0,"stats":{"Line":45}},{"line":153,"address":[],"length":0,"stats":{"Line":10}},{"line":156,"address":[],"length":0,"stats":{"Line":2}},{"line":158,"address":[],"length":0,"stats":{"Line":1}},{"line":160,"address":[],"length":0,"stats":{"Line":3}},{"line":165,"address":[],"length":0,"stats":{"Line":2}},{"line":166,"address":[],"length":0,"stats":{"Line":4}},{"line":167,"address":[],"length":0,"stats":{"Line":4}},{"line":169,"address":[],"length":0,"stats":{"Line":2}},{"line":170,"address":[],"length":0,"stats":{"Line":1}},{"line":171,"address":[],"length":0,"stats":{"Line":3}},{"line":172,"address":[],"length":0,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":3}},{"line":178,"address":[],"length":0,"stats":{"Line":6}},{"line":179,"address":[],"length":0,"stats":{"Line":22}},{"line":180,"address":[],"length":0,"stats":{"Line":4}},{"line":184,"address":[],"length":0,"stats":{"Line":1}}],"covered":46,"coverable":46},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","mod.rs"],"content":"pub mod activation;\npub mod linear;\npub mod module;\npub mod moe;\npub mod norm;\n\npub use activation::Activation;\npub use linear::{AllowedLinearRank, Linear};\npub use module::Module;\npub use norm::RMSNorm;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","module.rs"],"content":"use crate::tensor::TensorElem;\nuse std::fmt::Debug;\n\n/// A Module trait for Neural Network layers.\n///\n/// # Why is this needed?\n///\n/// The `Module` trait serves as the fundamental building block for all neural network components\n/// in `xla-rs`. It enforces a common interface that ensures:\n///\n/// 1.  **Thread Safety**: By requiring `Send` and `Sync`, we ensure that models can be safely\n///     shared across threads (e.g., for parallel inference or data loading).\n/// 2.  **Debuggability**: Requiring `Debug` ensures that the structure of any model can be\n///     easily inspected.\n/// 3.  **Extensibility**: While currently a marker trait, this abstraction allows us to generically\n///     implement features like parameter counting, device movement, and serialization for all layers\n///     in the future without breaking changes.\npub trait Module\u003cT: TensorElem\u003e: Debug + Send + Sync {}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[derive(Debug)]\n    struct MockModule;\n\n    impl Module\u003cf32\u003e for MockModule {}\n\n    #[test]\n    fn test_module_implementation() {\n        let module = MockModule;\n        // Just verify it compiles and runs\n        println!(\"{:?}\", module);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","moe.rs"],"content":"//! # Mixture of Experts (MoE)\n//!\n//! This module implements a sparse Mixture of Experts (MoE) layer, a technique to scale model capacity\n//! without proportionally increasing computational cost.\n//!\n//! ## What is Mixture of Experts?\n//!\n//! In a standard dense model, every input token is processed by every parameter in the network.\n//! In an MoE model, the \"FeedForward\" block is replaced by a set of \"Experts\" (usually smaller FeedForward networks)\n//! and a \"Router\" (or Gate). For each token, the Router selects a small subset (Top-K) of experts to process it.\n//!\n//! $$ \\text{Output} = \\sum_{i \\in \\text{TopK}} w_i \\cdot \\text{Expert}_i(x) $$\n//!\n//! This allows the model to have a massive number of parameters (high capacity) while only using a fraction\n//! of them per token (low inference latency).\n//!\n//! ## Implementation Details\n//!\n//! This implementation provides:\n//! - **`TopKRouter`**: A learnable gating mechanism that projects inputs to expert logits and selects the top-k indices.\n//! - **`MoELayer`**: The container that holds the router and the list of experts.\n//! - **`Expert` Trait**: An abstraction for what constitutes an expert (typically an MLP).\n//!\n//! ### Routing Mechanism\n//!\n//! We use a standard Top-K routing mechanism:\n//! 1.  Compute logits: $H(x) = x \\cdot W_{gate}$\n//! 2.  Select Top-K: Identify the $k$ experts with the highest logits.\n//! 3.  Normalize: Apply Softmax to the selected logits to get routing weights.\n//! 4.  Dispatch: Send tokens to their respective experts.\n//! 5.  Combine: Weighted sum of expert outputs.\n//!\n//! ## Trade-offs and Design Decisions\n//!\n//! ### 1. Explicit Loops vs. Scatter/Gather\n//! **Decision**: We use explicit iteration and grouping (bucketing) of tokens per expert rather than\n//! optimized scatter/gather tensor operations.\n//!\n//! **Why?**\n//! - **Simplicity**: `xla-rs` is designed for clarity and education. Implementing efficient sparse scatter/gather\n//!   kernels is complex and hardware-specific.\n//! - **CPU Focus**: On CPU, the overhead of grouping tokens is often negligible compared to the matrix multiplications\n//!   inside the experts. Explicit grouping allows us to use standard dense matrix multiplication for each expert,\n//!   which is well-optimized.\n//!\n//! ### 2. Dynamic Control Flow\n//! **Decision**: The routing logic dynamically constructs batches for each expert at runtime.\n//!\n//! **Why?**\n//! - This avoids padding and wasted computation associated with fixed-size expert buffers (common in TPU/GPU implementations).\n//! - It handles load imbalance naturally (though extreme imbalance can still hurt performance due to stragglers).\n//!\n//! ### 3. Generic Experts\n//! **Decision**: Experts are generic modules implementing the `Expert` trait.\n//!\n//! **Why?**\n//! - Allows experimenting with different expert architectures (e.g., different activation functions,\n//!   or even nested MoEs) without changing the routing logic.\n\nuse crate::nn::{Linear, Module};\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n/// Top-K Router for Mixture of Experts.\n///\n/// Routes inputs to the top-k experts based on gate logits.\n#[derive(Debug)]\npub struct TopKRouter\u003cT: TensorElem\u003e {\n    pub gate: Linear\u003cT\u003e,\n    pub num_experts: usize,\n    pub k: usize,\n}\n\nimpl\u003cT: TensorElem + Float\u003e TopKRouter\u003cT\u003e {\n    /// Creates a new TopKRouter.\n    ///\n    /// # Arguments\n    ///\n    /// * `gate` - The linear layer used to compute routing logits.\n    /// * `num_experts` - Total number of experts.\n    /// * `k` - Number of experts to route to per token.\n    pub fn new(gate: Linear\u003cT\u003e, num_experts: usize, k: usize) -\u003e Self {\n        Self {\n            gate,\n            num_experts,\n            k,\n        }\n    }\n\n    /// Performs routing.\n    ///\n    /// # Returns\n    ///\n    /// A tuple containing:\n    /// * `weights` - The routing weights for the top-k experts.\n    /// * `indices` - The indices of the top-k experts.\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n    ) -\u003e Result\u003c(Tensor\u003cT, 3, Cpu\u003e, Tensor\u003cusize, 3, Cpu\u003e)\u003e {\n        let logits = self.gate.forward(x)?;\n\n        let [b, s, n_e] = *logits.shape();\n\n        let mut weights_out = Tensor::zeros([b, s, self.k]);\n        let mut indices_out = Tensor::zeros([b, s, self.k]);\n\n        weights_out\n            .data_mut()\n            .par_chunks_mut(self.k)\n            .zip(indices_out.data_mut().par_chunks_mut(self.k))\n            .zip(logits.data().par_chunks(n_e))\n            .for_each(|((w_row, i_row), l_row)| {\n                let mut pairs: Vec\u003c(T, usize)\u003e = l_row\n                    .iter()\n                    .copied()\n                    .enumerate()\n                    .map(|(i, v)| (v, i))\n                    .collect();\n\n                pairs.sort_by(|a, b| b.0.partial_cmp(\u0026a.0).unwrap_or(std::cmp::Ordering::Equal));\n\n                let top_k = \u0026pairs[0..self.k];\n\n                let max_val = top_k[0].0;\n                let mut sum_exp = T::zero();\n                let mut exps = Vec::with_capacity(self.k);\n\n                for (val, _) in top_k {\n                    let exp_v = (*val - max_val).to_f32().unwrap().exp();\n                    let exp_v_t = T::from_f32(exp_v).unwrap();\n                    sum_exp += exp_v_t;\n                    exps.push(exp_v_t);\n                }\n\n                let inv_sum = T::one() / sum_exp;\n\n                for idx in 0..self.k {\n                    w_row[idx] = exps[idx] * inv_sum;\n                    i_row[idx] = top_k[idx].1;\n                }\n            });\n\n        Ok((weights_out, indices_out))\n    }\n}\n\n/// Mixture of Experts Layer.\n///\n/// Consists of a router and a set of experts.\n#[derive(Debug)]\npub struct MoELayer\u003cT: TensorElem, E: Module\u003cT\u003e\u003e {\n    pub router: TopKRouter\u003cT\u003e,\n    pub experts: Vec\u003cE\u003e,\n}\n\nimpl\u003cT: TensorElem + Float, E: Module\u003cT\u003e\u003e MoELayer\u003cT, E\u003e {\n    /// Creates a new MoELayer.\n    pub fn new(router: TopKRouter\u003cT\u003e, experts: Vec\u003cE\u003e) -\u003e Self {\n        Self { router, experts }\n    }\n\n    // Note: This forward signature assumes E (Expert) takes Tensor\u003cT, 3\u003e and returns Tensor\u003cT, 3\u003e.\n    // Since Module trait is generic and doesn't enforce forward signature (Rust traits can't easily enforce generic methods with varying ranks),\n    // we might need to assume E has a forward method or define a more specific Expert trait.\n    // For now, we'll implement it assuming E has a forward method compatible with MLP.\n    // But wait, Module trait is empty marker.\n    // We should probably add `forward` to Module or create `Expert` trait.\n    // For this refactor, I'll keep it simple and assume E is MLP-like but we can't call forward on generic E without a trait method.\n    // So I will define a trait `Expert` here or use `Module` if I update it.\n    // Let's update `Module` trait in a separate step if needed, or just define `Expert` trait here.\n\n    // Actually, to avoid changing `Module` trait too much right now, let's define a local trait or just rely on the fact that we moved it.\n    // But the plan said \"Make MoELayer generic over Expert\".\n    // Let's define `Expert` trait in this file for now.\n}\n\n/// Trait for Experts in MoE.\n///\n/// Experts must implement `Module` and provide a `forward` method accepting Rank 3 tensors.\npub trait Expert\u003cT: TensorElem\u003e: Module\u003cT\u003e {\n    fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e;\n}\n\nimpl\u003cT: TensorElem + Float, E: Expert\u003cT\u003e\u003e MoELayer\u003cT, E\u003e {\n    /// Performs the forward pass of the MoE layer.\n    ///\n    /// Routes inputs to experts and aggregates the results.\n    pub fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let [b, s, h] = *x.shape();\n        let (weights, indices) = self.router.forward(x)?;\n\n        let mut final_output = Tensor::zeros([b, s, h]);\n\n        let mut assignments: Vec\u003cVec\u003cusize\u003e\u003e = vec![vec![]; self.experts.len()];\n        let mut assignment_weights: Vec\u003cVec\u003cT\u003e\u003e = vec![vec![]; self.experts.len()];\n\n        let w_data = weights.data();\n        let i_data = indices.data();\n\n        for idx in 0..(b * s) {\n            for k_i in 0..self.router.k {\n                let expert_idx = i_data[idx * self.router.k + k_i];\n                let weight = w_data[idx * self.router.k + k_i];\n                assignments[expert_idx].push(idx);\n                assignment_weights[expert_idx].push(weight);\n            }\n        }\n\n        type ExpertResult\u003cT\u003e = Option\u003c(Vec\u003cusize\u003e, Tensor\u003cT, 2, Cpu\u003e, Vec\u003cT\u003e)\u003e;\n        let results: Vec\u003cExpertResult\u003cT\u003e\u003e = self\n            .experts\n            .par_iter()\n            .enumerate()\n            .map(|(e_idx, expert)| {\n                let indices: \u0026Vec\u003cusize\u003e = \u0026assignments[e_idx];\n                if indices.is_empty() {\n                    return None;\n                }\n\n                let num_samples = indices.len();\n                let mut input_data = Vec::with_capacity(num_samples * h);\n\n                for \u0026token_idx in indices {\n                    let start = token_idx * h;\n                    input_data.extend_from_slice(\u0026x.data()[start..start + h]);\n                }\n\n                let input_tensor = Tensor::new(input_data, [num_samples, h]).unwrap();\n\n                let input_3d = input_tensor.reshape([num_samples, 1, h]).unwrap();\n\n                let output_3d = expert.forward(\u0026input_3d).unwrap();\n                let output_2d = output_3d.reshape([num_samples, h]).unwrap();\n\n                Some((\n                    indices.clone(),\n                    output_2d,\n                    assignment_weights[e_idx].clone(),\n                ))\n            })\n            .collect();\n\n        let out_data = final_output.data_mut();\n\n        for (indices, output_tensor, weights) in results.into_iter().flatten() {\n            let out_vals = output_tensor.data();\n            for (i, \u0026token_idx) in indices.iter().enumerate() {\n                let weight = weights[i];\n                let out_offset = token_idx * h;\n                let val_offset = i * h;\n\n                for j in 0..h {\n                    out_data[out_offset + j] += out_vals[val_offset + j] * weight;\n                }\n            }\n        }\n\n        Ok(final_output)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    // Mock Expert\n    #[derive(Debug)]\n    struct MockExpert {\n        id: usize,\n    }\n\n    impl Module\u003cf32\u003e for MockExpert {}\n\n    impl Expert\u003cf32\u003e for MockExpert {\n        fn forward(\u0026self, x: \u0026Tensor\u003cf32, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cf32, 3, Cpu\u003e\u003e {\n            // Expert returns input * id\n            let scale = self.id as f32;\n            let out = x.map(|v| v * scale);\n            Ok(out)\n        }\n    }\n\n    #[test]\n    fn test_moe_forward() {\n        // 2 Experts, Top-1 Routing\n        // Input: [1, 2, 2] (Batch=1, Seq=2, Dim=2)\n        // Router Gate: Identity-like to force routing\n\n        // Input:\n        // [[1.0, 0.0],  -\u003e Should route to Expert 0 if gate favors index 0\n        //  [0.0, 1.0]]  -\u003e Should route to Expert 1 if gate favors index 1\n\n        let input_data = vec![1.0, 0.0, 0.0, 1.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 2]).unwrap();\n\n        // Gate weights: Identity [2, 2]\n        // [1, 0]\n        // [0, 1]\n        let gate_w_data = vec![1.0, 0.0, 0.0, 1.0];\n        let gate_w = Tensor::\u003cf32, 2, Cpu\u003e::new(gate_w_data, [2, 2]).unwrap();\n        let gate = Linear::new(gate_w, None);\n\n        let router = TopKRouter::new(gate, 2, 1);\n\n        let experts = vec![\n            MockExpert { id: 10 }, // Expert 0 scales by 10\n            MockExpert { id: 20 }, // Expert 1 scales by 20\n        ];\n\n        let moe = MoELayer::new(router, experts);\n\n        let output = moe.forward(\u0026input).unwrap();\n\n        // Token 0: [1, 0] -\u003e Gate -\u003e [1, 0] -\u003e Top1 is Index 0 (Score 1.0).\n        // Softmax([1, 0]) -\u003e [0.73, 0.27]. Top1 weight approx 0.73?\n        // Wait, TopKRouter implementation:\n        // pairs sorted. top_k = pairs[0..k].\n        // max_val = top_k[0].0\n        // sum_exp...\n        // If k=1, max_val = val. exp(val - max_val) = exp(0) = 1.\n        // sum_exp = 1.\n        // weight = 1/1 = 1.\n        // So weight is 1.0 for top-1.\n\n        // Token 0 routes to Expert 0 (id 10). Input [1, 0]. Output [10, 0]. Weight 1.\n        // Final Token 0: [10, 0].\n\n        // Token 1: [0, 1] -\u003e Gate -\u003e [0, 1] -\u003e Top1 is Index 1 (Score 1.0).\n        // Weight 1.0.\n        // Token 1 routes to Expert 1 (id 20). Input [0, 1]. Output [0, 20]. Weight 1.\n        // Final Token 1: [0, 20].\n\n        let out_data = output.data();\n        assert!((out_data[0] - 10.0).abs() \u003c 1e-4);\n        assert!((out_data[1] - 0.0).abs() \u003c 1e-4);\n        assert!((out_data[2] - 0.0).abs() \u003c 1e-4);\n        assert!((out_data[3] - 20.0).abs() \u003c 1e-4);\n    }\n}\n","traces":[{"line":83,"address":[],"length":0,"stats":{"Line":1}},{"line":98,"address":[],"length":0,"stats":{"Line":1}},{"line":102,"address":[],"length":0,"stats":{"Line":4}},{"line":104,"address":[],"length":0,"stats":{"Line":4}},{"line":106,"address":[],"length":0,"stats":{"Line":4}},{"line":107,"address":[],"length":0,"stats":{"Line":4}},{"line":109,"address":[],"length":0,"stats":{"Line":1}},{"line":111,"address":[],"length":0,"stats":{"Line":2}},{"line":112,"address":[],"length":0,"stats":{"Line":4}},{"line":113,"address":[],"length":0,"stats":{"Line":4}},{"line":114,"address":[],"length":0,"stats":{"Line":3}},{"line":115,"address":[],"length":0,"stats":{"Line":6}},{"line":116,"address":[],"length":0,"stats":{"Line":2}},{"line":117,"address":[],"length":0,"stats":{"Line":2}},{"line":118,"address":[],"length":0,"stats":{"Line":2}},{"line":119,"address":[],"length":0,"stats":{"Line":10}},{"line":120,"address":[],"length":0,"stats":{"Line":2}},{"line":122,"address":[],"length":0,"stats":{"Line":14}},{"line":124,"address":[],"length":0,"stats":{"Line":4}},{"line":126,"address":[],"length":0,"stats":{"Line":4}},{"line":127,"address":[],"length":0,"stats":{"Line":4}},{"line":128,"address":[],"length":0,"stats":{"Line":6}},{"line":130,"address":[],"length":0,"stats":{"Line":8}},{"line":131,"address":[],"length":0,"stats":{"Line":12}},{"line":132,"address":[],"length":0,"stats":{"Line":10}},{"line":133,"address":[],"length":0,"stats":{"Line":4}},{"line":134,"address":[],"length":0,"stats":{"Line":4}},{"line":137,"address":[],"length":0,"stats":{"Line":4}},{"line":139,"address":[],"length":0,"stats":{"Line":6}},{"line":140,"address":[],"length":0,"stats":{"Line":6}},{"line":141,"address":[],"length":0,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":1}},{"line":160,"address":[],"length":0,"stats":{"Line":1}},{"line":190,"address":[],"length":0,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":4}},{"line":192,"address":[],"length":0,"stats":{"Line":5}},{"line":194,"address":[],"length":0,"stats":{"Line":4}},{"line":196,"address":[],"length":0,"stats":{"Line":6}},{"line":197,"address":[],"length":0,"stats":{"Line":6}},{"line":199,"address":[],"length":0,"stats":{"Line":3}},{"line":200,"address":[],"length":0,"stats":{"Line":3}},{"line":202,"address":[],"length":0,"stats":{"Line":3}},{"line":203,"address":[],"length":0,"stats":{"Line":6}},{"line":204,"address":[],"length":0,"stats":{"Line":6}},{"line":205,"address":[],"length":0,"stats":{"Line":6}},{"line":206,"address":[],"length":0,"stats":{"Line":8}},{"line":207,"address":[],"length":0,"stats":{"Line":4}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":3}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":216,"address":[],"length":0,"stats":{"Line":3}},{"line":217,"address":[],"length":0,"stats":{"Line":6}},{"line":218,"address":[],"length":0,"stats":{"Line":4}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":6}},{"line":223,"address":[],"length":0,"stats":{"Line":6}},{"line":225,"address":[],"length":0,"stats":{"Line":8}},{"line":226,"address":[],"length":0,"stats":{"Line":6}},{"line":227,"address":[],"length":0,"stats":{"Line":10}},{"line":230,"address":[],"length":0,"stats":{"Line":10}},{"line":232,"address":[],"length":0,"stats":{"Line":10}},{"line":234,"address":[],"length":0,"stats":{"Line":10}},{"line":235,"address":[],"length":0,"stats":{"Line":10}},{"line":237,"address":[],"length":0,"stats":{"Line":2}},{"line":238,"address":[],"length":0,"stats":{"Line":6}},{"line":239,"address":[],"length":0,"stats":{"Line":4}},{"line":240,"address":[],"length":0,"stats":{"Line":2}},{"line":245,"address":[],"length":0,"stats":{"Line":3}},{"line":247,"address":[],"length":0,"stats":{"Line":9}},{"line":248,"address":[],"length":0,"stats":{"Line":6}},{"line":249,"address":[],"length":0,"stats":{"Line":8}},{"line":250,"address":[],"length":0,"stats":{"Line":4}},{"line":251,"address":[],"length":0,"stats":{"Line":4}},{"line":252,"address":[],"length":0,"stats":{"Line":4}},{"line":254,"address":[],"length":0,"stats":{"Line":10}},{"line":255,"address":[],"length":0,"stats":{"Line":8}},{"line":260,"address":[],"length":0,"stats":{"Line":1}}],"covered":75,"coverable":77},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","norm.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n/// RMSNorm (Root Mean Square Layer Normalization).\n///\n/// Normalizes the input tensor using the root mean square of the elements.\n/// Used in modern LLM architectures like Gemma and Llama.\n#[derive(Debug)]\npub struct RMSNorm\u003cT: TensorElem\u003e {\n    pub weight: Tensor\u003cT, 1, Cpu\u003e,\n    pub eps: T,\n}\n\nimpl\u003cT: TensorElem + Float\u003e RMSNorm\u003cT\u003e {\n    /// Creates a new RMSNorm layer.\n    ///\n    /// # Arguments\n    ///\n    /// * `weight` - The scale weights of shape `[features]`.\n    /// * `eps` - A small constant for numerical stability.\n    pub fn new(weight: Tensor\u003cT, 1, Cpu\u003e, eps: T) -\u003e Self {\n        Self { weight, eps }\n    }\n\n    /// Performs the forward pass of RMSNorm.\n    ///\n    /// Normalizes the input over the last dimension.\n    pub fn forward\u003cconst RANK: usize\u003e(\n        \u0026self,\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e {\n        let shape = x.shape();\n        let last_dim = shape[RANK - 1];\n        if last_dim != self.weight.shape()[0] {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![last_dim],\n                got: vec![self.weight.shape()[0]],\n            });\n        }\n\n        let mut out = Tensor::zeros(*shape);\n\n        out.data_mut()\n            .par_chunks_mut(last_dim)\n            .zip(x.data().par_chunks(last_dim))\n            .for_each(|(out_row, in_row)| {\n                let mut sum_sq = T::zero();\n                for \u0026val in in_row {\n                    sum_sq += val * val;\n                }\n                let mean_sq = sum_sq / T::from_usize(last_dim).unwrap();\n                let rsqrt = T::one() / (mean_sq + self.eps).sqrt();\n\n                for i in 0..last_dim {\n                    out_row[i] = in_row[i] * rsqrt * self.weight.data()[i];\n                }\n            });\n\n        Ok(out)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_rmsnorm_forward() {\n        // Input: [1.0, 2.0, 3.0]\n        // RMS = sqrt((1+4+9)/3) = sqrt(14/3) = sqrt(4.666) approx 2.1602\n        // Norm: [1/2.16, 2/2.16, 3/2.16] -\u003e [0.4629, 0.9258, 1.3887]\n        // Weight: [1, 1, 1] -\u003e Output same as norm\n\n        let input_data = vec![1.0, 2.0, 3.0];\n        let input = Tensor::\u003cf32, 1, Cpu\u003e::new(input_data, [3]).unwrap();\n\n        let weight = Tensor::\u003cf32, 1, Cpu\u003e::ones([3]);\n        let norm = RMSNorm::new(weight, 1e-5); // Small eps\n\n        let output = norm.forward(\u0026input).unwrap();\n        let out_data = output.data();\n\n        // Expected values calculation\n        let rms = (14.0f32 / 3.0).sqrt();\n        let expected = vec![1.0 / rms, 2.0 / rms, 3.0 / rms];\n\n        for (got, exp) in out_data.iter().zip(expected.iter()) {\n            assert!((got - exp).abs() \u003c 1e-4);\n        }\n    }\n\n    #[test]\n    fn test_rmsnorm_shape_mismatch() {\n        let weight = Tensor::\u003cf32, 1, Cpu\u003e::ones([4]); // Expect last dim 4\n        let norm = RMSNorm::new(weight, 1e-5);\n\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 3]); // Last dim 3\n        let res = norm.forward(\u0026input);\n        assert!(res.is_err());\n    }\n}\n","traces":[{"line":22,"address":[],"length":0,"stats":{"Line":7}},{"line":29,"address":[],"length":0,"stats":{"Line":9}},{"line":33,"address":[],"length":0,"stats":{"Line":27}},{"line":34,"address":[],"length":0,"stats":{"Line":18}},{"line":35,"address":[],"length":0,"stats":{"Line":27}},{"line":36,"address":[],"length":0,"stats":{"Line":1}},{"line":37,"address":[],"length":0,"stats":{"Line":3}},{"line":38,"address":[],"length":0,"stats":{"Line":3}},{"line":42,"address":[],"length":0,"stats":{"Line":24}},{"line":44,"address":[],"length":0,"stats":{"Line":8}},{"line":45,"address":[],"length":0,"stats":{"Line":16}},{"line":46,"address":[],"length":0,"stats":{"Line":32}},{"line":47,"address":[],"length":0,"stats":{"Line":23}},{"line":48,"address":[],"length":0,"stats":{"Line":30}},{"line":49,"address":[],"length":0,"stats":{"Line":2712}},{"line":50,"address":[],"length":0,"stats":{"Line":1798}},{"line":52,"address":[],"length":0,"stats":{"Line":60}},{"line":53,"address":[],"length":0,"stats":{"Line":45}},{"line":55,"address":[],"length":0,"stats":{"Line":1813}},{"line":56,"address":[],"length":0,"stats":{"Line":3596}},{"line":60,"address":[],"length":0,"stats":{"Line":8}}],"covered":21,"coverable":21},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","device.rs"],"content":"//! Device abstraction for Tensor storage.\n//!\n//! This module defines the `Device` trait and the `Cpu` device implementation.\n//! Devices determine where tensor data is allocated and how operations are executed.\n//!\n//! # ML Context\n//!\n//! In machine learning frameworks, a \"Device\" represents the hardware accelerator where\n//! computation happens. Common devices include:\n//! - **CPU**: Central Processing Unit (host memory). Good for sequential logic and small models.\n//! - **GPU**: Graphics Processing Unit (device memory). Excellent for massive parallel matrix operations.\n//! - **TPU**: Tensor Processing Unit. Specialized hardware for ML workloads.\n//!\n//! Abstracting the device allows the same neural network code to run on a laptop (CPU)\n//! or a cluster of GPUs without modification.\n\nuse crate::tensor::{Storage, TensorElem};\nuse std::fmt::Debug;\n\n/// A trait representing the underlying storage device for a Tensor.\n///\n/// Devices determine where the data is stored (e.g., CPU, GPU) and how it is accessed.\n/// Currently, only `Cpu` is implemented.\n///\n/// # Design\n///\n/// This trait is designed to be extensible. Future implementations could include `Cuda` or `Mps`\n/// devices. The `Storage` associated type allows each device to define its own memory container\n/// (e.g., `Vec\u003cT\u003e` for CPU, `CudaBuffer\u003cT\u003e` for GPU).\npub trait Device: Clone + Debug + PartialEq + Send + Sync {\n    /// The type of storage used by this device.\n    type Storage\u003cT\u003e: Storage\u003cT\u003e\n    where\n        T: TensorElem;\n\n    /// Returns the name of the device.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::tensor::{Cpu, Device};\n    /// let device = Cpu;\n    /// assert_eq!(device.name(), \"CPU\");\n    /// ```\n    fn name(\u0026self) -\u003e \u0026'static str;\n}\n\n/// A CPU Device.\n///\n/// Represents the standard system CPU. Data is stored in system RAM using `Vec\u003cT\u003e`.\n/// This is the default device for all tensors.\n///\n/// # Performance\n///\n/// Operations on the CPU are generally slower than GPU for large matrix multiplications,\n/// but `xla-rs` uses `rayon` to parallelize operations across all available CPU cores,\n/// providing decent performance for development and inference on smaller models.\n#[derive(Clone, Debug, PartialEq)]\npub struct Cpu;\n\nimpl Device for Cpu {\n    type Storage\u003cT\u003e\n        = Vec\u003cT\u003e\n    where\n        T: TensorElem;\n\n    fn name(\u0026self) -\u003e \u0026'static str {\n        \"CPU\"\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_cpu_device_name() {\n        let device = Cpu;\n        assert_eq!(device.name(), \"CPU\");\n    }\n\n    #[test]\n    fn test_cpu_device_traits() {\n        let device = Cpu;\n        let device_clone = device.clone();\n        assert_eq!(device, device_clone);\n        assert_eq!(format!(\"{:?}\", device), \"Cpu\");\n    }\n}\n","traces":[{"line":67,"address":[],"length":0,"stats":{"Line":2}},{"line":68,"address":[],"length":0,"stats":{"Line":2}}],"covered":2,"coverable":2},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","mod.rs"],"content":"//! Core Tensor implementation.\n//!\n//! This module defines the `Tensor` struct, which is the central data structure in `xla-rs`.\n//! It supports N-dimensional arrays, automatic differentiation (via the `autograd` module),\n//! and various mathematical operations.\n//!\n//! # Key Components\n//!\n//! - [`Tensor`]: The main struct representing an N-dimensional array.\n//! - [`TensorError`]: Error type for tensor operations.\n//! - [`TensorElem`]: Trait bound for elements that can be stored in a tensor.\n//!\n//! # ML Context\n//!\n//! Tensors are the fundamental data structure in deep learning. They generalize scalars (0D),\n//! vectors (1D), and matrices (2D) to N dimensions.\n//!\n//! - **0D**: Scalar (loss value).\n//! - **1D**: Vector (bias term, embedding).\n//! - **2D**: Matrix (weights, grayscale image).\n//! - **3D**: (RGB image, sequence of vectors).\n//! - **4D**: (Batch of RGB images).\n//!\n//! In `xla-rs`, tensors are strongly typed by element type `T` and rank `RANK`.\n//! This allows for some compile-time safety and optimization.\n//!\n//! # Examples\n//!\n//! ```rust\n//! use xla_rs::tensor::Tensor;\n//!\n//! let data = vec![1.0, 2.0, 3.0, 4.0];\n//! let tensor = Tensor::\u003cf32, 2\u003e::new(data, [2, 2]).unwrap();\n//! assert_eq!(tensor.shape(), \u0026[2, 2]);\n//! ```\n\nuse num_traits::{FromPrimitive, Num, NumAssign, ToPrimitive};\nuse std::fmt::Debug;\nuse thiserror::Error;\n\npub mod device;\npub mod ops;\npub mod storage;\n\npub use device::{Cpu, Device};\npub use storage::Storage;\n\n/// Error type for Tensor operations.\n#[derive(Error, Debug)]\npub enum TensorError {\n    /// The shape of the data does not match the expected shape.\n    #[error(\"Shape mismatch: expected {expected:?}, got {got:?}\")]\n    ShapeMismatch {\n        expected: Vec\u003cusize\u003e,\n        got: Vec\u003cusize\u003e,\n    },\n    /// Broadcasting is not possible between the given shapes.\n    #[error(\"Incompatible shapes for broadcasting: {0:?} and {1:?}\")]\n    BroadcastError(Vec\u003cusize\u003e, Vec\u003cusize\u003e),\n    /// An index is out of bounds for the given shape.\n    #[error(\"Index out of bounds: index {index:?} for shape {shape:?}\")]\n    IndexOutOfBounds {\n        index: Vec\u003cusize\u003e,\n        shape: Vec\u003cusize\u003e,\n    },\n    /// Operations between tensors on different devices are not allowed.\n    #[error(\"Device mismatch\")]\n    DeviceMismatch,\n    /// The requested operation is not supported (e.g., for a specific rank or type).\n    #[error(\"Unsupported operation: {0}\")]\n    Unsupported(String),\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, TensorError\u003e;\n\n/// Trait bound for elements that can be stored in a Tensor.\n///\n/// # Requirements\n/// - `Copy + Clone`: Essential for efficient storage in contiguous memory (e.g., `Vec\u003cT\u003e`) and fast element access.\n/// - `Num + ...`: Provides necessary numeric operations for tensor math.\n/// - `Send + Sync`: Required for parallel execution via `rayon`.\npub trait TensorElem:\n    Num + NumAssign + Copy + Clone + Debug + Send + Sync + FromPrimitive + ToPrimitive + PartialOrd\n{\n}\n\nimpl\u003cT\u003e TensorElem for T where\n    T: Num\n        + NumAssign\n        + Copy\n        + Clone\n        + Debug\n        + Send\n        + Sync\n        + FromPrimitive\n        + ToPrimitive\n        + PartialOrd\n{\n}\n\n/// The core Tensor struct.\n///\n/// Represents an N-dimensional array of elements.\n///\n/// # Generics\n///\n/// - `T`: The element type (must implement `TensorElem`).\n/// - `RANK`: The number of dimensions (const generic).\n/// - `D`: The device where data is stored (defaults to `Cpu`).\n/// # Design Philosophy: `const RANK` vs `const SHAPE`\n///\n/// The `Tensor` struct uses `const RANK: usize` rather than encoding the full shape in the type system\n/// (e.g., `Tensor\u003cT, [32, 128]\u003e`). This is a deliberate trade-off to balance **correctness** with **usability**.\n///\n/// **Why not `const SHAPE`?**\n/// - **Dynamic Batching:** Deep learning models often need to handle variable batch sizes (e.g., training with batch size 32,\n///   inference with batch size 1). Encoding shape in the type would require re-instantiating the model for every batch size.\n/// - **Complexity:** Operations like `reshape` or `matmul` would require complex type-level arithmetic, making compiler\n///   errors difficult to read and the API rigid.\n///\n/// **The Trade-off:**\n/// - **Pros:** We gain the flexibility to handle variable sequence lengths and batch sizes without recompilation.\n///   Function signatures remain readable (e.g., `fn forward(x: Tensor\u003cf32, 2\u003e)`).\n/// - **Cons:** Shape mismatches (e.g., multiplying `[32, 10]` by `[5, 20]`) are caught at runtime rather than compile-time.\n///\n/// This approach aligns with the industry standard for most deep learning frameworks (like PyTorch, TensorFlow, and many Rust crates),\n/// prioritizing the flexibility required for real-world model training and inference.\n#[derive(Clone)]\npub struct Tensor\u003cT, const RANK: usize, D: Device = Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    shape: [usize; RANK],\n    strides: [usize; RANK],\n    data: D::Storage\u003cT\u003e,\n    device: D,\n}\n\nimpl\u003cT, const RANK: usize\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    /// Creates a new Tensor from a vector of data and a shape.\n    ///\n    /// # Arguments\n    ///\n    /// * `data` - A flat vector containing the tensor elements.\n    /// * `shape` - An array representing the dimensions of the tensor.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::ShapeMismatch` if the length of `data` does not match the product of `shape`.\n    pub fn new(data: Vec\u003cT\u003e, shape: [usize; RANK]) -\u003e Result\u003cSelf\u003e {\n        let size: usize = shape.iter().product();\n        if data.len() != size {\n            return Err(TensorError::ShapeMismatch {\n                expected: vec![size],\n                got: vec![data.len()],\n            });\n        }\n\n        let strides = compute_strides(\u0026shape);\n        Ok(Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        })\n    }\n\n    /// Creates a new Tensor filled with zeros.\n    ///\n    /// # Arguments\n    ///\n    /// * `shape` - An array representing the dimensions of the tensor.\n    pub fn zeros(shape: [usize; RANK]) -\u003e Self {\n        let size: usize = shape.iter().product();\n        let data = vec![T::zero(); size];\n        let strides = compute_strides(\u0026shape);\n        Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        }\n    }\n\n    /// Creates a new Tensor filled with ones.\n    ///\n    /// # Arguments\n    ///\n    /// * `shape` - An array representing the dimensions of the tensor.\n    pub fn ones(shape: [usize; RANK]) -\u003e Self {\n        let size: usize = shape.iter().product();\n        let data = vec![T::one(); size];\n        let strides = compute_strides(\u0026shape);\n        Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        }\n    }\n\n    /// Reshapes the tensor to a new shape.\n    ///\n    /// The number of elements must remain the same.\n    ///\n    /// # Arguments\n    ///\n    /// * `new_shape` - The target shape.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::ShapeMismatch` if the total number of elements in `new_shape`\n    /// does not match the current size of the tensor.\n    pub fn reshape\u003cconst NEW_RANK: usize\u003e(\n        self,\n        new_shape: [usize; NEW_RANK],\n    ) -\u003e Result\u003cTensor\u003cT, NEW_RANK, Cpu\u003e\u003e {\n        let current_size: usize = self.shape.iter().product();\n        let new_size: usize = new_shape.iter().product();\n\n        if current_size != new_size {\n            return Err(TensorError::ShapeMismatch {\n                expected: vec![current_size],\n                got: vec![new_size],\n            });\n        }\n\n        let strides = compute_strides(\u0026new_shape);\n        Ok(Tensor {\n            shape: new_shape,\n            strides,\n            data: self.data,\n            device: self.device,\n        })\n    }\n}\n\n/// Computes the strides for a given shape.\n///\n/// Strides represent the number of elements to skip in memory to move to the next element\n/// along a specific dimension. This implementation assumes a row-major (C-style) memory layout.\n///\n/// # Arguments\n///\n/// * `shape` - The shape of the tensor.\n///\n/// # Returns\n///\n/// An array of strides corresponding to the input shape.\nfn compute_strides\u003cconst RANK: usize\u003e(shape: \u0026[usize; RANK]) -\u003e [usize; RANK] {\n    let mut strides = [0; RANK];\n    let mut stride = 1;\n    for i in (0..RANK).rev() {\n        strides[i] = stride;\n        stride *= shape[i];\n    }\n    strides\n}\n\nimpl\u003cT, const RANK: usize, D: Device\u003e Tensor\u003cT, RANK, D\u003e\nwhere\n    T: TensorElem,\n{\n    /// Returns the shape of the tensor.\n    ///\n    /// The shape is an array of length `RANK` representing the size of each dimension.\n    pub fn shape(\u0026self) -\u003e \u0026[usize; RANK] {\n        \u0026self.shape\n    }\n\n    /// Returns the strides of the tensor.\n    ///\n    /// Strides represent the number of elements to skip in memory to move to the next element\n    /// along a specific dimension.\n    pub fn strides(\u0026self) -\u003e \u0026[usize; RANK] {\n        \u0026self.strides\n    }\n\n    /// Returns a reference to the underlying data as a slice.\n    pub fn data(\u0026self) -\u003e \u0026[T] {\n        self.data.as_slice()\n    }\n\n    /// Returns a mutable reference to the underlying data as a slice.\n    ///\n    /// # Warning\n    ///\n    /// Modifying the data directly can be dangerous if you violate invariants (though `Tensor`\n    /// doesn't have many invariants on the values themselves). Use with caution.\n    pub fn data_mut(\u0026mut self) -\u003e \u0026mut [T] {\n        self.data.as_mut_slice()\n    }\n\n    /// Returns the total number of elements in the tensor.\n    ///\n    /// This is equal to the product of the dimensions in the shape.\n    pub fn size(\u0026self) -\u003e usize {\n        self.shape.iter().product()\n    }\n}\n\nimpl\u003cT, const RANK: usize, D: Device\u003e Debug for Tensor\u003cT, RANK, D\u003e\nwhere\n    T: TensorElem,\n{\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        f.debug_struct(\"Tensor\")\n            .field(\"shape\", \u0026self.shape)\n            .field(\"device\", \u0026self.device.name())\n            .field(\"data_len\", \u0026self.data.len())\n            .finish()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_tensor_creation() {\n        // Positive case\n        let data = vec![1.0, 2.0, 3.0, 4.0];\n        let tensor = Tensor::\u003cf32, 2\u003e::new(data.clone(), [2, 2]).unwrap();\n        assert_eq!(tensor.shape(), \u0026[2, 2]);\n        assert_eq!(tensor.data(), \u0026data[..]);\n\n        // Negative case: Size mismatch\n        let err = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 2.0, 3.0], [2, 2]);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_zeros_ones() {\n        let zeros = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        assert_eq!(zeros.data(), \u0026[0.0; 6]);\n\n        let ones = Tensor::\u003cf32, 2\u003e::ones([2, 3]);\n        assert_eq!(ones.data(), \u0026[1.0; 6]);\n    }\n\n    #[test]\n    fn test_reshape() {\n        let tensor = Tensor::\u003cf32, 2\u003e::zeros([2, 3]); // 6 elements\n\n        // Valid reshape\n        let reshaped = tensor.reshape([3, 2]).unwrap();\n        assert_eq!(reshaped.shape(), \u0026[3, 2]);\n\n        // Invalid reshape\n        let err = reshaped.clone().reshape([4, 2]); // 8 elements\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_arithmetic() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let b = Tensor::\u003cf32, 1\u003e::new(vec![3.0, 4.0], [2]).unwrap();\n\n        // Add\n        let c = (\u0026a + \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[4.0, 6.0]);\n\n        // Mul\n        let d = (\u0026a * \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[3.0, 8.0]);\n\n        // Mismatch\n        let _e = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let f = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let err = \u0026a + \u0026f;\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_matmul_2d() {\n        // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n        let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n        let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n        let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2]);\n\n        // Row 0: 1*7 + 2*9 + 3*2 = 7 + 18 + 6 = 31\n        // Row 0, Col 1: 1*8 + 2*1 + 3*3 = 8 + 2 + 9 = 19\n        // Row 1: 4*7 + 5*9 + 6*2 = 28 + 45 + 12 = 85\n        // Row 1, Col 1: 4*8 + 5*1 + 6*3 = 32 + 5 + 18 = 55\n        assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n    }\n\n    #[test]\n    fn test_matmul_broadcast_error() {\n        let a = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        let b = Tensor::\u003cf32, 2\u003e::zeros([4, 2]); // K mismatch (3 vs 4)\n\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_transpose() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n        // [ 1 2 3 ]\n        // [ 4 5 6 ]\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[3, 2]);\n        // [ 1 4 ]\n        // [ 2 5 ]\n        // [ 3 6 ]\n        assert_eq!(t_t.data(), \u0026[1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_axes() {\n        // Rank 4 tensor [B, S, H, D] -\u003e [B, H, S, D]\n        // Shape: [1, 2, 2, 2] -\u003e [1, 2, 2, 2] for simplicity but distinct values\n        let data: Vec\u003cf32\u003e = (0..8).map(|i| i as f32).collect();\n\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 2]).unwrap();\n\n        let permuted = t.transpose_axes(1, 2).unwrap();\n        assert_eq!(permuted.shape(), \u0026[1, 2, 2, 2]); // H, S swapped but sizes same\n\n        assert_eq!(permuted.data(), \u0026[0.0, 1.0, 4.0, 5.0, 2.0, 3.0, 6.0, 7.0]);\n    }\n\n    #[test]\n    fn test_macro() {\n        let t = tensor!([1.0, 2.0, 3.0, 4.0], [2, 2]);\n        assert_eq!(t.shape(), \u0026[2, 2]);\n        assert_eq!(t.data(), \u0026[1.0, 2.0, 3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor_accessors() {\n        let mut t = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        assert_eq!(t.size(), 6);\n        assert_eq!(t.strides(), \u0026[3, 1]);\n\n        // Test data_mut\n        {\n            let data = t.data_mut();\n            data[0] = 1.0;\n        }\n        assert_eq!(t.data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_compute_strides() {\n        let shape = [2, 3, 4];\n        let strides = compute_strides(\u0026shape);\n        // Stride for dim 2 (last) is 1\n        // Stride for dim 1 is 4\n        // Stride for dim 0 is 3 * 4 = 12\n        assert_eq!(strides, [12, 4, 1]);\n    }\n\n    #[test]\n    fn test_tensor_error_display() {\n        let err = TensorError::ShapeMismatch {\n            expected: vec![2, 2],\n            got: vec![4],\n        };\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Shape mismatch: expected [2, 2], got [4]\"\n        );\n\n        let err = TensorError::BroadcastError(vec![2, 3], vec![2, 2]);\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Incompatible shapes for broadcasting: [2, 3] and [2, 2]\"\n        );\n\n        let err = TensorError::IndexOutOfBounds {\n            index: vec![3],\n            shape: vec![2],\n        };\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Index out of bounds: index [3] for shape [2]\"\n        );\n\n        let err = TensorError::DeviceMismatch;\n        assert_eq!(format!(\"{}\", err), \"Device mismatch\");\n\n        let err = TensorError::Unsupported(\"foo\".to_string());\n        assert_eq!(format!(\"{}\", err), \"Unsupported operation: foo\");\n    }\n\n    #[test]\n    fn test_tensor_clone() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let t2 = t.clone();\n        assert_eq!(t.data(), t2.data());\n        assert_eq!(t.shape(), t2.shape());\n    }\n\n    #[test]\n    fn test_tensor_debug() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let debug_str = format!(\"{:?}\", t);\n        assert!(debug_str.contains(\"Tensor\"));\n        assert!(debug_str.contains(\"shape\"));\n        assert!(debug_str.contains(\"device\"));\n        assert!(debug_str.contains(\"CPU\"));\n    }\n}\n","traces":[{"line":153,"address":[],"length":0,"stats":{"Line":60124}},{"line":154,"address":[],"length":0,"stats":{"Line":300620}},{"line":155,"address":[],"length":0,"stats":{"Line":120248}},{"line":156,"address":[],"length":0,"stats":{"Line":1}},{"line":157,"address":[],"length":0,"stats":{"Line":3}},{"line":158,"address":[],"length":0,"stats":{"Line":2}},{"line":162,"address":[],"length":0,"stats":{"Line":180369}},{"line":163,"address":[],"length":0,"stats":{"Line":60123}},{"line":164,"address":[],"length":0,"stats":{"Line":120246}},{"line":165,"address":[],"length":0,"stats":{"Line":120246}},{"line":166,"address":[],"length":0,"stats":{"Line":60123}},{"line":167,"address":[],"length":0,"stats":{"Line":60123}},{"line":176,"address":[],"length":0,"stats":{"Line":320218}},{"line":177,"address":[],"length":0,"stats":{"Line":1601090}},{"line":178,"address":[],"length":0,"stats":{"Line":1280872}},{"line":179,"address":[],"length":0,"stats":{"Line":960654}},{"line":193,"address":[],"length":0,"stats":{"Line":20055}},{"line":194,"address":[],"length":0,"stats":{"Line":100275}},{"line":195,"address":[],"length":0,"stats":{"Line":80220}},{"line":196,"address":[],"length":0,"stats":{"Line":60165}},{"line":217,"address":[],"length":0,"stats":{"Line":175}},{"line":221,"address":[],"length":0,"stats":{"Line":875}},{"line":222,"address":[],"length":0,"stats":{"Line":875}},{"line":224,"address":[],"length":0,"stats":{"Line":175}},{"line":225,"address":[],"length":0,"stats":{"Line":1}},{"line":226,"address":[],"length":0,"stats":{"Line":3}},{"line":227,"address":[],"length":0,"stats":{"Line":1}},{"line":231,"address":[],"length":0,"stats":{"Line":522}},{"line":232,"address":[],"length":0,"stats":{"Line":174}},{"line":233,"address":[],"length":0,"stats":{"Line":348}},{"line":234,"address":[],"length":0,"stats":{"Line":348}},{"line":235,"address":[],"length":0,"stats":{"Line":174}},{"line":236,"address":[],"length":0,"stats":{"Line":174}},{"line":253,"address":[],"length":0,"stats":{"Line":600751}},{"line":254,"address":[],"length":0,"stats":{"Line":1201502}},{"line":255,"address":[],"length":0,"stats":{"Line":1201502}},{"line":256,"address":[],"length":0,"stats":{"Line":3604956}},{"line":257,"address":[],"length":0,"stats":{"Line":1201727}},{"line":258,"address":[],"length":0,"stats":{"Line":1201727}},{"line":260,"address":[],"length":0,"stats":{"Line":600751}},{"line":270,"address":[],"length":0,"stats":{"Line":20235}},{"line":271,"address":[],"length":0,"stats":{"Line":20235}},{"line":278,"address":[],"length":0,"stats":{"Line":2}},{"line":279,"address":[],"length":0,"stats":{"Line":2}},{"line":283,"address":[],"length":0,"stats":{"Line":101722}},{"line":284,"address":[],"length":0,"stats":{"Line":101722}},{"line":293,"address":[],"length":0,"stats":{"Line":80056}},{"line":294,"address":[],"length":0,"stats":{"Line":80056}},{"line":300,"address":[],"length":0,"stats":{"Line":1}},{"line":301,"address":[],"length":0,"stats":{"Line":3}},{"line":309,"address":[],"length":0,"stats":{"Line":1}},{"line":310,"address":[],"length":0,"stats":{"Line":5}},{"line":311,"address":[],"length":0,"stats":{"Line":4}},{"line":312,"address":[],"length":0,"stats":{"Line":4}},{"line":313,"address":[],"length":0,"stats":{"Line":3}}],"covered":55,"coverable":55},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","ops.rs"],"content":"//! Tensor operations.\n//!\n//! This module implements various mathematical operations for Tensors, including\n//! element-wise arithmetic (Add, Sub, Mul, Div), matrix multiplication, and broadcasting.\n//! It uses `rayon` for parallel execution on the CPU.\n\nuse super::{Cpu, Result, Tensor, TensorElem, TensorError};\nuse rayon::prelude::*;\nuse std::ops::{Add, Div, Mul, Sub};\n\n/// Implements a binary arithmetic operation trait (e.g., `Add`, `Sub`) for `\u0026Tensor`.\n///\n/// This macro handles the boilerplate of checking shape compatibility, creating a new\n/// output tensor, and performing the element-wise operation in parallel using `rayon`.\n///\n/// # Arguments\n///\n/// * `$trait` - The trait to implement (e.g., `Add`).\n/// * `$method` - The method name of the trait (e.g., `add`).\nmacro_rules! impl_bin_op {\n    ($trait:ident, $method:ident) =\u003e {\n        impl\u003cT, const RANK: usize\u003e $trait for \u0026Tensor\u003cT, RANK, Cpu\u003e\n        where\n            T: TensorElem,\n        {\n            type Output = Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e;\n\n            fn $method(self, rhs: Self) -\u003e Self::Output {\n                if self.shape != rhs.shape {\n                    return Err(TensorError::ShapeMismatch {\n                        expected: self.shape.to_vec(),\n                        got: rhs.shape.to_vec(),\n                    });\n                }\n\n                let mut out = Tensor::zeros(self.shape);\n                // Seamless parallelism using rayon\n                out.data\n                    .as_mut_slice()\n                    .par_iter_mut()\n                    .zip(self.data.as_slice().par_iter())\n                    .zip(rhs.data.as_slice().par_iter())\n                    .for_each(|((o, a), b)| {\n                        *o = a.$method(*b);\n                    });\n\n                Ok(out)\n            }\n        }\n    };\n}\n\nimpl_bin_op!(Add, add);\nimpl_bin_op!(Sub, sub);\nimpl_bin_op!(Mul, mul);\nimpl_bin_op!(Div, div);\n\nimpl\u003cT, const RANK: usize\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    /// Applies a function element-wise to the tensor.\n    ///\n    /// Creates a new tensor with the same shape, where each element is the result of applying\n    /// the closure `f` to the corresponding element in the input tensor.\n    ///\n    /// # Arguments\n    ///\n    /// * `f` - A closure that takes an element of type `T` and returns a value of type `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::tensor::Tensor;\n    /// let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n    /// let squared = t.map(|x| x * x);\n    /// assert_eq!(squared.data(), \u0026[1.0, 4.0, 9.0]);\n    /// ```\n    pub fn map\u003cF\u003e(\u0026self, f: F) -\u003e Self\n    where\n        F: Fn(T) -\u003e T + Sync + Send,\n    {\n        let mut out = Tensor::zeros(self.shape);\n        out.data\n            .as_mut_slice()\n            .par_iter_mut()\n            .zip(self.data.as_slice().par_iter())\n            .for_each(|(o, i)| *o = f(*i));\n        out\n    }\n\n    /// Matrix Multiplication.\n    /// Supports:\n    /// - 2D x 2D: [M, K] x [K, N] -\u003e [M, N]\n    /// - 3D x 3D: [B, M, K] x [B, K, N] -\u003e [B, M, N] (Batched Matmul)\n    ///\n    /// Performs matrix multiplication on the last two dimensions of the tensors.\n    /// If the rank is greater than 2, the leading dimensions are treated as batch dimensions.\n    /// This is known as **Batched Matrix Multiplication**.\n    ///\n    /// # Mathematical Definition\n    ///\n    /// For tensors of rank $N$, this operation corresponds to the Einstein summation:\n    /// `...mk,...kn-\u003e...mn`\n    ///\n    /// # Examples\n    ///\n    /// - **Rank 2 (Matrix x Matrix)**: `[M, K] x [K, N] -\u003e [M, N]`\n    /// - **Rank 3 (Batch x Matrix x Matrix)**: `[B, M, K] x [B, K, N] -\u003e [B, M, N]`\n    /// - **Rank 4**: `[B, H, M, K] x [B, H, K, N] -\u003e [B, H, M, N]`\n    pub fn matmul(\u0026self, rhs: \u0026Self) -\u003e Result\u003cSelf\u003e {\n        // Compile-time check\n        const { assert!(RANK \u003e= 2, \"Matmul requires rank \u003e= 2\") };\n        self.matmul_impl(rhs)\n    }\n\n    /// Internal implementation of Matrix Multiplication.\n    fn matmul_impl(\u0026self, rhs: \u0026Self) -\u003e Result\u003cSelf\u003e {\n        let m = self.shape[RANK - 2];\n        let k = self.shape[RANK - 1];\n        let k2 = rhs.shape[RANK - 2];\n        let n = rhs.shape[RANK - 1];\n\n        // Check batch dimensions\n        if self.shape[..RANK - 2] != rhs.shape[..RANK - 2] {\n            return Err(TensorError::ShapeMismatch {\n                expected: self.shape.to_vec(),\n                got: rhs.shape.to_vec(),\n            });\n        }\n\n        let mut out_shape = self.shape;\n        out_shape[RANK - 2] = m;\n        out_shape[RANK - 1] = n;\n\n        // Delegate to the kernel\n        // This is where you would swap in a BLAS call or other accelerator\n        let out_data = xla_rs_kernels::cpu_matmul(\n            self.data.as_slice(),\n            rhs.data.as_slice(),\n            \u0026self.shape,\n            \u0026rhs.shape,\n        )\n        .map_err(|e| match e {\n            xla_rs_kernels::KernelError::ShapeMismatch { expected, got } =\u003e {\n                TensorError::ShapeMismatch { expected, got }\n            }\n        })?;\n\n        let strides = crate::tensor::compute_strides(\u0026out_shape);\n        Ok(Tensor {\n            shape: out_shape,\n            strides,\n            data: out_data,\n            device: Cpu,\n        })\n    }\n\n    /// Transposes the tensor.\n    ///\n    /// Swaps the last two dimensions.\n    /// - For 2D tensors (matrices), this is a standard matrix transpose.\n    /// - For N-D tensors, it swaps the last two axes (e.g., [B, M, N] -\u003e [B, N, M]).\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::Unsupported` if the tensor rank is less than 2.\n    pub fn transpose(\u0026self) -\u003e Result\u003cSelf\u003e {\n        if RANK \u003c 2 {\n            return Err(TensorError::Unsupported(\n                \"Transpose requires rank \u003e= 2\".into(),\n            ));\n        }\n\n        let mut new_shape = self.shape;\n        new_shape.swap(RANK - 1, RANK - 2);\n\n        // Delegate to the kernel\n        let out_data = xla_rs_kernels::cpu_transpose(self.data.as_slice(), \u0026self.shape).map_err(\n            |e| match e {\n                xla_rs_kernels::KernelError::ShapeMismatch { expected, got } =\u003e {\n                    TensorError::ShapeMismatch { expected, got }\n                }\n            },\n        )?;\n\n        let strides = crate::tensor::compute_strides(\u0026new_shape);\n        Ok(Tensor {\n            shape: new_shape,\n            strides,\n            data: out_data,\n            device: Cpu,\n        })\n    }\n\n    /// Transposes two specific axes of the tensor.\n    ///\n    /// This operation creates a new tensor with the data physically permuted to match the new shape.\n    ///\n    /// # Arguments\n    ///\n    /// * `ax1` - The first axis to swap.\n    /// * `ax2` - The second axis to swap.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::IndexOutOfBounds` if `ax1` or `ax2` are out of bounds.\n    /// Returns `TensorError::Unsupported` for complex permutations not yet optimized.\n    pub fn transpose_axes(\u0026self, ax1: usize, ax2: usize) -\u003e Result\u003cSelf\u003e {\n        if ax1 \u003e= RANK || ax2 \u003e= RANK {\n            return Err(TensorError::IndexOutOfBounds {\n                index: vec![ax1, ax2],\n                shape: self.shape.to_vec(),\n            });\n        }\n        if ax1 == ax2 {\n            return Ok(self.clone());\n        }\n\n        let mut new_shape = self.shape;\n        new_shape.swap(ax1, ax2);\n\n        // Compute new strides based on old strides\n        // But since we own data, we can't just change strides without permuting data if we want it contiguous.\n        // My Tensor struct enforces contiguous storage (Vec\u003cT\u003e).\n        // So we must physically move data.\n\n        let mut out = Tensor::zeros(new_shape);\n\n        // Generic permute is hard. Implementing for specific cases used in Attention.\n        // Case: [B, S, H, D] -\u003e [B, H, S, D] (swap 1 and 2) where RANK=4.\n\n        if RANK == 4 \u0026\u0026 ((ax1 == 1 \u0026\u0026 ax2 == 2) || (ax1 == 2 \u0026\u0026 ax2 == 1)) {\n            let [_, _, _, _] = self.shape[0..4].try_into().unwrap();\n            // Input: B, S, H, D. Output: B, H, S, D.\n            // We want to write to [b, h, s, d].\n            // Iterate output\n            let out_s = out.strides;\n            let in_s = self.strides;\n\n            // We can use a generic loop with recursion or specific nested loops.\n            // Nested loops for 4D.\n            // Parallelize on B.\n\n            // Check if ax1/ax2 match expectation.\n            // The shape in `self` is [B, S, H, D] (if coming from reshape).\n            // If we swap 1 and 2, we get [B, H, S, D].\n\n            // But let's use the strides to be generic for 4D.\n\n            let out_ptr = out.data.as_mut_slice();\n            let in_ptr = self.data.as_slice();\n\n            // Parallelize outer dim\n            out_ptr\n                .par_chunks_mut(out_s[0])\n                .enumerate()\n                .for_each(|(i, batch_chunk)| {\n                    // i is batch index\n                    // batch_chunk is [H, S, D] size flattened\n                    // We need to iterate over H, S, D\n                    let new_dim1 = new_shape[1]; // H\n                    let new_dim2 = new_shape[2]; // S\n                    let new_dim3 = new_shape[3]; // D\n\n                    for j in 0..new_dim1 {\n                        for k in 0..new_dim2 {\n                            for l in 0..new_dim3 {\n                                // Output index: i, j, k, l (linear: i*s0 + j*s1 + k*s2 + l*s3)\n                                // This chunk corresponds to `i`. so offset is j*out_s[1] + ...\n                                let out_idx = j * out_s[1] + k * out_s[2] + l * out_s[3];\n\n                                // Input index: i, k, j, l (swapped j and k compared to output dims mapping)\n                                // Wait, we swapped axes.\n                                // Input indices:\n                                // idx[ax1] = out_idx[ax2]\n                                // idx[ax2] = out_idx[ax1]\n                                // indices: [i, k, j, l] if we swapped 1 and 2.\n                                // Input stride:\n                                let in_idx = i * in_s[0] + k * in_s[1] + j * in_s[2] + l * in_s[3];\n\n                                batch_chunk[out_idx] = in_ptr[in_idx];\n                            }\n                        }\n                    }\n                });\n            Ok(out)\n        } else {\n            Err(TensorError::Unsupported(format!(\n                \"General transpose_axes not impl for rank {} axes {},{}\",\n                RANK, ax1, ax2\n            )))\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_arithmetic() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let b = Tensor::\u003cf32, 1\u003e::new(vec![3.0, 4.0], [2]).unwrap();\n\n        // Add\n        let c = (\u0026a + \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[4.0, 6.0]);\n\n        // Sub\n        let c = (\u0026a - \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[-2.0, -2.0]);\n\n        // Mul\n        let d = (\u0026a * \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[3.0, 8.0]);\n\n        // Div\n        let d = (\u0026a / \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[1.0 / 3.0, 2.0 / 4.0]);\n\n        // Mismatch\n        let _e = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let f = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let err = \u0026a + \u0026f;\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_map() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let b = a.map(|x| x * 2.0);\n        assert_eq!(b.data(), \u0026[2.0, 4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_matmul_2d() {\n        // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n        let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n        let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n        let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2]);\n\n        // Row 0: 1*7 + 2*9 + 3*2 = 7 + 18 + 6 = 31\n        // Row 0, Col 1: 1*8 + 2*1 + 3*3 = 8 + 2 + 9 = 19\n        // Row 1: 4*7 + 5*9 + 6*2 = 28 + 45 + 12 = 85\n        // Row 1, Col 1: 4*8 + 5*1 + 6*3 = 32 + 5 + 18 = 55\n        assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n    }\n\n    #[test]\n    fn test_matmul_3d() {\n        // Batched Matmul: [B, M, K] x [B, K, N] -\u003e [B, M, N]\n        // B=2, M=2, K=2, N=2\n        // Batch 1: Identity * Identity = Identity\n        // Batch 2: 2*Identity * 3*Identity = 6*Identity\n\n        let batch1_a = vec![1.0, 0.0, 0.0, 1.0]; // Identity\n        let batch2_a = vec![2.0, 0.0, 0.0, 2.0]; // 2*Identity\n        let mut a_data = batch1_a;\n        a_data.extend(batch2_a);\n        let a = Tensor::\u003cf32, 3\u003e::new(a_data, [2, 2, 2]).unwrap();\n\n        let batch1_b = vec![1.0, 0.0, 0.0, 1.0]; // Identity\n        let batch2_b = vec![3.0, 0.0, 0.0, 3.0]; // 3*Identity\n        let mut b_data = batch1_b;\n        b_data.extend(batch2_b);\n        let b = Tensor::\u003cf32, 3\u003e::new(b_data, [2, 2, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2, 2]);\n\n        let expected_batch1 = vec![1.0, 0.0, 0.0, 1.0];\n        let expected_batch2 = vec![6.0, 0.0, 0.0, 6.0];\n        let mut expected = expected_batch1;\n        expected.extend(expected_batch2);\n\n        assert_eq!(c.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_matmul_broadcast_error() {\n        let a = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        let b = Tensor::\u003cf32, 2\u003e::zeros([4, 2]); // K mismatch (3 vs 4)\n\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_transpose() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n        // [ 1 2 3 ]\n        // [ 4 5 6 ]\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[3, 2]);\n        // [ 1 4 ]\n        // [ 2 5 ]\n        // [ 3 6 ]\n        assert_eq!(t_t.data(), \u0026[1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_axes() {\n        // Rank 4 tensor [B, S, H, D] -\u003e [B, H, S, D]\n        // Shape: [1, 2, 2, 2] -\u003e [1, 2, 2, 2] for simplicity but distinct values\n        let data: Vec\u003cf32\u003e = (0..8).map(|i| i as f32).collect();\n\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 2]).unwrap();\n\n        let permuted = t.transpose_axes(1, 2).unwrap();\n        assert_eq!(permuted.shape(), \u0026[1, 2, 2, 2]); // H, S swapped but sizes same\n\n        assert_eq!(permuted.data(), \u0026[0.0, 1.0, 4.0, 5.0, 2.0, 3.0, 6.0, 7.0]);\n    }\n\n    #[test]\n    fn test_matmul_4d() {\n        // [B, S, M, K] x [B, S, K, N] -\u003e [B, S, M, N]\n        // Shape: [1, 2, 2, 2] x [1, 2, 2, 2] -\u003e [1, 2, 2, 2]\n        // Batch 1, Seq 1: Identity * Identity = Identity\n        // Batch 1, Seq 2: 2*Identity * 3*Identity = 6*Identity\n\n        let s1_a = vec![1.0, 0.0, 0.0, 1.0];\n        let s2_a = vec![2.0, 0.0, 0.0, 2.0];\n        let mut a_data = s1_a;\n        a_data.extend(s2_a);\n        let a = Tensor::\u003cf32, 4\u003e::new(a_data, [1, 2, 2, 2]).unwrap();\n\n        let s1_b = vec![1.0, 0.0, 0.0, 1.0];\n        let s2_b = vec![3.0, 0.0, 0.0, 3.0];\n        let mut b_data = s1_b;\n        b_data.extend(s2_b);\n        let b = Tensor::\u003cf32, 4\u003e::new(b_data, [1, 2, 2, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[1, 2, 2, 2]);\n\n        let expected_s1 = vec![1.0, 0.0, 0.0, 1.0];\n        let expected_s2 = vec![6.0, 0.0, 0.0, 6.0];\n        let mut expected = expected_s1;\n        expected.extend(expected_s2);\n\n        assert_eq!(c.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_matmul_10d() {\n        // 10D Tensor\n        // Shape: [1, ..., 1, 2, 2] (8 ones, then 2, 2)\n        let shape = [1, 1, 1, 1, 1, 1, 1, 1, 2, 2];\n        let data = vec![1.0, 2.0, 3.0, 4.0]; // [1 2; 3 4]\n        let a = Tensor::\u003cf32, 10\u003e::new(data.clone(), shape).unwrap();\n        let b = Tensor::\u003cf32, 10\u003e::new(data, shape).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026shape);\n\n        // [1 2; 3 4] * [1 2; 3 4] = [7 10; 15 22]\n        assert_eq!(c.data(), \u0026[7.0, 10.0, 15.0, 22.0]);\n    }\n\n    #[test]\n    fn test_transpose_4d() {\n        // [B, S, M, N] -\u003e [B, S, N, M]\n        // [1, 2, 2, 3] -\u003e [1, 2, 3, 2]\n        let data: Vec\u003cf32\u003e = (0..12).map(|i| i as f32).collect();\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 3]).unwrap();\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[1, 2, 3, 2]);\n\n        // First matrix (0..6):\n        // [0 1 2]\n        // [3 4 5]\n        // Transpose:\n        // [0 3]\n        // [1 4]\n        // [2 5]\n        // -\u003e 0, 3, 1, 4, 2, 5\n\n        // Second matrix (6..12):\n        // [6 7 8]\n        // [9 10 11]\n        // Transpose:\n        // [6 9]\n        // [7 10]\n        // [8 11]\n        // -\u003e 6, 9, 7, 10, 8, 11\n\n        let expected = vec![0.0, 3.0, 1.0, 4.0, 2.0, 5.0, 6.0, 9.0, 7.0, 10.0, 8.0, 11.0];\n        assert_eq!(t_t.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_transpose_10d() {\n        // 10D Tensor\n        // Shape: [1, ..., 1, 2, 3]\n        let shape = [1, 1, 1, 1, 1, 1, 1, 1, 2, 3];\n        let data: Vec\u003cf32\u003e = (0..6).map(|i| i as f32).collect();\n        let t = Tensor::\u003cf32, 10\u003e::new(data, shape).unwrap();\n\n        let t_t = t.transpose().unwrap();\n        let expected_shape = [1, 1, 1, 1, 1, 1, 1, 1, 3, 2];\n        assert_eq!(t_t.shape(), \u0026expected_shape);\n\n        // [0 1 2]\n        // [3 4 5]\n        // -\u003e\n        // [0 3]\n        // [1 4]\n        // [2 5]\n        assert_eq!(t_t.data(), \u0026[0.0, 3.0, 1.0, 4.0, 2.0, 5.0]);\n    }\n\n    #[test]\n    fn test_transpose_error() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let err = t.transpose();\n        assert!(matches!(err, Err(TensorError::Unsupported(_))));\n    }\n\n    #[test]\n    fn test_transpose_axes_error() {\n        let t = Tensor::\u003cf32, 2\u003e::zeros([2, 2]);\n        let err = t.transpose_axes(0, 2); // 2 is out of bounds\n        assert!(matches!(err, Err(TensorError::IndexOutOfBounds { .. })));\n    }\n\n    #[test]\n    fn test_transpose_axes_identity() {\n        let t = Tensor::\u003cf32, 2\u003e::zeros([2, 2]);\n        let t2 = t.transpose_axes(0, 0).unwrap();\n        assert_eq!(t.shape(), t2.shape());\n    }\n\n    #[test]\n    fn test_transpose_axes_unsupported() {\n        let t = Tensor::\u003cf32, 3\u003e::zeros([2, 2, 2]);\n        // 3D transpose axes not fully implemented in the simplified logic\n        let err = t.transpose_axes(0, 1);\n        assert!(matches!(err, Err(TensorError::Unsupported(_))));\n    }\n\n    #[test]\n    fn test_matmul_batch_mismatch() {\n        let a = Tensor::\u003cf32, 3\u003e::zeros([2, 2, 2]);\n        let b = Tensor::\u003cf32, 3\u003e::zeros([3, 2, 2]); // Batch size 3 vs 2\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n}\n","traces":[{"line":28,"address":[],"length":0,"stats":{"Line":240086}},{"line":29,"address":[],"length":0,"stats":{"Line":240086}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":6}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":720252}},{"line":38,"address":[],"length":0,"stats":{"Line":240084}},{"line":39,"address":[],"length":0,"stats":{"Line":240084}},{"line":40,"address":[],"length":0,"stats":{"Line":240084}},{"line":41,"address":[],"length":0,"stats":{"Line":720252}},{"line":42,"address":[],"length":0,"stats":{"Line":720252}},{"line":43,"address":[],"length":0,"stats":{"Line":601595}},{"line":44,"address":[],"length":0,"stats":{"Line":361511}},{"line":47,"address":[],"length":0,"stats":{"Line":240084}},{"line":79,"address":[],"length":0,"stats":{"Line":80024}},{"line":83,"address":[],"length":0,"stats":{"Line":240072}},{"line":84,"address":[],"length":0,"stats":{"Line":80024}},{"line":87,"address":[],"length":0,"stats":{"Line":240072}},{"line":88,"address":[],"length":0,"stats":{"Line":280929}},{"line":89,"address":[],"length":0,"stats":{"Line":80024}},{"line":111,"address":[],"length":0,"stats":{"Line":120107}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":360321}},{"line":118,"address":[],"length":0,"stats":{"Line":120107}},{"line":119,"address":[],"length":0,"stats":{"Line":240214}},{"line":120,"address":[],"length":0,"stats":{"Line":240214}},{"line":121,"address":[],"length":0,"stats":{"Line":240214}},{"line":122,"address":[],"length":0,"stats":{"Line":240214}},{"line":125,"address":[],"length":0,"stats":{"Line":240214}},{"line":126,"address":[],"length":0,"stats":{"Line":1}},{"line":127,"address":[],"length":0,"stats":{"Line":3}},{"line":128,"address":[],"length":0,"stats":{"Line":1}},{"line":132,"address":[],"length":0,"stats":{"Line":240212}},{"line":133,"address":[],"length":0,"stats":{"Line":120106}},{"line":134,"address":[],"length":0,"stats":{"Line":120106}},{"line":139,"address":[],"length":0,"stats":{"Line":120106}},{"line":140,"address":[],"length":0,"stats":{"Line":120106}},{"line":141,"address":[],"length":0,"stats":{"Line":120106}},{"line":142,"address":[],"length":0,"stats":{"Line":120106}},{"line":144,"address":[],"length":0,"stats":{"Line":120108}},{"line":145,"address":[],"length":0,"stats":{"Line":4}},{"line":146,"address":[],"length":0,"stats":{"Line":2}},{"line":150,"address":[],"length":0,"stats":{"Line":360312}},{"line":151,"address":[],"length":0,"stats":{"Line":120104}},{"line":152,"address":[],"length":0,"stats":{"Line":240208}},{"line":153,"address":[],"length":0,"stats":{"Line":240208}},{"line":154,"address":[],"length":0,"stats":{"Line":120104}},{"line":155,"address":[],"length":0,"stats":{"Line":120104}},{"line":168,"address":[],"length":0,"stats":{"Line":80077}},{"line":169,"address":[],"length":0,"stats":{"Line":80077}},{"line":170,"address":[],"length":0,"stats":{"Line":1}},{"line":171,"address":[],"length":0,"stats":{"Line":1}},{"line":175,"address":[],"length":0,"stats":{"Line":160152}},{"line":176,"address":[],"length":0,"stats":{"Line":320304}},{"line":179,"address":[],"length":0,"stats":{"Line":400380}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":240228}},{"line":188,"address":[],"length":0,"stats":{"Line":80076}},{"line":189,"address":[],"length":0,"stats":{"Line":160152}},{"line":190,"address":[],"length":0,"stats":{"Line":160152}},{"line":191,"address":[],"length":0,"stats":{"Line":80076}},{"line":192,"address":[],"length":0,"stats":{"Line":80076}},{"line":209,"address":[],"length":0,"stats":{"Line":41}},{"line":210,"address":[],"length":0,"stats":{"Line":82}},{"line":211,"address":[],"length":0,"stats":{"Line":1}},{"line":212,"address":[],"length":0,"stats":{"Line":4}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":216,"address":[],"length":0,"stats":{"Line":40}},{"line":217,"address":[],"length":0,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":78}},{"line":221,"address":[],"length":0,"stats":{"Line":156}},{"line":228,"address":[],"length":0,"stats":{"Line":117}},{"line":233,"address":[],"length":0,"stats":{"Line":115}},{"line":234,"address":[],"length":0,"stats":{"Line":76}},{"line":238,"address":[],"length":0,"stats":{"Line":76}},{"line":239,"address":[],"length":0,"stats":{"Line":76}},{"line":251,"address":[],"length":0,"stats":{"Line":114}},{"line":252,"address":[],"length":0,"stats":{"Line":114}},{"line":255,"address":[],"length":0,"stats":{"Line":38}},{"line":256,"address":[],"length":0,"stats":{"Line":76}},{"line":258,"address":[],"length":0,"stats":{"Line":76}},{"line":262,"address":[],"length":0,"stats":{"Line":76}},{"line":263,"address":[],"length":0,"stats":{"Line":76}},{"line":264,"address":[],"length":0,"stats":{"Line":76}},{"line":266,"address":[],"length":0,"stats":{"Line":125}},{"line":267,"address":[],"length":0,"stats":{"Line":279}},{"line":268,"address":[],"length":0,"stats":{"Line":3104}},{"line":271,"address":[],"length":0,"stats":{"Line":7280}},{"line":280,"address":[],"length":0,"stats":{"Line":8736}},{"line":282,"address":[],"length":0,"stats":{"Line":1456}},{"line":287,"address":[],"length":0,"stats":{"Line":38}},{"line":289,"address":[],"length":0,"stats":{"Line":1}},{"line":290,"address":[],"length":0,"stats":{"Line":1}},{"line":291,"address":[],"length":0,"stats":{"Line":1}}],"covered":92,"coverable":96},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","storage.rs"],"content":"//! Storage abstraction for Tensors.\n//!\n//! This module defines the `Storage` trait, which abstracts over the underlying data container.\n//!\n//! # ML Context\n//!\n//! Tensors need to store their elements somewhere. \"Storage\" is the container that holds\n//! the raw data.\n//! - **Contiguous Memory**: Most tensor operations (like matrix multiplication) rely on data\n//!   being stored contiguously in memory for cache efficiency and SIMD usage.\n//! - **Abstraction**: By abstracting storage, we can support different backends (CPU `Vec`,\n//!   GPU buffers, mmap files) without changing the high-level Tensor API.\n\nuse crate::tensor::TensorElem;\nuse std::fmt::Debug;\n\n/// A trait for the underlying data storage.\n///\n/// Abstracts over the container used to hold tensor data.\n/// For `Cpu` device, this is typically `Vec\u003cT\u003e`.\n///\n/// # Requirements\n///\n/// Implementations must provide access to the raw data as slices. This allows\n/// efficient interaction with low-level linear algebra routines.\npub trait Storage\u003cT\u003e: Clone + Debug + Send + Sync {\n    /// Returns the data as an immutable slice.\n    fn as_slice(\u0026self) -\u003e \u0026[T];\n\n    /// Returns the data as a mutable slice.\n    fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [T];\n\n    /// Returns the number of elements in the storage.\n    fn len(\u0026self) -\u003e usize;\n\n    /// Returns `true` if the storage contains no elements.\n    fn is_empty(\u0026self) -\u003e bool {\n        self.len() == 0\n    }\n\n    /// Copies data from a slice into the storage.\n    ///\n    /// # Arguments\n    ///\n    /// * `src` - The source slice to copy from.\n    fn copy_from_slice(\u0026mut self, src: \u0026[T])\n    where\n        T: Copy,\n    {\n        self.as_mut_slice().copy_from_slice(src);\n    }\n}\n\nimpl\u003cT: TensorElem\u003e Storage\u003cT\u003e for Vec\u003cT\u003e {\n    fn as_slice(\u0026self) -\u003e \u0026[T] {\n        self\n    }\n    fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [T] {\n        self\n    }\n    fn len(\u0026self) -\u003e usize {\n        self.len()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_vec_storage() {\n        let mut storage = vec![1.0, 2.0, 3.0];\n\n        // Test as_slice\n        assert_eq!(storage.as_slice(), \u0026[1.0, 2.0, 3.0]);\n\n        // Test len\n        assert_eq!(storage.len(), 3);\n        assert!(!storage.is_empty());\n\n        // Test as_mut_slice\n        storage.as_mut_slice()[0] = 10.0;\n        assert_eq!(storage.as_slice(), \u0026[10.0, 2.0, 3.0]);\n\n        // Test copy_from_slice\n        storage.copy_from_slice(\u0026[4.0, 5.0, 6.0]);\n        assert_eq!(storage.as_slice(), \u0026[4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_empty_storage() {\n        let storage: Vec\u003cf32\u003e = vec![];\n        assert!(storage.is_empty());\n        assert_eq!(storage.len(), 0);\n    }\n\n    #[derive(Clone, Debug)]\n    struct MockStorage {\n        data: Vec\u003cf32\u003e,\n    }\n\n    impl Storage\u003cf32\u003e for MockStorage {\n        fn as_slice(\u0026self) -\u003e \u0026[f32] {\n            \u0026self.data\n        }\n        fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [f32] {\n            \u0026mut self.data\n        }\n        fn len(\u0026self) -\u003e usize {\n            self.data.len()\n        }\n    }\n\n    #[test]\n    fn test_storage_defaults() {\n        let mut storage = MockStorage {\n            data: vec![1.0, 2.0],\n        };\n        // Test default is_empty\n        assert!(!storage.is_empty());\n\n        let empty = MockStorage { data: vec![] };\n        assert!(empty.is_empty());\n\n        // Test default copy_from_slice\n        storage.copy_from_slice(\u0026[3.0, 4.0]);\n        assert_eq!(storage.as_slice(), \u0026[3.0, 4.0]);\n    }\n}\n","traces":[{"line":37,"address":[],"length":0,"stats":{"Line":2}},{"line":38,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":2}},{"line":50,"address":[],"length":0,"stats":{"Line":6}},{"line":55,"address":[],"length":0,"stats":{"Line":101722}},{"line":56,"address":[],"length":0,"stats":{"Line":101722}},{"line":58,"address":[],"length":0,"stats":{"Line":80057}},{"line":59,"address":[],"length":0,"stats":{"Line":80057}},{"line":61,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":2}}],"covered":10,"coverable":10},{"path":["/","Users","blitz","my-oss","xla-rs","test_link.rs"],"content":"extern crate xla_rs; fn main() {}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","autograd_tests.rs"],"content":"use xla_rs::autograd::Variable;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_scalar_autograd() {\n    // f(x) = x^2 + 3x\n    // f'(x) = 2x + 3\n    // Let x = 2.0\n    // f(2) = 4 + 6 = 10\n    // f'(2) = 4 + 3 = 7\n\n    let x_data = Tensor::\u003cf32, 1\u003e::new(vec![2.0], [1]).unwrap();\n    let x = Variable::new(x_data);\n\n    // x^2 = x * x\n    let x_sq = x.clone() * x.clone();\n\n    // 3x\n    let three = Variable::new(Tensor::\u003cf32, 1\u003e::new(vec![3.0], [1]).unwrap());\n    let three_x = three * x.clone();\n\n    // y = x^2 + 3x\n    let y = x_sq + three_x;\n\n    assert_eq!(y.data.data()[0], 10.0);\n\n    y.backward();\n\n    // Check grad of x\n    // x contributes to x_sq (2x) and three_x (3)\n    // grad should be 7.0\n\n    let x_grad = x.grad.borrow();\n    assert!(x_grad.is_some());\n    assert_eq!(x_grad.as_ref().unwrap().data()[0], 7.0);\n}\n\n#[test]\nfn test_matmul_autograd() {\n    // C = A @ B\n    // A: [1, 2] = [1, 2]\n    // B: [2, 1] = [3, 4]\n    // C: [1, 1] = 1*3 + 2*4 = 3 + 8 = 11\n\n    // dC/dA = B^T = [3, 4]\n    // dC/dB = A^T = [1, 2]\n\n    let a_data = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 2.0], [1, 2]).unwrap();\n    let b_data = Tensor::\u003cf32, 2\u003e::new(vec![3.0, 4.0], [2, 1]).unwrap();\n\n    let a = Variable::new(a_data);\n    let b = Variable::new(b_data);\n\n    let c = a.matmul(\u0026b).unwrap();\n\n    assert_eq!(c.data.data()[0], 11.0);\n\n    c.backward();\n\n    let a_grad = a.grad.borrow();\n    let b_grad = b.grad.borrow();\n\n    assert!(a_grad.is_some());\n    assert!(b_grad.is_some());\n\n    // Check A grad: should be B^T = [3, 4]\n    assert_eq!(a_grad.as_ref().unwrap().data(), \u0026[3.0, 4.0]);\n\n    // Check B grad: should be A^T = [1, 2]\n    assert_eq!(b_grad.as_ref().unwrap().data(), \u0026[1.0, 2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","book_tests.rs"],"content":"use doc_comment::doctest;\n\ndoctest!(\"../book/src/chapter_06_gemma.md\");\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_01_setup.rs"],"content":"#[test]\nfn test_environment_setup() {\n    println!(\"Welcome to xla-rs!\");\n    println!(\"If you see this, your Rust environment is correctly set up.\");\n\n    // Verify we can allocate a vector\n    let v: Vec\u003cf32\u003e = vec![1.0, 2.0, 3.0];\n    assert_eq!(v.len(), 3);\n\n    // Verify rayon is working\n    use rayon::prelude::*;\n    let sum: f32 = v.par_iter().sum();\n    assert_eq!(sum, 6.0);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_02_tensors.rs"],"content":"use xla_rs::tensor::Tensor;\n\n#[test]\nfn test_tensor_basics() {\n    // 1. Create a 2x3 tensor\n    let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n    let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n\n    assert_eq!(t.shape(), \u0026[2, 3]);\n    assert_eq!(t.strides(), \u0026[3, 1]); // Row-major\n}\n\n#[test]\nfn test_broadcasting() {\n    // A: [2, 2]\n    let a = Tensor::\u003cf32, 2\u003e::ones([2, 2]);\n    // Broadcasting is not yet implemented, so we manually broadcast for now\n    let b_expanded = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 1.0, 1.0, 1.0], [2, 2]).unwrap();\n\n    let c = (\u0026a + \u0026b_expanded).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2]);\n    assert_eq!(c.data(), \u0026[2.0, 2.0, 2.0, 2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_03_autograd.rs"],"content":"use xla_rs::autograd::Variable;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_autograd_simple() {\n    // a = 2, b = 3\n    let a_data = Tensor::\u003cf32, 1\u003e::new(vec![2.0], [1]).unwrap();\n    let b_data = Tensor::\u003cf32, 1\u003e::new(vec![3.0], [1]).unwrap();\n\n    let a = Variable::new(a_data);\n    let b = Variable::new(b_data);\n\n    // c = a * b = 6\n    let c = a.clone() * b.clone();\n\n    assert_eq!(c.data.data(), \u0026[6.0]);\n\n    // Backward\n    c.backward();\n\n    // da = b = 3\n    let a_grad = a.grad.borrow();\n    assert_eq!(a_grad.as_ref().unwrap().data(), \u0026[3.0]);\n\n    // db = a = 2\n    let b_grad = b.grad.borrow();\n    assert_eq!(b_grad.as_ref().unwrap().data(), \u0026[2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_04_nn.rs"],"content":"use xla_rs::nn::Linear;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_linear_forward() {\n    // Weight: [2, 2] (all ones)\n    let weight = Tensor::\u003cf32, 2\u003e::ones([2, 2]);\n    // Bias: [2] (all ones)\n    let bias = Tensor::\u003cf32, 1\u003e::ones([2]);\n\n    let linear = Linear::new(weight, Some(bias));\n\n    // Input: [1, 2] (all ones)\n    let input = Tensor::\u003cf32, 2\u003e::ones([1, 2]);\n\n    // Output = Input * Weight^T + Bias\n    // [1, 1] * [1, 1]^T + [1, 1]\n    // [1*1 + 1*1] + 1 = 3\n\n    let output = linear.forward(\u0026input).unwrap();\n\n    assert_eq!(output.shape(), \u0026[1, 2]);\n    assert_eq!(output.data(), \u0026[3.0, 3.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_05_attention.rs"],"content":"use xla_rs::models::gemma::attention::MultiHeadAttention;\nuse xla_rs::models::gemma::rope::precompute_freqs_cis;\nuse xla_rs::nn::Linear;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_attention_forward() {\n    let dim = 16;\n    let num_heads = 4;\n    let num_kv_heads = 4; // Standard MHA for simplicity\n    let head_dim = 4;\n\n    // Create dummy projections (Identity-like would be ideal, but random/ones is fine for shape check)\n    let q_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let k_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let v_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let o_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n\n    let attn = MultiHeadAttention::new(\n        dim,\n        num_heads,\n        num_kv_heads,\n        head_dim,\n        q_proj,\n        k_proj,\n        v_proj,\n        o_proj,\n    );\n\n    // Input: [Batch=1, Seq=2, Dim=16]\n    let x = Tensor::\u003cf32, 3\u003e::ones([1, 2, dim]);\n\n    // RoPE freqs\n    let (cos, sin) = precompute_freqs_cis(head_dim, 10, 10000.0);\n\n    let output = attn.forward(\u0026x, \u0026cos, \u0026sin, None).unwrap();\n\n    assert_eq!(output.shape(), \u0026[1, 2, dim]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_06_gemma.rs"],"content":"use xla_rs::models::gemma::attention::MultiHeadAttention;\nuse xla_rs::models::gemma::rope::precompute_freqs_cis;\nuse xla_rs::models::gemma::{GemmaBlock, GemmaConfig, GemmaModel, MLP};\nuse xla_rs::nn::{Linear, RMSNorm};\nuse xla_rs::tensor::Tensor;\n\nfn create_dummy_model(config: \u0026GemmaConfig) -\u003e GemmaModel\u003cf32\u003e {\n    let mut layers = Vec::new();\n\n    for _ in 0..config.num_hidden_layers {\n        let dim = config.hidden_size;\n\n        let kv_dim = config.num_key_value_heads * config.head_dim;\n        let attn = MultiHeadAttention::new(\n            dim,\n            config.num_attention_heads,\n            config.num_key_value_heads,\n            config.head_dim,\n            Linear::new(Tensor::ones([dim, dim]), None),\n            Linear::new(Tensor::ones([kv_dim, dim]), None),\n            Linear::new(Tensor::ones([kv_dim, dim]), None),\n            Linear::new(Tensor::ones([dim, dim]), None),\n        );\n\n        let mlp = MLP {\n            gate_proj: Linear::new(Tensor::ones([config.intermediate_size, dim]), None),\n            up_proj: Linear::new(Tensor::ones([config.intermediate_size, dim]), None),\n            down_proj: Linear::new(Tensor::ones([dim, config.intermediate_size]), None),\n        };\n\n        layers.push(GemmaBlock {\n            self_attn: attn,\n            mlp,\n            input_layernorm: RMSNorm::new(Tensor::ones([dim]), config.rms_norm_eps),\n            post_attention_layernorm: RMSNorm::new(Tensor::ones([dim]), config.rms_norm_eps),\n        });\n    }\n\n    GemmaModel {\n        layers,\n        norm: RMSNorm::new(Tensor::ones([config.hidden_size]), config.rms_norm_eps),\n    }\n}\n\n#[test]\nfn test_gemma_forward() {\n    let config = GemmaConfig::tiny_test();\n    let model = create_dummy_model(\u0026config);\n\n    // Input: [Batch=1, Seq=2, Dim=64]\n    let x = Tensor::\u003cf32, 3\u003e::ones([1, 2, config.hidden_size]);\n\n    // RoPE\n    let (cos, sin) = precompute_freqs_cis(config.head_dim, 10, 10000.0);\n\n    // Step 1: Norm\n    let norm_x = model.layers[0].input_layernorm.forward(\u0026x).unwrap();\n    assert_eq!(norm_x.shape(), \u0026[1, 2, 64]);\n\n    // Step 2: Attn\n    let attn_out = model.layers[0]\n        .self_attn\n        .forward(\u0026norm_x, \u0026cos, \u0026sin, None)\n        .unwrap();\n    assert_eq!(attn_out.shape(), \u0026[1, 2, 64]);\n\n    // Step 3: Residual\n    let x2 = (\u0026x + \u0026attn_out).unwrap();\n\n    // Step 4: Post Norm\n    let norm_x2 = model.layers[0]\n        .post_attention_layernorm\n        .forward(\u0026x2)\n        .unwrap();\n    assert_eq!(norm_x2.shape(), \u0026[1, 2, 64]);\n\n    // Step 5: MLP\n    let mlp_out = model.layers[0].mlp.forward(\u0026norm_x2).unwrap();\n    assert_eq!(mlp_out.shape(), \u0026[1, 2, 64]);\n\n    // Full forward\n    let output = model.forward(\u0026x, \u0026cos, \u0026sin, None).unwrap();\n    assert_eq!(output.shape(), \u0026[1, 2, config.hidden_size]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","matmul_verification.rs"],"content":"use xla_rs::tensor::Tensor;\n\n#[test]\nfn test_matmul_2d_verification() {\n    // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n    let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n    let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n    let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n    let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2]);\n    assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n}\n\n#[test]\nfn test_matmul_3d_verification() {\n    // Batch size 2\n    // Batch 0: Same as 2D test\n    // Batch 1: All ones\n    // A: [2, 2, 3]\n    // B: [2, 3, 2]\n    // C: [2, 2, 2]\n\n    let mut a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // Batch 0\n    // Batch 1: [1, 1, 1; 1, 1, 1] (2x3)\n    a_data.extend_from_slice(\u0026[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]);\n\n    let mut b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0]; // Batch 0\n    // Batch 1: [1, 1; 1, 1; 1, 1] (3x2)\n    b_data.extend_from_slice(\u0026[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]);\n\n    let a = Tensor::\u003cf32, 3\u003e::new(a_data, [2, 2, 3]).unwrap();\n    let b = Tensor::\u003cf32, 3\u003e::new(b_data, [2, 3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2, 2]);\n\n    let c_data = c.data();\n    // Batch 0 check\n    assert_eq!(\u0026c_data[0..4], \u0026[31.0, 19.0, 85.0, 55.0]);\n    // Batch 1 check: 1*1 + 1*1 + 1*1 = 3\n    assert_eq!(\u0026c_data[4..8], \u0026[3.0, 3.0, 3.0, 3.0]);\n}\n\n#[test]\nfn test_matmul_4d_verification() {\n    // Rank 4: [Batch, Heads, M, K] x [Batch, Heads, K, N] -\u003e [Batch, Heads, M, N]\n    // Shape: [1, 2, 2, 3] x [1, 2, 3, 2] -\u003e [1, 2, 2, 2]\n\n    // Batch 0, Head 0:\n    // A: [[1, 2, 3], [4, 5, 6]] (2x3)\n    // B: [[7, 8], [9, 1], [2, 3]] (3x2)\n    // Expected: [[31, 19], [85, 55]]\n\n    // Batch 0, Head 1:\n    // A: All ones (2x3)\n    // B: All ones (3x2)\n    // Expected: [[3, 3], [3, 3]]\n\n    let mut a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // Head 0\n    a_data.extend_from_slice(\u0026[1.0; 6]); // Head 1\n\n    let mut b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0]; // Head 0\n    b_data.extend_from_slice(\u0026[1.0; 6]); // Head 1\n\n    let a = Tensor::\u003cf32, 4\u003e::new(a_data, [1, 2, 2, 3]).unwrap();\n    let b = Tensor::\u003cf32, 4\u003e::new(b_data, [1, 2, 3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[1, 2, 2, 2]);\n\n    let c_data = c.data();\n    // Head 0 check\n    assert_eq!(\u0026c_data[0..4], \u0026[31.0, 19.0, 85.0, 55.0]);\n    // Head 1 check\n    assert_eq!(\u0026c_data[4..8], \u0026[3.0, 3.0, 3.0, 3.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","nn_integration_test.rs"],"content":"#![allow(unused)]\nuse std::cell::RefCell;\nuse std::rc::Rc;\nuse xla_rs::autograd::{GraphNode, Variable};\nuse xla_rs::tensor::{Cpu, Tensor, TensorElem};\n\n// --- Sigmoid Activation ---\n\n#[derive(Debug)]\nstruct SigmoidNode\u003cT: TensorElem, const RANK: usize\u003e {\n    input: Tensor\u003cT, RANK, Cpu\u003e,\n    output: Tensor\u003cT, RANK, Cpu\u003e,\n    input_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem + num_traits::Float, const RANK: usize\u003e GraphNode for SigmoidNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // dL/dx = dL/dy * y * (1 - y)\n            let one = T::one();\n            let y = \u0026self.output;\n\n            // y * (1 - y)\n            // We need to do this element-wise.\n            // Since we don't have extensive tensor ops, we'll use map.\n\n            let derivative = y.map(|val| val * (one - val));\n            let dx = (grad * \u0026derivative).unwrap();\n\n            let mut input_grad = self.input_grad.borrow_mut();\n            if let Some(g) = input_grad.as_mut() {\n                *g = (g as \u0026Tensor\u003cT, RANK, Cpu\u003e + \u0026dx).unwrap(); // Add to existing gradient\n            } else {\n                *input_grad = Some(dx);\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nfn sigmoid\u003cconst RANK: usize\u003e(x: \u0026Variable\u003cf32, RANK\u003e) -\u003e Variable\u003cf32, RANK\u003e {\n    let data = x.data.map(|v| 1.0 / (1.0 + (-v).exp()));\n\n    let mut parents = Vec::new();\n    if let Some(p) = \u0026x.node {\n        parents.push(p.clone());\n    }\n\n    let out_grad = Rc::new(RefCell::new(None));\n\n    let node = Rc::new(SigmoidNode {\n        input: x.data.clone(),\n        output: data.clone(),\n        input_grad: x.grad.clone(),\n        out_grad: out_grad.clone(),\n        parents,\n    });\n\n    Variable {\n        data,\n        grad: out_grad,\n        node: Some(node),\n    }\n}\n\n// --- Linear Layer ---\n\nstruct Linear {\n    weight: Variable\u003cf32, 2\u003e,\n    bias: Variable\u003cf32, 2\u003e,\n}\n\nimpl Linear {\n    fn new(in_features: usize, out_features: usize) -\u003e Self {\n        // Pseudo-random initialization\n        let w_data = (0..in_features * out_features)\n            .map(|i| {\n                let s = (i + 1) as f32;\n                ((s * 12.9898).sin() * 43758.5453).fract() - 0.5\n            })\n            .collect();\n        let weight = Variable::new(Tensor::new(w_data, [in_features, out_features]).unwrap());\n\n        // Bias shape [1, out_features] for easy addition with batch_size=1\n        let b_data = vec![0.0; out_features];\n        let bias = Variable::new(Tensor::new(b_data, [1, out_features]).unwrap());\n\n        Self { weight, bias }\n    }\n\n    fn forward(\u0026self, x: \u0026Variable\u003cf32, 2\u003e) -\u003e Variable\u003cf32, 2\u003e {\n        // y = x @ W + b\n        // x: [B, In], W: [In, Out] -\u003e [B, Out]\n        let xw = x.matmul(\u0026self.weight).unwrap();\n        xw + self.bias.clone()\n    }\n}\n\n#[test]\nfn test_xor_training() {\n    // XOR Problem\n    // Inputs: (0,0), (0,1), (1,0), (1,1)\n    // Targets: 0, 1, 1, 0\n\n    // We'll use inputs with an extra 1.0 for bias:\n    // (0,0,1), (0,1,1), (1,0,1), (1,1,1)\n\n    let inputs = vec![\n        vec![0.0, 0.0],\n        vec![0.0, 1.0],\n        vec![1.0, 0.0],\n        vec![1.0, 1.0],\n    ];\n\n    let targets = vec![vec![0.0], vec![1.0], vec![1.0], vec![0.0]];\n\n    // Network: 2 inputs -\u003e 4 hidden -\u003e 1 output\n    let mut l1 = Linear::new(2, 4);\n    let mut l2 = Linear::new(4, 1);\n\n    let lr = 0.5;\n    let epochs = 5000;\n\n    for epoch in 0..epochs {\n        let mut total_loss = 0.0;\n\n        for (x_vec, t_vec) in inputs.iter().zip(targets.iter()) {\n            // Prepare input\n            let x = Variable::new(Tensor::new(x_vec.clone(), [1, 2]).unwrap());\n            let t = Variable::new(Tensor::new(t_vec.clone(), [1, 1]).unwrap());\n\n            // Forward\n            let h1 = l1.forward(\u0026x);\n            let h1_act = sigmoid(\u0026h1);\n\n            let h2 = l2.forward(\u0026h1_act);\n            let pred = sigmoid(\u0026h2); // Sigmoid output for 0/1\n\n            // Loss = (pred - target)^2\n            let neg_one = Variable::new(Tensor::new(vec![-1.0], [1, 1]).unwrap());\n            let neg_target = t * neg_one;\n            let diff = pred.clone() + neg_target;\n            let loss = diff.clone() * diff.clone();\n\n            total_loss += loss.data.data()[0];\n\n            // Backward\n            loss.backward();\n\n            // Update weights\n            update_param(\u0026mut l1.weight, lr);\n            update_param(\u0026mut l1.bias, lr);\n            update_param(\u0026mut l2.weight, lr);\n            update_param(\u0026mut l2.bias, lr);\n\n            // Zero gradients\n            zero_grad(\u0026l1.weight);\n            zero_grad(\u0026l1.bias);\n            zero_grad(\u0026l2.weight);\n            zero_grad(\u0026l2.bias);\n        }\n\n        if epoch % 500 == 0 {\n            println!(\"Epoch {}: Loss = {}\", epoch, total_loss);\n        }\n    }\n\n    // Verify predictions\n    println!(\"Predictions:\");\n    for (x_vec, t_vec) in inputs.iter().zip(targets.iter()) {\n        let x = Variable::new(Tensor::new(x_vec.clone(), [1, 2]).unwrap());\n        let h1 = l1.forward(\u0026x);\n        let h1_act = sigmoid(\u0026h1);\n        let h2 = l2.forward(\u0026h1_act);\n        let pred = sigmoid(\u0026h2);\n\n        let val = pred.data.data()[0];\n        println!(\"Input: {:?}, Target: {:?}, Pred: {}\", x_vec, t_vec, val);\n\n        let target = t_vec[0];\n        assert!((val - target).abs() \u003c 0.4);\n    }\n}\n\nfn update_param\u003cconst RANK: usize\u003e(var: \u0026mut Variable\u003cf32, RANK\u003e, lr: f32) {\n    let data = var.data.data_mut();\n    if let Some(grad) = var.grad.borrow().as_ref() {\n        let grad_data = grad.data();\n        for (w, g) in data.iter_mut().zip(grad_data.iter()) {\n            *w -= lr * g;\n        }\n    }\n}\n\nfn zero_grad\u003cconst RANK: usize\u003e(var: \u0026Variable\u003cf32, RANK\u003e) {\n    *var.grad.borrow_mut() = None;\n}\n","traces":[],"covered":0,"coverable":0}]};
        var previousData = {"files":[{"path":["/","Users","blitz","my-oss","xla-rs","kernels","benches","ops_bench.rs"],"content":"use criterion::{Criterion, black_box, criterion_group, criterion_main};\nuse xla_rs_kernels::{cpu_matmul, cpu_transpose};\n\nfn benchmark_matmul(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"matmul\");\n    let sizes = [64, 128, 256, 512];\n\n    for \u0026size in \u0026sizes {\n        let m = size;\n        let k = size;\n        let n = size;\n        let lhs_shape = [m, k];\n        let rhs_shape = [k, n];\n        let lhs_data = vec![1.0f32; m * k];\n        let rhs_data = vec![1.0f32; k * n];\n\n        group.bench_function(format!(\"{}x{}\", size, size), |b| {\n            b.iter(|| {\n                cpu_matmul(\n                    black_box(\u0026lhs_data),\n                    black_box(\u0026rhs_data),\n                    black_box(\u0026lhs_shape),\n                    black_box(\u0026rhs_shape),\n                )\n                .unwrap()\n            })\n        });\n    }\n    group.finish();\n}\n\nfn benchmark_transpose(c: \u0026mut Criterion) {\n    let mut group = c.benchmark_group(\"transpose\");\n    let sizes = [128, 512, 1024, 2048];\n\n    for \u0026size in \u0026sizes {\n        let m = size;\n        let n = size;\n        let shape = [m, n];\n        let data = vec![1.0f32; m * n];\n\n        group.bench_function(format!(\"{}x{}\", size, size), |b| {\n            b.iter(|| cpu_transpose(black_box(\u0026data), black_box(\u0026shape)).unwrap())\n        });\n    }\n    group.finish();\n}\n\ncriterion_group!(benches, benchmark_matmul, benchmark_transpose);\ncriterion_main!(benches);\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","cpu_matmul.rs"],"content":"use crate::{KernelElem, Result};\nuse rayon::prelude::*;\n\n/// CPU Implementation of Matrix Multiplication.\n///\n/// This function is the \"kernel\" that performs the actual computation.\n/// It is separated from the `Tensor` struct to allow for easy swapping with\n/// optimized libraries (like BLAS) in the future.\n///\n/// # SOTA Integration Guide\n///\n/// To integrate a SOTA library like `cblas` or `matrixmultiply`:\n/// 1. Replace the body of this function with a call to the library's `sgemm` or `dgemm`.\n/// 2. Ensure the memory layout matches (Row-Major vs Column-Major).\n///    - `xla-rs` uses Row-Major.\n///    - BLAS typically defaults to Column-Major but supports Row-Major via flags.\n/// 3. Handle batching:\n///    - If the library supports batched matmul, pass the batch count.\n///    - Otherwise, loop over the batch dimension here (parallelized with `rayon`).\npub fn cpu_matmul\u003cT, const RANK: usize\u003e(\n    lhs_data: \u0026[T],\n    rhs_data: \u0026[T],\n    lhs_shape: \u0026[usize; RANK],\n    rhs_shape: \u0026[usize; RANK],\n) -\u003e Result\u003cVec\u003cT\u003e\u003e\nwhere\n    T: KernelElem,\n{\n    let m = lhs_shape[RANK - 2];\n    let k = lhs_shape[RANK - 1];\n    let n = rhs_shape[RANK - 1];\n\n    if k != rhs_shape[RANK - 2] {\n        return Err(crate::KernelError::ShapeMismatch {\n            expected: vec![k],\n            got: vec![rhs_shape[RANK - 2]],\n        });\n    }\n\n    let mut out_shape = *lhs_shape;\n    out_shape[RANK - 2] = m;\n    out_shape[RANK - 1] = n;\n    let size: usize = out_shape.iter().product();\n    let mut out_data = vec![T::zero(); size];\n\n    // Optimization: Transpose rhs to allow sequential access (cache friendly)\n    // We need to transpose the RHS data. Since we don't have a Tensor object here,\n    // we call the kernel directly.\n    // rhs is [..., K, N], we want [..., N, K]\n    let rhs_t_data = super::cpu_transpose::cpu_transpose(rhs_data, rhs_shape)?;\n\n    // Parallelize over rows of the output matrices across all batches\n    // Output shape is [Batch..., M, N]\n    // We iterate over (Batch... * M) rows, each of size N\n\n    out_data\n        .as_mut_slice()\n        .par_chunks_mut(n)\n        .enumerate()\n        .for_each(|(global_row_idx, out_row)| {\n            let batch_idx = global_row_idx / m;\n            let row_in_matrix = global_row_idx % m;\n\n            // Calculate offsets for input tensors\n            let a_batch_offset = batch_idx * m * k;\n            // rhs_t has shape [..., N, K], so batch offset is batch_idx * N * K\n            let b_t_batch_offset = batch_idx * n * k;\n\n            let a_row_start = a_batch_offset + row_in_matrix * k;\n            let a_slice = \u0026lhs_data[a_row_start..a_row_start + k];\n\n            for (col_in_matrix, out_elem) in out_row.iter_mut().enumerate() {\n                // We want dot product of:\n                // A row: `row_in_matrix`\n                // B col: `col_in_matrix` -\u003e which is rhs_t row `col_in_matrix`\n\n                let b_t_row_start = b_t_batch_offset + col_in_matrix * k;\n                let b_t_slice = \u0026rhs_t_data[b_t_row_start..b_t_row_start + k];\n\n                let mut sum = T::zero();\n                // Vectorizable loop\n                for (\u0026val_a, \u0026val_b) in a_slice.iter().zip(b_t_slice.iter()) {\n                    sum += val_a * val_b;\n                }\n                *out_elem = sum;\n            }\n        });\n\n    Ok(out_data)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::KernelError;\n\n    #[test]\n    fn test_matmul_simple() {\n        let a = vec![1.0, 2.0, 3.0, 4.0]; // 2x2\n        let b = vec![5.0, 6.0, 7.0, 8.0]; // 2x2\n        let shape_a = [2, 2];\n        let shape_b = [2, 2];\n\n        let result = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b).unwrap();\n        // Expected:\n        // [1*5+2*7, 1*6+2*8] = [19, 22]\n        // [3*5+4*7, 3*6+4*8] = [43, 50]\n        assert_eq!(result, vec![19.0, 22.0, 43.0, 50.0]);\n    }\n\n    #[test]\n    fn test_matmul_batch() {\n        // Batch size 2, 2x2 matrices\n        let a = vec![\n            1.0, 0.0, 0.0, 1.0, // Identity\n            2.0, 0.0, 0.0, 2.0, // Scaled Identity\n        ];\n        let b = vec![\n            1.0, 2.0, 3.0, 4.0, // Matrix B1\n            5.0, 6.0, 7.0, 8.0, // Matrix B2\n        ];\n        let shape_a = [2, 2, 2];\n        let shape_b = [2, 2, 2];\n\n        let result = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b).unwrap();\n        // Expected:\n        // Batch 1: I * B1 = B1\n        // Batch 2: 2I * B2 = 2 * B2\n        let expected = vec![1.0, 2.0, 3.0, 4.0, 10.0, 12.0, 14.0, 16.0];\n        assert_eq!(result, expected);\n    }\n\n    #[test]\n    fn test_matmul_shape_mismatch() {\n        let a = vec![1.0; 4]; // 2x2\n        let b = vec![1.0; 6]; // 3x2\n        let shape_a = [2, 2];\n        let shape_b = [3, 2]; // Inner dim mismatch: 2 != 3\n\n        let err = cpu_matmul(\u0026a, \u0026b, \u0026shape_a, \u0026shape_b);\n        assert!(matches!(err, Err(KernelError::ShapeMismatch { .. })));\n    }\n}\n","traces":[{"line":20,"address":[],"length":0,"stats":{"Line":120103}},{"line":29,"address":[],"length":0,"stats":{"Line":240206}},{"line":30,"address":[],"length":0,"stats":{"Line":240206}},{"line":31,"address":[],"length":0,"stats":{"Line":240206}},{"line":33,"address":[],"length":0,"stats":{"Line":120103}},{"line":34,"address":[],"length":0,"stats":{"Line":3}},{"line":35,"address":[],"length":0,"stats":{"Line":9}},{"line":36,"address":[],"length":0,"stats":{"Line":3}},{"line":40,"address":[],"length":0,"stats":{"Line":240200}},{"line":41,"address":[],"length":0,"stats":{"Line":120100}},{"line":42,"address":[],"length":0,"stats":{"Line":120100}},{"line":43,"address":[],"length":0,"stats":{"Line":600500}},{"line":44,"address":[],"length":0,"stats":{"Line":480400}},{"line":50,"address":[],"length":0,"stats":{"Line":480400}},{"line":56,"address":[],"length":0,"stats":{"Line":120100}},{"line":58,"address":[],"length":0,"stats":{"Line":240200}},{"line":60,"address":[],"length":0,"stats":{"Line":320368}},{"line":61,"address":[],"length":0,"stats":{"Line":400536}},{"line":62,"address":[],"length":0,"stats":{"Line":400536}},{"line":65,"address":[],"length":0,"stats":{"Line":400536}},{"line":67,"address":[],"length":0,"stats":{"Line":400536}},{"line":69,"address":[],"length":0,"stats":{"Line":600804}},{"line":70,"address":[],"length":0,"stats":{"Line":801072}},{"line":72,"address":[],"length":0,"stats":{"Line":1528950}},{"line":77,"address":[],"length":0,"stats":{"Line":1392219}},{"line":78,"address":[],"length":0,"stats":{"Line":1856292}},{"line":80,"address":[],"length":0,"stats":{"Line":928146}},{"line":82,"address":[],"length":0,"stats":{"Line":5159205}},{"line":83,"address":[],"length":0,"stats":{"Line":1892560}},{"line":85,"address":[],"length":0,"stats":{"Line":464073}},{"line":89,"address":[],"length":0,"stats":{"Line":120100}}],"covered":31,"coverable":31},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","cpu_transpose.rs"],"content":"use crate::{KernelElem, Result};\nuse rayon::prelude::*;\n\n/// CPU Implementation of Transpose.\n///\n/// Swaps the last two dimensions of the input data.\n///\n/// # SOTA Integration Guide\n///\n/// Optimized transpose operations often use tiling (blocking) to improve cache usage.\n/// Libraries like `hptt` (High Performance Tensor Transpose) can be used here.\npub fn cpu_transpose\u003cT, const RANK: usize\u003e(data: \u0026[T], shape: \u0026[usize; RANK]) -\u003e Result\u003cVec\u003cT\u003e\u003e\nwhere\n    T: KernelElem,\n{\n    let m = shape[RANK - 2];\n    let n = shape[RANK - 1];\n\n    let mut new_shape = *shape;\n    new_shape.swap(RANK - 1, RANK - 2);\n    let size: usize = new_shape.iter().product();\n    let mut out_data = vec![T::zero(); size];\n\n    // We parallelize over the rows of the OUTPUT tensor.\n    // The output tensor has shape [Batch..., N, M].\n    // So we view it as `batch_size * N` rows, each of length `M`.\n    out_data\n        .as_mut_slice()\n        .par_chunks_mut(m)\n        .enumerate()\n        .for_each(|(i, out_row)| {\n            // `i` is the global row index in the flattened output [Batch * N, M]\n            let batch_idx = i / n;\n            let col_idx = i % n; // This corresponds to the column index in the input matrix\n\n            // Calculate the base offset for this batch in the input data\n            let input_batch_offset = batch_idx * m * n;\n\n            // Copy the column `col_idx` from the input matrix to `out_row`\n            for (r, out_elem) in out_row.iter_mut().enumerate() {\n                // Input is [M, N]. We want element at (r, col_idx).\n                // Index = input_batch_offset + r * N + col_idx\n                *out_elem = data[input_batch_offset + r * n + col_idx];\n            }\n        });\n\n    Ok(out_data)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_transpose_simple() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // 2x3\n        let shape = [2, 3];\n\n        let result = cpu_transpose(\u0026data, \u0026shape).unwrap();\n        // Expected 3x2:\n        // [1, 4]\n        // [2, 5]\n        // [3, 6]\n        assert_eq!(result, vec![1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_batch() {\n        // Batch size 2, 2x2 matrices\n        let data = vec![\n            1.0, 2.0, 3.0, 4.0, // Matrix 1\n            5.0, 6.0, 7.0, 8.0, // Matrix 2\n        ];\n        let shape = [2, 2, 2];\n\n        let result = cpu_transpose(\u0026data, \u0026shape).unwrap();\n        // Expected:\n        // Batch 1: [1, 3, 2, 4]\n        // Batch 2: [5, 7, 6, 8]\n        let expected = vec![1.0, 3.0, 2.0, 4.0, 5.0, 7.0, 6.0, 8.0];\n        assert_eq!(result, expected);\n    }\n}\n","traces":[{"line":12,"address":[],"length":0,"stats":{"Line":200173}},{"line":16,"address":[],"length":0,"stats":{"Line":400346}},{"line":17,"address":[],"length":0,"stats":{"Line":400346}},{"line":19,"address":[],"length":0,"stats":{"Line":400346}},{"line":20,"address":[],"length":0,"stats":{"Line":800692}},{"line":21,"address":[],"length":0,"stats":{"Line":1000865}},{"line":22,"address":[],"length":0,"stats":{"Line":800692}},{"line":27,"address":[],"length":0,"stats":{"Line":200173}},{"line":29,"address":[],"length":0,"stats":{"Line":400346}},{"line":31,"address":[],"length":0,"stats":{"Line":744198}},{"line":33,"address":[],"length":0,"stats":{"Line":1088050}},{"line":34,"address":[],"length":0,"stats":{"Line":1088050}},{"line":37,"address":[],"length":0,"stats":{"Line":1088050}},{"line":40,"address":[],"length":0,"stats":{"Line":5129454}},{"line":43,"address":[],"length":0,"stats":{"Line":2331586}},{"line":47,"address":[],"length":0,"stats":{"Line":200173}}],"covered":16,"coverable":16},{"path":["/","Users","blitz","my-oss","xla-rs","kernels","src","lib.rs"],"content":"use num_traits::{FromPrimitive, Num, NumAssign, ToPrimitive};\nuse std::fmt::Debug;\nuse thiserror::Error;\n\npub mod cpu_matmul;\npub mod cpu_transpose;\n\npub use cpu_matmul::cpu_matmul;\npub use cpu_transpose::cpu_transpose;\n\n#[derive(Error, Debug)]\npub enum KernelError {\n    #[error(\"Shape mismatch: expected {expected:?}, got {got:?}\")]\n    ShapeMismatch {\n        expected: Vec\u003cusize\u003e,\n        got: Vec\u003cusize\u003e,\n    },\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, KernelError\u003e;\n\n/// Trait bound for elements that can be processed by kernels.\n/// This mirrors `TensorElem` in the main crate to avoid circular dependencies.\npub trait KernelElem:\n    Num + NumAssign + Copy + Clone + Debug + Send + Sync + FromPrimitive + ToPrimitive + PartialOrd\n{\n}\n\nimpl\u003cT\u003e KernelElem for T where\n    T: Num\n        + NumAssign\n        + Copy\n        + Clone\n        + Debug\n        + Send\n        + Sync\n        + FromPrimitive\n        + ToPrimitive\n        + PartialOrd\n{\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","engine.rs"],"content":"use super::GraphNode;\nuse std::collections::HashSet;\nuse std::rc::Rc;\n\n/// Runs the backward pass starting from the given root node.\n///\n/// This function performs a topological sort of the computation graph to ensure\n/// that dependencies are processed before their consumers. It then calls\n/// `.backward()` on each node in reverse topological order.\n///\n/// # Parallelism Note\n///\n/// The backward pass is currently executed serially. While the computation graph\n/// theoretically allows for inter-op parallelism (processing independent nodes concurrently),\n/// this implementation prioritizes simplicity and relies on **intra-op parallelism**.\n///\n/// - **Intra-op parallelism**: Individual operations (like matrix multiplication) are\n///   parallelized internally (e.g., using BLAS or multi-threaded implementations).\n///   This usually yields better performance gains for deep learning workloads as\n///   operations are often heavy enough to saturate system resources.\n/// - **Inter-op parallelism**: Running multiple graph nodes simultaneously requires\n///   thread-safe graph structures (`Arc\u003cMutex\u003c...\u003e\u003e`) and complex scheduling,\n///   which adds significant overhead and complexity for often marginal gains\n///   compared to optimizing the operations themselves.\npub fn backward(root: Option\u003cRc\u003cdyn GraphNode\u003e\u003e) {\n    let Some(root) = root else { return };\n\n    let mut topo = Vec::new();\n    let mut visited = HashSet::new();\n\n    build_topo(root.clone(), \u0026mut topo, \u0026mut visited);\n\n    for node in topo.into_iter().rev() {\n        node.backward();\n    }\n}\n\n/// Recursively builds the topological sort of the graph.\nfn build_topo(\n    node: Rc\u003cdyn GraphNode\u003e,\n    topo: \u0026mut Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n    visited: \u0026mut HashSet\u003c*const ()\u003e,\n) {\n    let ptr = Rc::as_ptr(\u0026node) as *const ();\n    if visited.contains(\u0026ptr) {\n        return;\n    }\n    visited.insert(ptr);\n\n    for parent in node.parents() {\n        build_topo(parent, topo, visited);\n    }\n\n    topo.push(node);\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::cell::RefCell;\n\n    #[derive(Debug)]\n    struct MockNode {\n        id: usize,\n        parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n        visited_order: Rc\u003cRefCell\u003cVec\u003cusize\u003e\u003e\u003e,\n    }\n\n    impl GraphNode for MockNode {\n        fn backward(\u0026self) {\n            self.visited_order.borrow_mut().push(self.id);\n        }\n\n        fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n            self.parents.clone()\n        }\n    }\n\n    #[test]\n    fn test_topological_sort() {\n        let order = Rc::new(RefCell::new(Vec::new()));\n\n        // Create a diamond graph:\n        //   3\n        //  / \\\n        // 1   2\n        //  \\ /\n        //   0\n\n        let n0 = Rc::new(MockNode {\n            id: 0,\n            parents: vec![],\n            visited_order: order.clone(),\n        });\n        let n1 = Rc::new(MockNode {\n            id: 1,\n            parents: vec![n0.clone()],\n            visited_order: order.clone(),\n        });\n        let n2 = Rc::new(MockNode {\n            id: 2,\n            parents: vec![n0.clone()],\n            visited_order: order.clone(),\n        });\n        let n3 = Rc::new(MockNode {\n            id: 3,\n            parents: vec![n1.clone(), n2.clone()],\n            visited_order: order.clone(),\n        });\n\n        backward(Some(n3));\n\n        let result = order.borrow();\n        // Expected order: 3 -\u003e (1 or 2) -\u003e (2 or 1) -\u003e 0\n        assert_eq!(result.len(), 4);\n        assert_eq!(result[0], 3);\n        assert_eq!(result[3], 0);\n        assert!(result.contains(\u00261));\n        assert!(result.contains(\u00262));\n    }\n}\n","traces":[{"line":25,"address":[],"length":0,"stats":{"Line":20019}},{"line":26,"address":[],"length":0,"stats":{"Line":40038}},{"line":28,"address":[],"length":0,"stats":{"Line":40032}},{"line":29,"address":[],"length":0,"stats":{"Line":40032}},{"line":31,"address":[],"length":0,"stats":{"Line":80064}},{"line":33,"address":[],"length":0,"stats":{"Line":420104}},{"line":34,"address":[],"length":0,"stats":{"Line":180028}},{"line":39,"address":[],"length":0,"stats":{"Line":200031}},{"line":44,"address":[],"length":0,"stats":{"Line":400062}},{"line":45,"address":[],"length":0,"stats":{"Line":600093}},{"line":46,"address":[],"length":0,"stats":{"Line":20003}},{"line":48,"address":[],"length":0,"stats":{"Line":540084}},{"line":50,"address":[],"length":0,"stats":{"Line":720086}},{"line":51,"address":[],"length":0,"stats":{"Line":540045}},{"line":54,"address":[],"length":0,"stats":{"Line":540084}}],"covered":15,"coverable":15},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","functional.rs"],"content":"use crate::autograd::Variable;\nuse crate::tensor::{Cpu, Tensor, TensorElem};\n\n/// Computes the gradient of a function `f` with respect to its input.\n///\n/// Returns a function that takes a `Tensor` input and returns the gradient `Tensor`.\n///\n/// # Example\n/// ```ignore\n/// let grad_square = grad(|x| x.clone() * x.clone());\n/// let g = grad_square(Tensor::new(vec![3.0], []).unwrap());\n/// // g = 6.0\n/// ```\npub fn grad\u003cF, T, const RANK: usize\u003e(f: F) -\u003e impl Fn(Tensor\u003cT, RANK, Cpu\u003e) -\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    F: Fn(Variable\u003cT, RANK\u003e) -\u003e Variable\u003cT, RANK\u003e,\n    T: TensorElem + 'static,\n{\n    move |x| {\n        let x_var = Variable::new(x);\n        let y_var = f(x_var.clone());\n        y_var.backward();\n\n        // Extract gradient. If None (no dependency), return zeros.\n        let grad = x_var.grad.borrow();\n        if let Some(g) = grad.as_ref() {\n            g.clone()\n        } else {\n            Tensor::zeros(*x_var.data.shape())\n        }\n    }\n}\n\ntype CpuTensor\u003cT, const RANK: usize\u003e = Tensor\u003cT, RANK, Cpu\u003e;\n\n/// Computes the value and gradient of a function `f` with respect to its input.\n///\n/// Returns a function that takes a `Tensor` input and returns a tuple `(Value, Gradient)`.\npub fn value_and_grad\u003cF, T, const RANK: usize\u003e(\n    f: F,\n) -\u003e impl Fn(CpuTensor\u003cT, RANK\u003e) -\u003e (CpuTensor\u003cT, RANK\u003e, CpuTensor\u003cT, RANK\u003e)\nwhere\n    F: Fn(Variable\u003cT, RANK\u003e) -\u003e Variable\u003cT, RANK\u003e,\n    T: TensorElem + 'static,\n{\n    move |x| {\n        let x_var = Variable::new(x);\n        let y_var = f(x_var.clone());\n        y_var.backward();\n\n        let grad = x_var.grad.borrow();\n        let g = if let Some(g) = grad.as_ref() {\n            g.clone()\n        } else {\n            Tensor::zeros(*x_var.data.shape())\n        };\n\n        (y_var.data, g)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_grad_square() {\n        // f(x) = x^2\n        // f'(x) = 2x\n        let square = |x: Variable\u003cf32, 0\u003e| x.clone() * x.clone();\n        let grad_square = grad(square);\n\n        let x = Tensor::new(vec![3.0], []).unwrap();\n        let g = grad_square(x);\n\n        assert_eq!(g.data()[0], 6.0);\n    }\n\n    #[test]\n    fn test_value_and_grad_cubic() {\n        // f(x) = x^3 = x * x^2\n        // f'(x) = 3x^2\n        let cubic = |x: Variable\u003cf32, 0\u003e| x.clone() * x.clone() * x.clone();\n        let vag_cubic = value_and_grad(cubic);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let (val, g) = vag_cubic(x);\n\n        assert_eq!(val.data()[0], 8.0); // 2^3\n        assert_eq!(g.data()[0], 12.0); // 3 * 2^2\n    }\n\n    #[test]\n    fn test_grad_constant() {\n        // f(x) = 5.0\n        // f'(x) = 0.0\n        let constant = |_x: Variable\u003cf32, 0\u003e| Variable::new(Tensor::new(vec![5.0], []).unwrap());\n        let grad_constant = grad(constant);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let g = grad_constant(x);\n\n        assert_eq!(g.data()[0], 0.0);\n    }\n\n    #[test]\n    fn test_value_and_grad_constant() {\n        // f(x) = 5.0\n        let constant = |_x: Variable\u003cf32, 0\u003e| Variable::new(Tensor::new(vec![5.0], []).unwrap());\n        let vag_constant = value_and_grad(constant);\n\n        let x = Tensor::new(vec![2.0], []).unwrap();\n        let (val, g) = vag_constant(x);\n\n        assert_eq!(val.data()[0], 5.0);\n        assert_eq!(g.data()[0], 0.0);\n    }\n}\n","traces":[{"line":14,"address":[],"length":0,"stats":{"Line":2}},{"line":19,"address":[],"length":0,"stats":{"Line":2}},{"line":20,"address":[],"length":0,"stats":{"Line":6}},{"line":21,"address":[],"length":0,"stats":{"Line":6}},{"line":22,"address":[],"length":0,"stats":{"Line":4}},{"line":25,"address":[],"length":0,"stats":{"Line":4}},{"line":26,"address":[],"length":0,"stats":{"Line":3}},{"line":27,"address":[],"length":0,"stats":{"Line":2}},{"line":29,"address":[],"length":0,"stats":{"Line":2}},{"line":39,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":2}},{"line":47,"address":[],"length":0,"stats":{"Line":6}},{"line":48,"address":[],"length":0,"stats":{"Line":6}},{"line":49,"address":[],"length":0,"stats":{"Line":4}},{"line":51,"address":[],"length":0,"stats":{"Line":4}},{"line":52,"address":[],"length":0,"stats":{"Line":5}},{"line":53,"address":[],"length":0,"stats":{"Line":2}},{"line":55,"address":[],"length":0,"stats":{"Line":2}},{"line":58,"address":[],"length":0,"stats":{"Line":2}}],"covered":19,"coverable":19},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","mod.rs"],"content":"//! Automatic Differentiation (Autograd) module.\n//!\n//! This module implements a \"Define-by-Run\" (Tape-based) automatic differentiation system,\n//! similar to PyTorch. It allows for automatic calculation of gradients for tensor operations,\n//! which is essential for training neural networks.\n//!\n//! # Key Components\n//!\n//! - [`Variable`]: The core struct that wraps a `Tensor` and tracks its gradient and computation history.\n//! - [`GraphNode`]: A trait representing an operation in the computation graph.\n//! - [`engine::backward`]: The engine that performs the backward pass (topological sort and gradient propagation).\n//! - [`functional`]: A submodule providing a JAX-style functional API (`grad`, `value_and_grad`).\n//!\n//! # Example\n//!\n//! ```rust\n//! # use xla_rs::tensor::Tensor;\n//! # use xla_rs::autograd::Variable;\n//! let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n//! let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n//!\n//! // c = a * b\n//! let c = a.clone() * b.clone();\n//!\n//! c.backward();\n//!\n//! // dc/da = b = 3.0\n//! assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n//! ```\n\nuse crate::tensor::{Cpu, Tensor, TensorElem};\nuse std::cell::RefCell;\nuse std::fmt::Debug;\nuse std::rc::Rc;\n\npub mod engine;\npub mod functional;\npub mod ops;\n\n/// A node in the computation graph.\n///\n/// This trait represents an operation that can be backpropagated through.\npub trait GraphNode: Debug {\n    /// Computes the gradient for this node and propagates it to its parents.\n    fn backward(\u0026self);\n    /// Returns the parent nodes of this node.\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e;\n}\n\n/// A variable in the computation graph.\n///\n/// Wraps a `Tensor` and tracks its gradient and the operation that created it.\n#[derive(Clone, Debug)]\npub struct Variable\u003cT, const RANK: usize\u003e\nwhere\n    T: TensorElem,\n{\n    /// The actual tensor data.\n    pub data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// The gradient of the loss with respect to this variable.\n    pub grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// The node in the computation graph that produced this variable.\n    pub node: Option\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT, const RANK: usize\u003e Variable\u003cT, RANK\u003e\nwhere\n    T: TensorElem + 'static,\n{\n    /// Creates a new leaf variable.\n    ///\n    /// Leaf variables are the inputs to the computation graph (e.g., weights, input data).\n    /// They do not have a parent node.\n    pub fn new(data: Tensor\u003cT, RANK, Cpu\u003e) -\u003e Self {\n        Self {\n            data,\n            grad: Rc::new(RefCell::new(None)),\n            node: None,\n        }\n    }\n\n    /// Creates a new variable with an associated graph node.\n    ///\n    /// This is typically used internally by operations to create output variables.\n    pub fn with_node(data: Tensor\u003cT, RANK, Cpu\u003e, node: Rc\u003cdyn GraphNode\u003e) -\u003e Self {\n        Self {\n            data,\n            grad: Rc::new(RefCell::new(None)),\n            node: Some(node),\n        }\n    }\n\n    /// Triggers the backward pass starting from this variable.\n    ///\n    /// This variable is typically the loss value (a scalar).\n    /// The gradient of this variable is seeded with 1.0.\n    pub fn backward(\u0026self) {\n        // Seed gradient\n        if self.grad.borrow().is_none() {\n            *self.grad.borrow_mut() = Some(Tensor::ones(*self.data.shape()));\n        }\n\n        crate::autograd::engine::backward(self.node.clone());\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_variable_creation() {\n        let data = Tensor::new(vec![1.0, 2.0], [2]).unwrap();\n        let var = Variable::new(data.clone());\n\n        assert_eq!(var.data.data(), data.data());\n        assert!(var.grad.borrow().is_none());\n        assert!(var.node.is_none());\n    }\n\n    #[test]\n    fn test_variable_backward_seed() {\n        let data = Tensor::new(vec![1.0], []).unwrap();\n        let var = Variable::new(data);\n\n        // Backward on leaf node should just seed the gradient\n        var.backward();\n\n        assert!(var.grad.borrow().is_some());\n        assert_eq!(var.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_variable_with_node() {\n        let data = Tensor::new(vec![10.0], []).unwrap();\n\n        // Create a mock node (using a simple struct that implements GraphNode)\n        #[derive(Debug)]\n        struct MockNode;\n        impl GraphNode for MockNode {\n            fn backward(\u0026self) {}\n            fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n                vec![]\n            }\n        }\n\n        let node = Rc::new(MockNode);\n        let var = Variable::with_node(data.clone(), node.clone());\n\n        assert_eq!(var.data.data(), data.data());\n        assert!(var.node.is_some());\n        assert!(var.grad.borrow().is_none());\n    }\n}\n","traces":[{"line":74,"address":[],"length":0,"stats":{"Line":60038}},{"line":77,"address":[],"length":0,"stats":{"Line":180114}},{"line":85,"address":[],"length":0,"stats":{"Line":1}},{"line":88,"address":[],"length":0,"stats":{"Line":3}},{"line":89,"address":[],"length":0,"stats":{"Line":1}},{"line":97,"address":[],"length":0,"stats":{"Line":20018}},{"line":99,"address":[],"length":0,"stats":{"Line":40033}},{"line":100,"address":[],"length":0,"stats":{"Line":60045}},{"line":103,"address":[],"length":0,"stats":{"Line":60054}}],"covered":9,"coverable":9},{"path":["/","Users","blitz","my-oss","xla-rs","src","autograd","ops.rs"],"content":"//! Operations for the autograd system.\n//!\n//! This module defines the nodes in the computation graph for various operations\n//! (Add, Mul, MatMul) and implements the `backward` pass for each.\n\nuse super::{GraphNode, Variable};\nuse crate::tensor::{Cpu, Tensor, TensorElem};\nuse std::cell::RefCell;\nuse std::fmt::Debug;\nuse std::ops::{Add, Mul};\nuse std::rc::Rc;\n\n// --- Add Node ---\n/// A node representing element-wise addition in the computation graph.\n// --- Add Node ---\n/// A node representing element-wise addition in the computation graph.\n#[derive(Debug)]\nstruct AddNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output (received from the parent node).\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for AddNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // d(x+y)/dx = 1 * grad\n            // d(x+y)/dy = 1 * grad\n\n            // Accumulate gradient for lhs\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(grad)).unwrap();\n                } else {\n                    *lhs = Some(grad.clone());\n                }\n            }\n\n            // Accumulate gradient for rhs\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(grad)).unwrap();\n                } else {\n                    *rhs = Some(grad.clone());\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Add for Variable\u003cT, RANK\u003e {\n    type Output = Variable\u003cT, RANK\u003e;\n\n    /// Adds two variables element-wise.\n    ///\n    /// This operation creates a new node in the computation graph.\n    fn add(self, rhs: Self) -\u003e Self::Output {\n        let data = (\u0026self.data + \u0026rhs.data).unwrap();\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        // Even leaf nodes need to be part of the graph if we want to backprop to them?\n        // Actually, leaf variables usually don't have a `node` (creator).\n        // But the `AddNode` needs to update their `grad`.\n        // So `AddNode` holds references to their `grad` cells.\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(AddNode {\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents, // This is wrong. Parents should be the nodes that created lhs/rhs.\n                     // If lhs is leaf, it has no parent node.\n                     // But topological sort needs to traverse.\n                     // If leaf has no node, traversal stops there. Correct.\n        });\n\n        Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        }\n    }\n}\n\n// --- Mul Node ---\n/// A node representing element-wise multiplication in the computation graph.\n// --- Mul Node ---\n/// A node representing element-wise multiplication in the computation graph.\n#[derive(Debug)]\nstruct MulNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Data of the left-hand side operand (needed for gradient calculation).\n    lhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Data of the right-hand side operand (needed for gradient calculation).\n    rhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output (received from the parent node).\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for MulNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // d(x*y)/dx = y * grad\n            // d(x*y)/dy = x * grad\n\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                let dl_dx = (\u0026self.rhs_data * grad).unwrap();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(\u0026dl_dx)).unwrap();\n                } else {\n                    *lhs = Some(dl_dx);\n                }\n            }\n\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                let dr_dy = (\u0026self.lhs_data * grad).unwrap();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(\u0026dr_dy)).unwrap();\n                } else {\n                    *rhs = Some(dr_dy);\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Mul for Variable\u003cT, RANK\u003e {\n    type Output = Variable\u003cT, RANK\u003e;\n\n    /// Multiplies two variables element-wise.\n    ///\n    /// This operation creates a new node in the computation graph.\n    fn mul(self, rhs: Self) -\u003e Self::Output {\n        let data = (\u0026self.data * \u0026rhs.data).unwrap();\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(MulNode {\n            lhs_data: self.data.clone(),\n            rhs_data: rhs.data.clone(),\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents,\n        });\n\n        Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        }\n    }\n}\n\n// --- MatMul Node ---\n/// A node representing matrix multiplication in the computation graph.\n// --- MatMul Node ---\n/// A node representing matrix multiplication in the computation graph.\n#[derive(Debug)]\nstruct MatMulNode\u003cT: TensorElem, const RANK: usize\u003e {\n    /// Data of the left-hand side operand.\n    lhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Data of the right-hand side operand.\n    rhs_data: Tensor\u003cT, RANK, Cpu\u003e,\n    /// Gradient of the left-hand side operand.\n    lhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the right-hand side operand.\n    rhs_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Gradient of the output.\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    /// Parent nodes in the computation graph.\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem, const RANK: usize\u003e GraphNode for MatMulNode\u003cT, RANK\u003e {\n    #[allow(clippy::collapsible_if)]\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // LHS Gradient\n            {\n                let mut lhs = self.lhs_grad.borrow_mut();\n                let rhs_t = self.rhs_data.transpose().unwrap();\n                let dl_da = grad.matmul(\u0026rhs_t).unwrap();\n                if let Some(l) = lhs.as_mut() {\n                    *l = (l.add(\u0026dl_da)).unwrap();\n                } else {\n                    *lhs = Some(dl_da);\n                }\n            }\n\n            // RHS Gradient\n            {\n                let mut rhs = self.rhs_grad.borrow_mut();\n                let lhs_t = self.lhs_data.transpose().unwrap();\n                let dr_db = lhs_t.matmul(grad).unwrap();\n                if let Some(r) = rhs.as_mut() {\n                    *r = (r.add(\u0026dr_db)).unwrap();\n                } else {\n                    *rhs = Some(dr_db);\n                }\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nimpl\u003cT: TensorElem + 'static, const RANK: usize\u003e Variable\u003cT, RANK\u003e {\n    /// Performs matrix multiplication between two variables.\n    ///\n    /// This operation creates a new node in the computation graph.\n    pub fn matmul(\u0026self, rhs: \u0026Self) -\u003e crate::tensor::Result\u003cSelf\u003e {\n        let data = self.data.matmul(\u0026rhs.data)?;\n\n        let mut parents = Vec::new();\n        if let Some(p) = \u0026self.node {\n            parents.push(p.clone());\n        }\n        if let Some(p) = \u0026rhs.node {\n            parents.push(p.clone());\n        }\n\n        let out_grad = Rc::new(RefCell::new(None));\n\n        let node = Rc::new(MatMulNode {\n            lhs_data: self.data.clone(),\n            rhs_data: rhs.data.clone(),\n            lhs_grad: self.grad.clone(),\n            rhs_grad: rhs.grad.clone(),\n            out_grad: out_grad.clone(),\n            parents,\n        });\n\n        Ok(Variable {\n            data,\n            grad: out_grad,\n            node: Some(node),\n        })\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_add_backward() {\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = a.clone() + b.clone();\n\n        c.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_mul_backward() {\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = a.clone() * b.clone();\n\n        c.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 2.0);\n    }\n\n    #[test]\n    fn test_chain_rule() {\n        // y = (a + b) * c\n        // a=2, b=3, c=4\n        // y = (2+3)*4 = 20\n        // dy/da = c = 4\n        // dy/db = c = 4\n        // dy/dc = a + b = 5\n\n        let a = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n        let b = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let c = Variable::new(Tensor::new(vec![4.0], []).unwrap());\n\n        let sum = a.clone() + b.clone();\n        let y = sum * c.clone();\n\n        y.backward();\n\n        assert_eq!(a.grad.borrow().as_ref().unwrap().data()[0], 4.0);\n        assert_eq!(b.grad.borrow().as_ref().unwrap().data()[0], 4.0);\n        assert_eq!(c.grad.borrow().as_ref().unwrap().data()[0], 5.0);\n    }\n\n    #[test]\n    fn test_matmul_backward() {\n        // A = [[1, 2], [3, 4]] (2x2)\n        // B = [[5, 6], [7, 8]] (2x2)\n        // C = A @ B\n        // C = [[19, 22], [43, 50]]\n\n        // Let Loss L = sum(C) = 19 + 22 + 43 + 50 = 134\n        // dL/dC = [[1, 1], [1, 1]]\n\n        // dL/dA = dL/dC @ B^T\n        //       = [[1, 1], [1, 1]] @ [[5, 7], [6, 8]]\n        //       = [[11, 15], [11, 15]]\n\n        // dL/dB = A^T @ dL/dC\n        //       = [[1, 3], [2, 4]] @ [[1, 1], [1, 1]]\n        //       = [[4, 4], [6, 6]]\n\n        let a_data = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], [2, 2]).unwrap();\n        let b_data = Tensor::new(vec![5.0, 6.0, 7.0, 8.0], [2, 2]).unwrap();\n\n        let a = Variable::new(a_data);\n        let b = Variable::new(b_data);\n\n        let c = a.matmul(\u0026b).unwrap();\n\n        // Manually seed gradient with ones (equivalent to sum(C))\n        *c.grad.borrow_mut() = Some(Tensor::ones([2, 2]));\n\n        // We need to manually trigger backward on the node because c is not a scalar\n        // and Variable::backward() assumes scalar and seeds with 1.0.\n        // But here we want to test the MatMulNode backward specifically.\n        // However, Variable::backward() calls engine::backward(self.node).\n        // If we seed grad manually, we can call c.backward() but we need to be careful\n        // that it doesn't overwrite our seed.\n        // Variable::backward() checks `if self.grad.borrow().is_none()`.\n        // So if we seed it first, it should be fine.\n\n        c.backward();\n\n        let a_grad = a.grad.borrow().as_ref().unwrap().clone();\n        let b_grad = b.grad.borrow().as_ref().unwrap().clone();\n\n        assert_eq!(a_grad.data(), \u0026[11.0, 15.0, 11.0, 15.0]);\n        assert_eq!(b_grad.data(), \u0026[4.0, 4.0, 6.0, 6.0]);\n    }\n\n    #[test]\n    fn test_matmul_chain_rule() {\n        // y = sum( (A @ x) * x )\n        // A = [[1, 2], [3, 4]]\n        // x = [1, 2]\n        // A@x = [5, 11]\n        // (A@x)*x = [5, 22]\n        // y = 27\n\n        // This is a bit complex to derive manually quickly.\n        // Let's try a simpler one: y = sum(A @ x)\n        // A = [[1, 2], [3, 4]]\n        // x = [1, 2]\n        // A@x = [5, 11]\n        // y = 16\n\n        // dy/dx = A^T @ ones\n        //       = [[1, 3], [2, 4]] @ [1, 1]\n        //       = [4, 6]\n\n        // dy/dA = ones @ x^T (outer product)\n        //       = [1, 1] @ [1, 2]\n        //       = [[1, 2], [1, 2]]\n\n        let a_data = Tensor::new(vec![1.0, 2.0, 3.0, 4.0], [2, 2]).unwrap();\n        let x_data = Tensor::new(vec![1.0, 2.0], [2, 1]).unwrap(); // Column vector\n\n        let a = Variable::new(a_data);\n        let x = Variable::new(x_data);\n\n        let y_vec = a.matmul(\u0026x).unwrap();\n\n        // To make it a scalar for easy backward:\n        // We don't have a \"sum\" operation in autograd yet.\n        // But we can simulate \"sum\" by doing dot product with ones, or just seeding gradient with ones.\n        // Let's seed gradient of y_vec with ones.\n\n        *y_vec.grad.borrow_mut() = Some(Tensor::ones([2, 1]));\n        y_vec.backward();\n\n        let x_grad = x.grad.borrow().as_ref().unwrap().clone();\n        let a_grad = a.grad.borrow().as_ref().unwrap().clone();\n\n        assert_eq!(x_grad.data(), \u0026[4.0, 6.0]);\n        assert_eq!(a_grad.data(), \u0026[1.0, 2.0, 1.0, 2.0]);\n    }\n\n    #[test]\n    fn test_add_accumulation() {\n        // y = x + x + x\n        // x = 3\n        // y = 9\n        // dy/dx = 3\n\n        let x = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let y = x.clone() + x.clone() + x.clone();\n\n        y.backward();\n\n        assert_eq!(x.grad.borrow().as_ref().unwrap().data()[0], 3.0);\n    }\n\n    #[test]\n    fn test_mul_accumulation() {\n        // y = x * x * x\n        // x = 3\n        // y = 27\n        // dy/dx = 3x^2 = 27\n\n        let x = Variable::new(Tensor::new(vec![3.0], []).unwrap());\n        let y = x.clone() * x.clone() * x.clone();\n\n        y.backward();\n\n        assert_eq!(x.grad.borrow().as_ref().unwrap().data()[0], 27.0);\n    }\n\n    #[test]\n    fn test_matmul_accumulation() {\n        // Y = X @ X @ X\n        // X = [[1, 0], [0, 1]] (Identity)\n        // Y = I\n        // Loss = sum(Y) = 2\n        // dL/dX should be 3 * I ?\n        // Let's use scalar logic for intuition: y = x^3, dy/dx = 3x^2. If x=1, dy/dx=3.\n        // For matrix: d(X^3)/dX.\n        // If X = I, X^2 = I, X^3 = I.\n        // dL/dX = 3 * X^2 = 3 * I.\n\n        let x_data = Tensor::new(vec![1.0, 0.0, 0.0, 1.0], [2, 2]).unwrap();\n        let x = Variable::new(x_data);\n\n        let y = x.matmul(\u0026x).unwrap().matmul(\u0026x).unwrap();\n\n        *y.grad.borrow_mut() = Some(Tensor::ones([2, 2]));\n        y.backward();\n\n        let x_grad = x.grad.borrow().as_ref().unwrap().clone();\n        // Expected gradient is 3 * ones (since we seeded with ones and dY/dX is 3*I effectively distributed)\n        // Wait.\n        // Y = X^3. L = sum(Y).\n        // dL/dX = 3 * (X^T)^2 @ Ones?\n        // Let's just check the result.\n        // dL/dX = [[3, 3], [3, 3]]\n\n        assert_eq!(x_grad.data(), \u0026[3.0, 3.0, 3.0, 3.0]);\n    }\n\n    #[test]\n    fn test_non_leaf_operations() {\n        // Test operations where RHS has a node (is not a leaf)\n        // This ensures coverage for `if let Some(p) = \u0026rhs.node` branches\n\n        let x = Variable::new(Tensor::new(vec![2.0], []).unwrap());\n\n        // a has a node\n        let a = x.clone() * x.clone(); // 4.0\n\n        // b = a + a. RHS a has node.\n        let b = a.clone() + a.clone(); // 8.0\n\n        // c = a * a. RHS a has node.\n        let c = a.clone() * a.clone(); // 16.0\n\n        // d = a @ a. RHS a has node. (Need rank 2 for matmul)\n        let m = Variable::new(Tensor::new(vec![2.0], [1, 1]).unwrap());\n        let n = m.matmul(\u0026m).unwrap(); // n has node\n        let _p = n.matmul(\u0026n).unwrap(); // RHS n has node\n\n        b.backward();\n        c.backward();\n        // We don't check gradients here, just ensuring the code paths run.\n    }\n}\n","traces":[{"line":30,"address":[],"length":0,"stats":{"Line":60006}},{"line":31,"address":[],"length":0,"stats":{"Line":120012}},{"line":37,"address":[],"length":0,"stats":{"Line":120012}},{"line":38,"address":[],"length":0,"stats":{"Line":60008}},{"line":39,"address":[],"length":0,"stats":{"Line":4}},{"line":41,"address":[],"length":0,"stats":{"Line":120010}},{"line":47,"address":[],"length":0,"stats":{"Line":120012}},{"line":48,"address":[],"length":0,"stats":{"Line":60010}},{"line":49,"address":[],"length":0,"stats":{"Line":8}},{"line":51,"address":[],"length":0,"stats":{"Line":120008}},{"line":57,"address":[],"length":0,"stats":{"Line":60006}},{"line":58,"address":[],"length":0,"stats":{"Line":120012}},{"line":68,"address":[],"length":0,"stats":{"Line":60014}},{"line":69,"address":[],"length":0,"stats":{"Line":180042}},{"line":71,"address":[],"length":0,"stats":{"Line":120028}},{"line":72,"address":[],"length":0,"stats":{"Line":180036}},{"line":73,"address":[],"length":0,"stats":{"Line":180033}},{"line":75,"address":[],"length":0,"stats":{"Line":100018}},{"line":76,"address":[],"length":0,"stats":{"Line":60006}},{"line":84,"address":[],"length":0,"stats":{"Line":240056}},{"line":86,"address":[],"length":0,"stats":{"Line":180042}},{"line":87,"address":[],"length":0,"stats":{"Line":180042}},{"line":88,"address":[],"length":0,"stats":{"Line":180042}},{"line":89,"address":[],"length":0,"stats":{"Line":120028}},{"line":90,"address":[],"length":0,"stats":{"Line":60014}},{"line":99,"address":[],"length":0,"stats":{"Line":60014}},{"line":125,"address":[],"length":0,"stats":{"Line":40013}},{"line":126,"address":[],"length":0,"stats":{"Line":80026}},{"line":131,"address":[],"length":0,"stats":{"Line":80026}},{"line":132,"address":[],"length":0,"stats":{"Line":120039}},{"line":133,"address":[],"length":0,"stats":{"Line":40023}},{"line":134,"address":[],"length":0,"stats":{"Line":20}},{"line":136,"address":[],"length":0,"stats":{"Line":40008}},{"line":141,"address":[],"length":0,"stats":{"Line":80026}},{"line":142,"address":[],"length":0,"stats":{"Line":120039}},{"line":143,"address":[],"length":0,"stats":{"Line":80027}},{"line":144,"address":[],"length":0,"stats":{"Line":80028}},{"line":146,"address":[],"length":0,"stats":{"Line":20006}},{"line":152,"address":[],"length":0,"stats":{"Line":40013}},{"line":153,"address":[],"length":0,"stats":{"Line":80026}},{"line":163,"address":[],"length":0,"stats":{"Line":40012}},{"line":164,"address":[],"length":0,"stats":{"Line":120036}},{"line":166,"address":[],"length":0,"stats":{"Line":80024}},{"line":167,"address":[],"length":0,"stats":{"Line":80020}},{"line":168,"address":[],"length":0,"stats":{"Line":60012}},{"line":170,"address":[],"length":0,"stats":{"Line":80014}},{"line":171,"address":[],"length":0,"stats":{"Line":60003}},{"line":174,"address":[],"length":0,"stats":{"Line":160048}},{"line":176,"address":[],"length":0,"stats":{"Line":120036}},{"line":177,"address":[],"length":0,"stats":{"Line":120036}},{"line":178,"address":[],"length":0,"stats":{"Line":120036}},{"line":179,"address":[],"length":0,"stats":{"Line":120036}},{"line":180,"address":[],"length":0,"stats":{"Line":120036}},{"line":181,"address":[],"length":0,"stats":{"Line":80024}},{"line":182,"address":[],"length":0,"stats":{"Line":40012}},{"line":188,"address":[],"length":0,"stats":{"Line":40012}},{"line":215,"address":[],"length":0,"stats":{"Line":40005}},{"line":216,"address":[],"length":0,"stats":{"Line":80010}},{"line":219,"address":[],"length":0,"stats":{"Line":80010}},{"line":220,"address":[],"length":0,"stats":{"Line":160020}},{"line":221,"address":[],"length":0,"stats":{"Line":200025}},{"line":222,"address":[],"length":0,"stats":{"Line":40007}},{"line":223,"address":[],"length":0,"stats":{"Line":4}},{"line":225,"address":[],"length":0,"stats":{"Line":40004}},{"line":231,"address":[],"length":0,"stats":{"Line":80010}},{"line":232,"address":[],"length":0,"stats":{"Line":160020}},{"line":233,"address":[],"length":0,"stats":{"Line":200025}},{"line":234,"address":[],"length":0,"stats":{"Line":40007}},{"line":235,"address":[],"length":0,"stats":{"Line":4}},{"line":237,"address":[],"length":0,"stats":{"Line":40004}},{"line":243,"address":[],"length":0,"stats":{"Line":40005}},{"line":244,"address":[],"length":0,"stats":{"Line":80010}},{"line":252,"address":[],"length":0,"stats":{"Line":40015}},{"line":253,"address":[],"length":0,"stats":{"Line":160060}},{"line":255,"address":[],"length":0,"stats":{"Line":80030}},{"line":256,"address":[],"length":0,"stats":{"Line":80027}},{"line":257,"address":[],"length":0,"stats":{"Line":60018}},{"line":259,"address":[],"length":0,"stats":{"Line":40017}},{"line":260,"address":[],"length":0,"stats":{"Line":3}},{"line":263,"address":[],"length":0,"stats":{"Line":160060}},{"line":265,"address":[],"length":0,"stats":{"Line":120045}},{"line":266,"address":[],"length":0,"stats":{"Line":120045}},{"line":267,"address":[],"length":0,"stats":{"Line":120045}},{"line":268,"address":[],"length":0,"stats":{"Line":120045}},{"line":269,"address":[],"length":0,"stats":{"Line":120045}},{"line":270,"address":[],"length":0,"stats":{"Line":80030}},{"line":271,"address":[],"length":0,"stats":{"Line":40015}},{"line":274,"address":[],"length":0,"stats":{"Line":40015}},{"line":275,"address":[],"length":0,"stats":{"Line":80030}},{"line":276,"address":[],"length":0,"stats":{"Line":40015}},{"line":277,"address":[],"length":0,"stats":{"Line":40015}}],"covered":91,"coverable":91},{"path":["/","Users","blitz","my-oss","xla-rs","src","lib.rs"],"content":"//! # xla-rs\n//!\n//! `xla-rs` is a pure Rust implementation of tensor operations and neural network building blocks,\n//! designed for educational purposes and understanding the internals of LLM inference.\n//!\n//! Despite the name, it currently runs on **CPU only** and does not yet integrate with the XLA compiler.\n//!\n//! ## Modules\n//!\n//! - [`mod@tensor`]: Core N-dimensional tensor implementation.\n//! - [`nn`]: Neural network layers (Linear, RMSNorm, MoE, etc.).\n//! - [`models`]: Model architectures (e.g., Gemma).\n//!\n//! ## Example\n//!\n//! ```rust\n//! use xla_rs::tensor::Tensor;\n//!\n//! let data = vec![1.0, 2.0, 3.0, 4.0];\n//! let tensor = Tensor::\u003cf32, 2\u003e::new(data, [2, 2]).unwrap();\n//! println!(\"{:?}\", tensor);\n//! ```\n\n/// Macro for creating a Tensor with compile-time shape checking.\n///\n/// # Examples\n///\n/// ```rust\n/// use xla_rs::tensor;\n/// use xla_rs::tensor::Tensor;\n///\n/// // Works\n/// let t = tensor!([1.0, 2.0, 3.0, 4.0], [2, 2]);\n///\n/// // Fails to compile:\n/// // let t = tensor!([1.0, 2.0, 3.0], [2, 2]);\n/// ```\n#[macro_export]\nmacro_rules! tensor {\n    ($data:expr, $shape:expr) =\u003e {{\n        // Constants to force compile-time evaluation\n        const DATA_LEN: usize = $data.len();\n        const SHAPE: [usize; $shape.len()] = $shape;\n        const EXPECTED_SIZE: usize = {\n            let mut size = 1;\n            let mut i = 0;\n            while i \u003c SHAPE.len() {\n                size *= SHAPE[i];\n                i += 1;\n            }\n            size\n        };\n\n        // This assertion triggers a compile-time error if false\n        const _: () = assert!(\n            DATA_LEN == EXPECTED_SIZE,\n            \"Shape mismatch: data length does not match shape product\"\n        );\n\n        // Safe to unwrap because we checked at compile time\n        $crate::tensor::Tensor::new($data.to_vec(), $shape).unwrap()\n    }};\n}\n\npub mod autograd;\npub mod models;\npub mod nn;\npub mod tensor;\n\npub use tensor::Tensor;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","attention.rs"],"content":"use super::rope::apply_rope;\nuse crate::nn::Linear;\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n#[derive(Debug)]\npub struct MultiHeadAttention\u003cT: TensorElem\u003e {\n    pub q_proj: Linear\u003cT\u003e,\n    pub k_proj: Linear\u003cT\u003e,\n    pub v_proj: Linear\u003cT\u003e,\n    pub o_proj: Linear\u003cT\u003e,\n\n    pub num_heads: usize,\n    pub num_kv_heads: usize,\n    pub head_dim: usize,\n    pub scaling: T,\n}\n\nimpl\u003cT: TensorElem + Float\u003e MultiHeadAttention\u003cT\u003e {\n    #[allow(clippy::too_many_arguments)]\n    pub fn new(\n        _dim: usize,\n        num_heads: usize,\n        num_kv_heads: usize,\n        head_dim: usize,\n        q_proj: Linear\u003cT\u003e,\n        k_proj: Linear\u003cT\u003e,\n        v_proj: Linear\u003cT\u003e,\n        o_proj: Linear\u003cT\u003e,\n    ) -\u003e Self {\n        Self {\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            scaling: T::one() / T::from_usize(head_dim).unwrap().sqrt(),\n        }\n    }\n\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let [b, s, _] = *x.shape();\n\n        let q = self.q_proj.forward(x)?;\n        let k = self.k_proj.forward(x)?;\n        let v = self.v_proj.forward(x)?;\n\n        // Reshape to [B, S, H, D]\n        let q = q.reshape([b, s, self.num_heads, self.head_dim])?;\n        let k = k.reshape([b, s, self.num_kv_heads, self.head_dim])?;\n        let v = v.reshape([b, s, self.num_kv_heads, self.head_dim])?;\n\n        // Permute to [B, H, S, D] using transpose_axes(1, 2)\n        let q = q.transpose_axes(1, 2)?;\n        let k = k.transpose_axes(1, 2)?;\n        let v = v.transpose_axes(1, 2)?;\n\n        // Apply RoPE (expects [B, H, S, D])\n        let q = apply_rope(\u0026q, freqs_cos, freqs_sin)?;\n        let k = apply_rope(\u0026k, freqs_cos, freqs_sin)?;\n\n        let (k, v) = if self.num_kv_heads != self.num_heads {\n            (self.repeat_kv(\u0026k)?, self.repeat_kv(\u0026v)?)\n        } else {\n            (k, v)\n        };\n\n        // Attention Score: q @ k.T\n        // q: [B, H, S, D]\n        // k: [B, H, S, D] -\u003e k.transpose() (swaps last two) -\u003e [B, H, D, S]\n        let k_t = k.transpose()?;\n\n        let q_flat = q.clone().reshape([b * self.num_heads, s, self.head_dim])?;\n        let k_t_flat = k_t.reshape([b * self.num_heads, self.head_dim, s])?;\n\n        let mut scores = q_flat.matmul(\u0026k_t_flat)?;\n\n        scores = scores.map(|val| val * self.scaling);\n\n        if let Some(m) = mask {\n            self.apply_mask(\u0026mut scores, m)?;\n        }\n\n        self.softmax_inplace(\u0026mut scores)?;\n\n        let v_flat = v.reshape([b * self.num_heads, s, self.head_dim])?;\n        let output = scores.matmul(\u0026v_flat)?;\n\n        // output: [B*H, S, D] -\u003e [B, H, S, D]\n        let output = output.reshape([b, self.num_heads, s, self.head_dim])?;\n\n        // We need [B, S, H, D].\n        // This is transpose_axes(1, 2) again on [B, H, S, D].\n        let output = output.transpose_axes(1, 2)?;\n\n        let output = output.reshape([b, s, self.num_heads * self.head_dim])?;\n\n        self.o_proj.forward(\u0026output)\n    }\n\n    fn repeat_kv(\u0026self, x: \u0026Tensor\u003cT, 4, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 4, Cpu\u003e\u003e {\n        let [b, n_kv, s, d] = *x.shape();\n        let n_rep = self.num_heads / n_kv;\n\n        if n_rep == 1 {\n            return Ok(x.clone());\n        }\n\n        let mut out = Tensor::zeros([b, self.num_heads, s, d]);\n        let src = x.data();\n        let dst = out.data_mut();\n\n        for batch in 0..b {\n            for h in 0..self.num_heads {\n                let src_h = h / n_rep;\n                let src_offset = (batch * n_kv + src_h) * s * d;\n                let dst_offset = (batch * self.num_heads + h) * s * d;\n\n                dst[dst_offset..dst_offset + s * d]\n                    .copy_from_slice(\u0026src[src_offset..src_offset + s * d]);\n            }\n        }\n        Ok(out)\n    }\n\n    fn softmax_inplace(\u0026self, x: \u0026mut Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003c()\u003e {\n        let [_, _, s] = *x.shape();\n        x.data_mut().par_chunks_mut(s).for_each(|row| {\n            let mut max_val = row[0];\n            for \u0026v in row.iter() {\n                if v \u003e max_val {\n                    max_val = v;\n                }\n            }\n\n            let mut sum_exp = T::zero();\n            for v in row.iter_mut() {\n                let exp_v = (*v - max_val).to_f32().unwrap().exp();\n                let exp_v_t = T::from_f32(exp_v).unwrap();\n                *v = exp_v_t;\n                sum_exp += exp_v_t;\n            }\n\n            let inv_sum = T::one() / sum_exp;\n            for v in row.iter_mut() {\n                *v *= inv_sum;\n            }\n        });\n        Ok(())\n    }\n\n    fn apply_mask(\u0026self, scores: \u0026mut Tensor\u003cT, 3, Cpu\u003e, mask: \u0026Tensor\u003cT, 2, Cpu\u003e) -\u003e Result\u003c()\u003e {\n        let [_, s, _] = *scores.shape();\n        let [ms1, ms2] = *mask.shape();\n\n        if s != ms1 || s != ms2 {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![s, s],\n                got: vec![ms1, ms2],\n            });\n        }\n\n        let mask_data = mask.data();\n\n        scores\n            .data_mut()\n            .par_chunks_mut(s * s)\n            .for_each(|score_matrix| {\n                for (i, val) in score_matrix.iter_mut().enumerate() {\n                    if mask_data[i] != T::one() {\n                        *val += mask_data[i];\n                    }\n                }\n            });\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::nn::Linear;\n\n    #[test]\n    fn test_mha_forward() {\n        // B=1, S=2, H=2, D=4 (Head Dim = 2)\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim; // 4\n\n        // Create dummy linear layers (identity weights for simplicity)\n        let weight_data = vec![\n            1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n        ]; // 4x4 Identity\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // Input [B, S, Hidden] -\u003e [1, 2, 4]\n        let input_data = vec![\n            1.0, 0.0, 1.0, 0.0, // Seq 1\n            0.0, 1.0, 0.0, 1.0, // Seq 2\n        ];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n\n        // RoPE cos/sin [S, HeadDim/2] -\u003e [2, 1] (since head_dim=2)\n        // Actually RoPE expects [S, HeadDim/2] for complex, but here implementation details might vary.\n        // Looking at rope.rs (not shown but inferred usage), usually [S, HeadDim/2] or [S, HeadDim].\n        // Let's assume [S, HeadDim/2] for complex rotation simulation or [S, HeadDim] for full rotation.\n        // The apply_rope signature is `freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e`.\n        // Let's use zeros/ones to be safe/no-op if possible or simple rotation.\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let output = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_mha_forward_with_mask() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        let weight_data = vec![1.0; 16]; // 4x4\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 8]; // 1x2x4\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        // Mask [S, S] -\u003e [2, 2]\n        let mask = Tensor::\u003cf32, 2\u003e::zeros([s, s]);\n\n        let output = mha\n            .forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, Some(\u0026mask))\n            .unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_mha_forward_gqa() {\n        // Grouped Query Attention: 4 heads, 2 KV heads\n        let b = 1;\n        let s = 2;\n        let num_heads = 4;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim; // 8\n        let kv_dim = num_kv_heads * head_dim; // 4\n\n        // Weights need to match dimensions\n        // Q: [Hidden, Hidden] -\u003e [8, 8]\n        // K, V: [Hidden, KV_Dim] -\u003e [8, 4] (Wait, Linear is [In, Out] or [Out, In]? Linear is usually x @ W.T + b.\n        // In xla-rs Linear, weight is [out_features, in_features].\n        // x is [B, S, Hidden].\n        // q_proj: [Hidden, Hidden] -\u003e Weight [8, 8]\n        // k_proj: [KV_Dim, Hidden] -\u003e Weight [4, 8]\n        // v_proj: [KV_Dim, Hidden] -\u003e Weight [4, 8]\n        // o_proj: [Hidden, Hidden] -\u003e Weight [8, 8]\n\n        let q_w = Tensor::new(vec![1.0; 64], [8, 8]).unwrap();\n        let k_w = Tensor::new(vec![1.0; 32], [4, 8]).unwrap();\n        let v_w = Tensor::new(vec![1.0; 32], [4, 8]).unwrap();\n        let o_w = Tensor::new(vec![1.0; 64], [8, 8]).unwrap();\n\n        let q_proj = Linear::new(q_w, None);\n        let k_proj = Linear::new(k_w, None);\n        let v_proj = Linear::new(v_w, None);\n        let o_proj = Linear::new(o_w, None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 16]; // 1x2x8\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let output = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n\n    #[test]\n    fn test_softmax_max_not_first() {\n        // Test case where max value is not at index 0 to cover \"v \u003e max_val\" branch\n        let b = 1;\n        let s = 2;\n        let num_heads = 1;\n        let num_kv_heads = 1;\n        let head_dim = 2;\n        let hidden_dim = 2;\n\n        // Identity weights\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0]; // 2x2\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [2, 2]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // S=2.\n        // Input:\n        // Seq1: [0, 1]\n        // Seq2: [0, 2]\n\n        let input_data = vec![0.0, 1.0, 0.0, 2.0];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        let _ = mha.forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, None).unwrap();\n    }\n\n    #[test]\n    fn test_repeat_kv_no_rep() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        // Dummy linear layers\n        let weight_data = vec![1.0; 16];\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        // Input [B, NumKV, S, D]\n        let input = Tensor::\u003cf32, 4\u003e::zeros([b, num_kv_heads, s, head_dim]);\n        let output = mha.repeat_kv(\u0026input).unwrap();\n\n        // Should be identical (clone)\n        assert_eq!(output.shape(), input.shape());\n    }\n\n    #[test]\n    fn test_mha_forward_mixed_mask() {\n        let b = 1;\n        let s = 2;\n        let num_heads = 2;\n        let num_kv_heads = 2;\n        let head_dim = 2;\n        let hidden_dim = num_heads * head_dim;\n\n        let weight_data = vec![1.0; 16];\n        let q_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let k_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let v_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n        let o_proj = Linear::new(Tensor::new(weight_data.clone(), [4, 4]).unwrap(), None);\n\n        let mha = MultiHeadAttention::new(\n            hidden_dim,\n            num_heads,\n            num_kv_heads,\n            head_dim,\n            q_proj,\n            k_proj,\n            v_proj,\n            o_proj,\n        );\n\n        let input_data = vec![1.0; 8];\n        let x = Tensor::\u003cf32, 3\u003e::new(input_data, [b, s, hidden_dim]).unwrap();\n        let freqs_cos = Tensor::\u003cf32, 2\u003e::ones([s, head_dim / 2]);\n        let freqs_sin = Tensor::\u003cf32, 2\u003e::zeros([s, head_dim / 2]);\n\n        // Mask [S, S] -\u003e [2, 2]\n        // [1.0, 0.0]\n        // [0.0, 1.0]\n        // 1.0 means \"keep\" (no change in additive mask logic if we assume 1.0 is identity? Wait)\n        // logic: if mask != 1.0 { val += mask }\n        // If mask is 1.0, we do nothing.\n        // If mask is 0.0, we add 0.0 (no change).\n        // Wait, usually mask is 0 for keep and -inf for mask out.\n        // Or 1 for keep and 0 for mask out (multiplicative).\n        // The code says: `if mask_data[i] != T::one() { *val += mask_data[i]; }`\n        // This implies if mask is 1.0, we do nothing.\n        // If mask is 0.0, we add 0.0.\n        // If mask is -1e9, we add -1e9.\n        // So to cover the `else` (do nothing), we need 1.0 in the mask.\n        let mask_data = vec![1.0, 0.0, 0.0, 1.0];\n        let mask = Tensor::\u003cf32, 2\u003e::new(mask_data, [s, s]).unwrap();\n\n        let output = mha\n            .forward(\u0026x, \u0026freqs_cos, \u0026freqs_sin, Some(\u0026mask))\n            .unwrap();\n        assert_eq!(output.shape(), \u0026[b, s, hidden_dim]);\n    }\n}\n","traces":[{"line":22,"address":[],"length":0,"stats":{"Line":8}},{"line":40,"address":[],"length":0,"stats":{"Line":32}},{"line":44,"address":[],"length":0,"stats":{"Line":8}},{"line":51,"address":[],"length":0,"stats":{"Line":24}},{"line":53,"address":[],"length":0,"stats":{"Line":32}},{"line":54,"address":[],"length":0,"stats":{"Line":32}},{"line":55,"address":[],"length":0,"stats":{"Line":32}},{"line":58,"address":[],"length":0,"stats":{"Line":48}},{"line":59,"address":[],"length":0,"stats":{"Line":48}},{"line":60,"address":[],"length":0,"stats":{"Line":48}},{"line":63,"address":[],"length":0,"stats":{"Line":24}},{"line":64,"address":[],"length":0,"stats":{"Line":24}},{"line":65,"address":[],"length":0,"stats":{"Line":24}},{"line":68,"address":[],"length":0,"stats":{"Line":40}},{"line":69,"address":[],"length":0,"stats":{"Line":40}},{"line":71,"address":[],"length":0,"stats":{"Line":24}},{"line":72,"address":[],"length":0,"stats":{"Line":24}},{"line":74,"address":[],"length":0,"stats":{"Line":4}},{"line":80,"address":[],"length":0,"stats":{"Line":24}},{"line":82,"address":[],"length":0,"stats":{"Line":48}},{"line":83,"address":[],"length":0,"stats":{"Line":40}},{"line":85,"address":[],"length":0,"stats":{"Line":32}},{"line":87,"address":[],"length":0,"stats":{"Line":232}},{"line":89,"address":[],"length":0,"stats":{"Line":10}},{"line":90,"address":[],"length":0,"stats":{"Line":8}},{"line":93,"address":[],"length":0,"stats":{"Line":24}},{"line":95,"address":[],"length":0,"stats":{"Line":40}},{"line":96,"address":[],"length":0,"stats":{"Line":32}},{"line":99,"address":[],"length":0,"stats":{"Line":48}},{"line":103,"address":[],"length":0,"stats":{"Line":24}},{"line":105,"address":[],"length":0,"stats":{"Line":48}},{"line":107,"address":[],"length":0,"stats":{"Line":24}},{"line":110,"address":[],"length":0,"stats":{"Line":9}},{"line":111,"address":[],"length":0,"stats":{"Line":45}},{"line":112,"address":[],"length":0,"stats":{"Line":18}},{"line":114,"address":[],"length":0,"stats":{"Line":9}},{"line":115,"address":[],"length":0,"stats":{"Line":1}},{"line":118,"address":[],"length":0,"stats":{"Line":40}},{"line":119,"address":[],"length":0,"stats":{"Line":24}},{"line":120,"address":[],"length":0,"stats":{"Line":24}},{"line":122,"address":[],"length":0,"stats":{"Line":16}},{"line":123,"address":[],"length":0,"stats":{"Line":72}},{"line":124,"address":[],"length":0,"stats":{"Line":96}},{"line":125,"address":[],"length":0,"stats":{"Line":96}},{"line":126,"address":[],"length":0,"stats":{"Line":96}},{"line":128,"address":[],"length":0,"stats":{"Line":160}},{"line":129,"address":[],"length":0,"stats":{"Line":160}},{"line":132,"address":[],"length":0,"stats":{"Line":8}},{"line":135,"address":[],"length":0,"stats":{"Line":8}},{"line":136,"address":[],"length":0,"stats":{"Line":16}},{"line":137,"address":[],"length":0,"stats":{"Line":84}},{"line":138,"address":[],"length":0,"stats":{"Line":104}},{"line":139,"address":[],"length":0,"stats":{"Line":208}},{"line":140,"address":[],"length":0,"stats":{"Line":104}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":104}},{"line":146,"address":[],"length":0,"stats":{"Line":312}},{"line":147,"address":[],"length":0,"stats":{"Line":624}},{"line":148,"address":[],"length":0,"stats":{"Line":520}},{"line":149,"address":[],"length":0,"stats":{"Line":208}},{"line":150,"address":[],"length":0,"stats":{"Line":104}},{"line":153,"address":[],"length":0,"stats":{"Line":104}},{"line":154,"address":[],"length":0,"stats":{"Line":312}},{"line":155,"address":[],"length":0,"stats":{"Line":104}},{"line":158,"address":[],"length":0,"stats":{"Line":8}},{"line":161,"address":[],"length":0,"stats":{"Line":2}},{"line":162,"address":[],"length":0,"stats":{"Line":4}},{"line":163,"address":[],"length":0,"stats":{"Line":6}},{"line":165,"address":[],"length":0,"stats":{"Line":4}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":6}},{"line":174,"address":[],"length":0,"stats":{"Line":2}},{"line":176,"address":[],"length":0,"stats":{"Line":4}},{"line":177,"address":[],"length":0,"stats":{"Line":6}},{"line":178,"address":[],"length":0,"stats":{"Line":44}},{"line":179,"address":[],"length":0,"stats":{"Line":28}},{"line":180,"address":[],"length":0,"stats":{"Line":12}},{"line":184,"address":[],"length":0,"stats":{"Line":2}}],"covered":76,"coverable":80},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","mod.rs"],"content":"use crate::models::gemma::attention::MultiHeadAttention;\nuse crate::nn::{Activation, Linear, RMSNorm};\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse std::ops::Add;\n\npub mod attention;\npub mod rope;\n\n#[derive(Debug, Clone)]\npub struct GemmaConfig {\n    pub hidden_size: usize,\n    pub intermediate_size: usize,\n    pub num_hidden_layers: usize,\n    pub num_attention_heads: usize,\n    pub num_key_value_heads: usize,\n    pub head_dim: usize,\n    pub rms_norm_eps: f32,\n    pub vocab_size: usize,\n}\n\nimpl GemmaConfig {\n    pub fn gemma_70b() -\u003e Self {\n        Self {\n            hidden_size: 8192,\n            intermediate_size: 32768,\n            num_hidden_layers: 80,\n            num_attention_heads: 64,\n            num_key_value_heads: 8,\n            head_dim: 128,\n            rms_norm_eps: 1e-6,\n            vocab_size: 256000,\n        }\n    }\n\n    pub fn tiny_test() -\u003e Self {\n        Self {\n            hidden_size: 64,\n            intermediate_size: 128,\n            num_hidden_layers: 2,\n            num_attention_heads: 4,\n            num_key_value_heads: 2,\n            head_dim: 16,\n            rms_norm_eps: 1e-6,\n            vocab_size: 100,\n        }\n    }\n}\n\n#[derive(Debug)]\npub struct MLP\u003cT: TensorElem\u003e {\n    pub gate_proj: Linear\u003cT\u003e,\n    pub up_proj: Linear\u003cT\u003e,\n    pub down_proj: Linear\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e MLP\u003cT\u003e {\n    pub fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let gate = self.gate_proj.forward(x)?;\n        let up = self.up_proj.forward(x)?;\n\n        let gate = Activation::silu(\u0026gate);\n        let fused = (\u0026gate * \u0026up)?;\n\n        self.down_proj.forward(\u0026fused)\n    }\n}\n\n#[derive(Debug)]\npub struct GemmaBlock\u003cT: TensorElem\u003e {\n    pub self_attn: MultiHeadAttention\u003cT\u003e,\n    pub mlp: MLP\u003cT\u003e,\n    pub input_layernorm: RMSNorm\u003cT\u003e,\n    pub post_attention_layernorm: RMSNorm\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e GemmaBlock\u003cT\u003e {\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let residual = x;\n\n        let norm_x = self.input_layernorm.forward(x)?;\n        let attn_out = self\n            .self_attn\n            .forward(\u0026norm_x, freqs_cos, freqs_sin, mask)?;\n\n        let x = (residual.add(\u0026attn_out))?;\n\n        let residual = \u0026x;\n        let norm_x = self.post_attention_layernorm.forward(\u0026x)?;\n        let mlp_out = self.mlp.forward(\u0026norm_x)?;\n\n        let x = (residual.add(\u0026mlp_out))?;\n\n        Ok(x)\n    }\n}\n\n/// The full Gemma Model.\n///\n/// Consists of a stack of `GemmaBlock` layers followed by a final RMSNorm.\n/// Note: This struct represents the transformer body. The embedding layer and language model head\n/// are typically handled separately or wrapped in a `GemmaForCausalLM` struct (not yet implemented).\n#[derive(Debug)]\npub struct GemmaModel\u003cT: TensorElem\u003e {\n    pub layers: Vec\u003cGemmaBlock\u003cT\u003e\u003e,\n    pub norm: RMSNorm\u003cT\u003e,\n}\n\nimpl\u003cT: TensorElem + Float\u003e GemmaModel\u003cT\u003e {\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n        freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n        mask: Option\u003c\u0026Tensor\u003cT, 2, Cpu\u003e\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let mut hidden = x.clone();\n\n        for layer in \u0026self.layers {\n            hidden = layer.forward(\u0026hidden, freqs_cos, freqs_sin, mask)?;\n        }\n\n        self.norm.forward(\u0026hidden)\n    }\n}\n","traces":[{"line":23,"address":[],"length":0,"stats":{"Line":0}},{"line":36,"address":[],"length":0,"stats":{"Line":1}},{"line":58,"address":[],"length":0,"stats":{"Line":3}},{"line":59,"address":[],"length":0,"stats":{"Line":12}},{"line":60,"address":[],"length":0,"stats":{"Line":12}},{"line":62,"address":[],"length":0,"stats":{"Line":9}},{"line":63,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":9}},{"line":78,"address":[],"length":0,"stats":{"Line":2}},{"line":85,"address":[],"length":0,"stats":{"Line":4}},{"line":87,"address":[],"length":0,"stats":{"Line":8}},{"line":88,"address":[],"length":0,"stats":{"Line":4}},{"line":89,"address":[],"length":0,"stats":{"Line":2}},{"line":90,"address":[],"length":0,"stats":{"Line":10}},{"line":92,"address":[],"length":0,"stats":{"Line":8}},{"line":94,"address":[],"length":0,"stats":{"Line":4}},{"line":95,"address":[],"length":0,"stats":{"Line":8}},{"line":96,"address":[],"length":0,"stats":{"Line":8}},{"line":98,"address":[],"length":0,"stats":{"Line":8}},{"line":100,"address":[],"length":0,"stats":{"Line":2}},{"line":116,"address":[],"length":0,"stats":{"Line":1}},{"line":123,"address":[],"length":0,"stats":{"Line":3}},{"line":125,"address":[],"length":0,"stats":{"Line":5}},{"line":126,"address":[],"length":0,"stats":{"Line":14}},{"line":129,"address":[],"length":0,"stats":{"Line":3}}],"covered":24,"coverable":25},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","gemma","rope.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\n\n/// Rotary Positional Embedding\n///\n/// Applies rotation to query and key tensors.\n/// x: [Batch, SeqLen, HeadDim] or [Batch, NumHeads, SeqLen, HeadDim]\n///\n/// Standard RoPE rotates adjacent pairs of elements.\npub fn apply_rope\u003cT: TensorElem\u003e(\n    x: \u0026Tensor\u003cT, 4, Cpu\u003e,\n    freqs_cos: \u0026Tensor\u003cT, 2, Cpu\u003e, // [SeqLen, HeadDim/2]\n    freqs_sin: \u0026Tensor\u003cT, 2, Cpu\u003e,\n) -\u003e Result\u003cTensor\u003cT, 4, Cpu\u003e\u003e {\n    // x shape: [Batch, Heads, SeqLen, HeadDim]\n    let [b, h, s, d] = *x.shape();\n\n    // This implementation assumes `d` is even.\n\n    let mut out = Tensor::zeros([b, h, s, d]);\n\n    // Parallelize\n    use rayon::prelude::*;\n\n    // We iterate over the flattened buffer for efficiency if possible, but indices are tricky.\n    // Let's iterate over Batch, Head, SeqLen.\n\n    out.data_mut()\n        .par_chunks_mut(s * d) // Chunk by (SeqLen * HeadDim) -\u003e one head's worth of data\n        .enumerate()\n        .for_each(|(_bh_idx, head_data)| {\n            // _bh_idx tracks batch and head, but we don't need them for freq lookup usually,\n            // unless freq depends on head (it doesn't usually).\n\n            // head_data is [SeqLen, HeadDim] flat\n            for t in 0..s {\n                let offset = t * d;\n                let freqs_idx = t; // corresponds to position\n\n                for i in 0..(d / 2) {\n                    let cos = freqs_cos.data()[freqs_idx * (d / 2) + i];\n                    let sin = freqs_sin.data()[freqs_idx * (d / 2) + i];\n\n                    let x0 = head_data[offset + 2 * i];\n                    let x1 = head_data[offset + 2 * i + 1];\n\n                    // Rotate\n                    head_data[offset + 2 * i] = x0 * cos - x1 * sin;\n                    head_data[offset + 2 * i + 1] = x0 * sin + x1 * cos;\n                }\n            }\n        });\n\n    Ok(out)\n}\n\n/// Precompute frequency cis for RoPE\npub fn precompute_freqs_cis\u003cT: TensorElem + Float\u003e(\n    dim: usize,\n    max_seq_len: usize,\n    theta: T,\n) -\u003e (Tensor\u003cT, 2, Cpu\u003e, Tensor\u003cT, 2, Cpu\u003e) {\n    let half_dim = dim / 2;\n    let mut cos_out = Tensor::zeros([max_seq_len, half_dim]);\n    let mut sin_out = Tensor::zeros([max_seq_len, half_dim]);\n\n    let data_cos = cos_out.data_mut();\n    let data_sin = sin_out.data_mut();\n\n    for seq in 0..max_seq_len {\n        for i in 0..half_dim {\n            // freq = 1.0 / (theta ^ (2*i / dim))\n            let freq =\n                T::one() / theta.powf(T::from_usize(2 * i).unwrap() / T::from_usize(dim).unwrap());\n            let val = T::from_usize(seq).unwrap() * freq;\n\n            data_cos[seq * half_dim + i] = val.cos();\n            data_sin[seq * half_dim + i] = val.sin();\n        }\n    }\n\n    (cos_out, sin_out)\n}\n","traces":[{"line":10,"address":[],"length":0,"stats":{"Line":16}},{"line":16,"address":[],"length":0,"stats":{"Line":80}},{"line":20,"address":[],"length":0,"stats":{"Line":80}},{"line":28,"address":[],"length":0,"stats":{"Line":16}},{"line":29,"address":[],"length":0,"stats":{"Line":32}},{"line":31,"address":[],"length":0,"stats":{"Line":60}},{"line":36,"address":[],"length":0,"stats":{"Line":132}},{"line":37,"address":[],"length":0,"stats":{"Line":176}},{"line":38,"address":[],"length":0,"stats":{"Line":176}},{"line":40,"address":[],"length":0,"stats":{"Line":800}},{"line":41,"address":[],"length":0,"stats":{"Line":1780}},{"line":42,"address":[],"length":0,"stats":{"Line":1780}},{"line":44,"address":[],"length":0,"stats":{"Line":1068}},{"line":45,"address":[],"length":0,"stats":{"Line":1068}},{"line":48,"address":[],"length":0,"stats":{"Line":1424}},{"line":49,"address":[],"length":0,"stats":{"Line":1068}},{"line":54,"address":[],"length":0,"stats":{"Line":16}},{"line":58,"address":[],"length":0,"stats":{"Line":2}},{"line":63,"address":[],"length":0,"stats":{"Line":4}},{"line":64,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":6}},{"line":67,"address":[],"length":0,"stats":{"Line":6}},{"line":68,"address":[],"length":0,"stats":{"Line":6}},{"line":70,"address":[],"length":0,"stats":{"Line":22}},{"line":71,"address":[],"length":0,"stats":{"Line":220}},{"line":73,"address":[],"length":0,"stats":{"Line":200}},{"line":74,"address":[],"length":0,"stats":{"Line":900}},{"line":75,"address":[],"length":0,"stats":{"Line":500}},{"line":77,"address":[],"length":0,"stats":{"Line":300}},{"line":78,"address":[],"length":0,"stats":{"Line":200}},{"line":82,"address":[],"length":0,"stats":{"Line":2}}],"covered":31,"coverable":31},{"path":["/","Users","blitz","my-oss","xla-rs","src","models","mod.rs"],"content":"pub mod gemma;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","activation.rs"],"content":"use crate::tensor::{Cpu, Tensor, TensorElem};\nuse num_traits::Float;\n\n/// Computes the SiLU (Sigmoid Linear Unit) activation function.\n///\n/// $$ \\text{SiLU}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}} $$\npub fn silu\u003cT: TensorElem + Float\u003e(x: T) -\u003e T {\n    let val = x.to_f32().unwrap();\n    let sig = 1.0 / (1.0 + (-val).exp());\n    T::from_f32(val * sig).unwrap()\n}\n\n/// Activation functions namespace.\npub struct Activation;\n\nimpl Activation {\n    /// Applies the SiLU activation function element-wise to a tensor.\n    pub fn silu\u003cconst RANK: usize, T: TensorElem + Float\u003e(\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Tensor\u003cT, RANK, Cpu\u003e {\n        x.map(silu)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_silu_value() {\n        let val = 2.0f32;\n        let res = silu(val);\n        // silu(2) = 2 * sigmoid(2) = 2 * (1 / (1 + exp(-2)))\n        // exp(-2) approx 0.135335\n        // 1 / 1.135335 approx 0.880797\n        // 2 * 0.880797 approx 1.76159\n        assert!((res - 1.76159).abs() \u003c 1e-4);\n    }\n\n    #[test]\n    fn test_activation_silu_tensor() {\n        let data = vec![0.0, 2.0];\n        let tensor = Tensor::\u003cf32, 1, Cpu\u003e::new(data, [2]).unwrap();\n        let res = Activation::silu(\u0026tensor);\n\n        let res_data = res.data();\n        assert!((res_data[0] - 0.0).abs() \u003c 1e-6); // 0 * sigmoid(0) = 0\n        assert!((res_data[1] - 1.76159).abs() \u003c 1e-4);\n    }\n}\n","traces":[{"line":7,"address":[],"length":0,"stats":{"Line":771}},{"line":8,"address":[],"length":0,"stats":{"Line":3084}},{"line":9,"address":[],"length":0,"stats":{"Line":1542}},{"line":10,"address":[],"length":0,"stats":{"Line":2313}},{"line":18,"address":[],"length":0,"stats":{"Line":4}},{"line":21,"address":[],"length":0,"stats":{"Line":8}}],"covered":6,"coverable":6},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","linear.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\n\nuse rayon::prelude::*;\n\n/// Constants for Linear Layer\nconst WEIGHT_RANK: usize = 2;\nconst BIAS_RANK: usize = 1;\n\n/// Trait to enforce allowed ranks for Linear layer forward pass.\npub trait AllowedLinearRank\u003cconst N: usize\u003e {}\nimpl AllowedLinearRank\u003c2\u003e for () {}\nimpl AllowedLinearRank\u003c3\u003e for () {}\n\n/// # Design Philosophy: Fixed Ranks\n///\n/// The `Linear` layer enforces `WEIGHT_RANK = 2` and `BIAS_RANK = 1`.\n/// This is a deliberate design choice to align with the mathematical definition of a Linear (or Fully Connected) layer:\n/// $$y = xA^T + b$$\n///\n/// - **Weights ($A$):** Must be a Matrix (Rank 2) mapping `in_features` $\\to$ `out_features`.\n/// - **Bias ($b$):** Must be a Vector (Rank 1) matching `out_features`.\n///\n/// While one might want to use a `Tensor\u003cT, 16, Cpu\u003e` as weights, doing so would strictly no longer be a\n/// standard \"Linear\" layer operation (it would be a Tensor Contraction or specialized convolution).\n/// By enforcing these ranks, we keep the `Linear` abstraction clean, predictable, and mathematically correct.\n/// If higher-dimensional weights are needed, they should be explicitly reshaped or flattened before being\n/// passed to a Linear layer.\n///\n/// Linear Layer: `y = xA^T + b`\n///\n/// Performs a linear transformation on the input data.\n/// This layer represents a collection of neurons where every input is connected to every output.\n///\n/// # Generics\n/// - `T`: The element type of the tensors (e.g., `f32`, `f64`).\n///\n/// # Examples\n/// ```rust\n/// use xla_rs::nn::Linear;\n/// use xla_rs::tensor::Tensor;\n/// // Create a layer with 10 inputs and 5 outputs\n/// let layer = Linear::\u003cf32\u003e::new(\n///     Tensor::zeros([5, 10]), // Weights: [out, in]\n///     Some(Tensor::zeros([5])) // Bias: [out]\n/// );\n/// ```\n#[derive(Debug)]\npub struct Linear\u003cT: TensorElem\u003e {\n    /// The learnable weights of the layer.\n    /// - Shape: `[out_features, in_features]`\n    pub weight: Tensor\u003cT, WEIGHT_RANK, Cpu\u003e,\n\n    /// The learnable bias of the layer.\n    /// - Shape: `[out_features]`\n    pub bias: Option\u003cTensor\u003cT, BIAS_RANK, Cpu\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem\u003e Linear\u003cT\u003e {\n    /// Creates a new Linear layer.\n    ///\n    /// # Arguments\n    ///\n    /// * `weight` - The weight tensor of shape `[out_features, in_features]`.\n    /// * `bias` - The optional bias tensor of shape `[out_features]`.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::nn::Linear;\n    /// use xla_rs::tensor::Tensor;\n    ///\n    /// let weight = Tensor::\u003cf32, 2, _\u003e::zeros([5, 10]);\n    /// let layer = Linear::new(weight, None);\n    /// ```\n    pub fn new(\n        weight: Tensor\u003cT, WEIGHT_RANK, Cpu\u003e,\n        bias: Option\u003cTensor\u003cT, BIAS_RANK, Cpu\u003e\u003e,\n    ) -\u003e Self {\n        Self { weight, bias }\n    }\n\n    /// Performs the forward pass of the Linear layer.\n    ///\n    /// Supports inputs of Rank 2 `[batch_size, in_features]` or Rank 3 `[batch_size, seq_len, in_features]`.\n    ///\n    /// # Arguments\n    ///\n    /// * `x` - The input tensor.\n    ///\n    /// # Returns\n    ///\n    /// The output tensor after applying the linear transformation.\n    pub fn forward\u003cconst RANK: usize\u003e(\n        \u0026self,\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\n    where\n        (): AllowedLinearRank\u003cRANK\u003e,\n    {\n        // Compile-time check (redundant with trait bound but satisfies request for safer check if desired,\n        // but trait bound `AllowedLinearRank` actually prevents compilation for other ranks, so strictly unnecessary\n        // to assert constant. However, we'll stick to the logic that handles 2 and 3).\n\n        let w_t = self.weight.transpose()?;\n\n        if RANK == 3 {\n            let [b, s, i] = x.shape()[0..3].try_into().unwrap();\n            // Explicitly specify type for flat_x\n            let flat_x: Tensor\u003cT, 2, Cpu\u003e = x.clone().reshape([b * s, i])?;\n            let out_flat = flat_x.matmul(\u0026w_t)?;\n\n            let out_biased = if let Some(b_bias) = \u0026self.bias {\n                Self::add_bias(\u0026out_flat, b_bias)?\n            } else {\n                out_flat\n            };\n\n            let out_features = self.weight.shape()[0];\n            let res = out_biased.reshape([b, s, out_features])?;\n\n            unsafe {\n                let res_ptr = \u0026res as *const Tensor\u003cT, 3, Cpu\u003e;\n                let ret = std::ptr::read(res_ptr as *const Tensor\u003cT, RANK, Cpu\u003e);\n                std::mem::forget(res);\n                Ok(ret)\n            }\n        } else {\n            // RANK == 2 guaranteed by AllowedLinearRank\u003cRANK\u003e if not 3\n\n            // We need to cast `w_t` (Rank 2) to `Tensor\u003cT, RANK\u003e` unsafely to call `matmul`\n            // because compiler sees generic RANK.\n\n            let w_t_cast: \u0026Tensor\u003cT, RANK, Cpu\u003e = unsafe { std::mem::transmute(\u0026w_t) };\n\n            let out = x.matmul(w_t_cast)?;\n            let out = if let Some(b) = \u0026self.bias {\n                let shape = out.shape();\n                let [_rows, cols] = [shape[0], shape[1]];\n\n                if b.shape()[0] != cols {\n                    return Err(crate::tensor::TensorError::ShapeMismatch {\n                        expected: vec![cols],\n                        got: vec![b.shape()[0]],\n                    });\n                }\n\n                let bias_data = b.data();\n                let mut out_mut = out;\n                let out_slice = out_mut.data_mut();\n\n                out_slice.par_chunks_mut(cols).for_each(|row| {\n                    for (r, bv) in row.iter_mut().zip(bias_data.iter()) {\n                        *r += *bv;\n                    }\n                });\n                out_mut\n            } else {\n                out\n            };\n            Ok(out)\n        }\n    }\n\n    /// Helper to add bias to a 2D tensor.\n    fn add_bias(x: \u0026Tensor\u003cT, 2, Cpu\u003e, bias: \u0026Tensor\u003cT, 1, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 2, Cpu\u003e\u003e {\n        let [_, cols] = *x.shape();\n        let [b_cols] = *bias.shape();\n\n        if cols != b_cols {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![cols],\n                got: vec![b_cols],\n            });\n        }\n\n        let mut out = x.clone();\n\n        out.data_mut().par_chunks_mut(cols).for_each(|row| {\n            for (r, b) in row.iter_mut().zip(bias.data().iter()) {\n                *r += *b;\n            }\n        });\n\n        Ok(out)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_linear_new() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([5, 10]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([5]);\n        let layer = Linear::new(weight, Some(bias));\n        assert!(layer.bias.is_some());\n    }\n\n    #[test]\n    fn test_linear_forward_rank2() {\n        // Input: [2, 3]\n        // Weight: [4, 3] (out=4, in=3)\n        // Bias: [4]\n        let input_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::new(input_data, [2, 3]).unwrap();\n\n        let weight_data = vec![\n            1.0, 0.0, 0.0, // 1st neuron\n            0.0, 1.0, 0.0, // 2nd neuron\n            0.0, 0.0, 1.0, // 3rd neuron\n            1.0, 1.0, 1.0, // 4th neuron\n        ];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [4, 3]).unwrap();\n\n        let bias_data = vec![0.1, 0.2, 0.3, 0.4];\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::new(bias_data, [4]).unwrap();\n\n        let layer = Linear::new(weight, Some(bias));\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[2, 4]);\n        // Row 1: [1, 2, 3]\n        // Out 1: 1*1 + 0.1 = 1.1\n        // Out 2: 2*1 + 0.2 = 2.2\n        // Out 3: 3*1 + 0.3 = 3.3\n        // Out 4: (1+2+3) + 0.4 = 6.4\n        let out_data = output.data();\n        assert!((out_data[0] - 1.1).abs() \u003c 1e-6);\n        assert!((out_data[1] - 2.2).abs() \u003c 1e-6);\n        assert!((out_data[2] - 3.3).abs() \u003c 1e-6);\n        assert!((out_data[3] - 6.4).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3() {\n        // Input: [1, 2, 3] (Batch=1, Seq=2, In=3)\n        let input_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 3]).unwrap();\n\n        let weight_data = vec![\n            1.0, 1.0, 1.0, // 1st neuron\n            2.0, 2.0, 2.0, // 2nd neuron\n        ];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 3]).unwrap();\n\n        let layer = Linear::new(weight, None);\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2, 2]);\n        // Row 1: [1, 2, 3] -\u003e Sum=6. Out1=6, Out2=12\n        // Row 2: [4, 5, 6] -\u003e Sum=15. Out1=15, Out2=30\n        let out_data = output.data();\n        assert!((out_data[0] - 6.0).abs() \u003c 1e-6);\n        assert!((out_data[1] - 12.0).abs() \u003c 1e-6);\n        assert!((out_data[2] - 15.0).abs() \u003c 1e-6);\n        assert!((out_data[3] - 30.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_shape_mismatch() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([5, 10]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([4]); // Wrong size\n        let layer = Linear::new(weight, Some(bias));\n\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 10]);\n        // This should fail inside forward when adding bias, or ideally we check in new?\n        // Current impl checks in add_bias which is called in forward.\n        // Actually, let's check add_bias directly via private access if possible or just run forward.\n        // Since we are in the same module (submodule tests), we can test private methods if we want,\n        // but forward is public.\n\n        let res = layer.forward(\u0026input);\n        assert!(res.is_err());\n    }\n\n    #[test]\n    fn test_linear_forward_rank2_no_bias() {\n        let input_data = vec![1.0, 2.0];\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::new(input_data, [1, 2]).unwrap();\n\n        // Weight: [2, 2]\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 2]).unwrap();\n\n        let layer = Linear::new(weight, None);\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2]);\n        let out_data = output.data();\n        assert!((out_data[0] - 1.0).abs() \u003c 1e-6);\n        assert!((out_data[1] - 2.0).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3_with_bias() {\n        // Input: [1, 2, 2] (Batch=1, Seq=2, In=2)\n        let input_data = vec![1.0, 1.0, 2.0, 2.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 2]).unwrap();\n\n        // Weight: [2, 2] (Identity)\n        let weight_data = vec![1.0, 0.0, 0.0, 1.0];\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::new(weight_data, [2, 2]).unwrap();\n\n        // Bias: [2]\n        let bias_data = vec![0.5, 0.5];\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::new(bias_data, [2]).unwrap();\n\n        let layer = Linear::new(weight, Some(bias));\n        let output = layer.forward(\u0026input).unwrap();\n\n        assert_eq!(output.shape(), \u0026[1, 2, 2]);\n        let out_data = output.data();\n        // Row 1: [1, 1] + [0.5, 0.5] = [1.5, 1.5]\n        // Row 2: [2, 2] + [0.5, 0.5] = [2.5, 2.5]\n        assert!((out_data[0] - 1.5).abs() \u003c 1e-6);\n        assert!((out_data[1] - 1.5).abs() \u003c 1e-6);\n        assert!((out_data[2] - 2.5).abs() \u003c 1e-6);\n        assert!((out_data[3] - 2.5).abs() \u003c 1e-6);\n    }\n\n    #[test]\n    fn test_linear_forward_rank3_mismatch() {\n        let weight = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 2]);\n        let bias = Tensor::\u003cf32, 1, Cpu\u003e::zeros([3]); // Wrong size\n        let layer = Linear::new(weight, Some(bias));\n\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::zeros([1, 2, 2]);\n        let res = layer.forward(\u0026input);\n        assert!(res.is_err());\n    }\n}\n","traces":[{"line":75,"address":[],"length":0,"stats":{"Line":47}},{"line":93,"address":[],"length":0,"stats":{"Line":49}},{"line":104,"address":[],"length":0,"stats":{"Line":147}},{"line":106,"address":[],"length":0,"stats":{"Line":49}},{"line":107,"address":[],"length":0,"stats":{"Line":225}},{"line":109,"address":[],"length":0,"stats":{"Line":270}},{"line":110,"address":[],"length":0,"stats":{"Line":180}},{"line":112,"address":[],"length":0,"stats":{"Line":91}},{"line":113,"address":[],"length":0,"stats":{"Line":7}},{"line":115,"address":[],"length":0,"stats":{"Line":43}},{"line":118,"address":[],"length":0,"stats":{"Line":132}},{"line":119,"address":[],"length":0,"stats":{"Line":220}},{"line":122,"address":[],"length":0,"stats":{"Line":88}},{"line":123,"address":[],"length":0,"stats":{"Line":132}},{"line":124,"address":[],"length":0,"stats":{"Line":88}},{"line":125,"address":[],"length":0,"stats":{"Line":44}},{"line":133,"address":[],"length":0,"stats":{"Line":16}},{"line":135,"address":[],"length":0,"stats":{"Line":16}},{"line":136,"address":[],"length":0,"stats":{"Line":10}},{"line":137,"address":[],"length":0,"stats":{"Line":9}},{"line":138,"address":[],"length":0,"stats":{"Line":9}},{"line":140,"address":[],"length":0,"stats":{"Line":6}},{"line":141,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":3}},{"line":143,"address":[],"length":0,"stats":{"Line":3}},{"line":147,"address":[],"length":0,"stats":{"Line":6}},{"line":148,"address":[],"length":0,"stats":{"Line":4}},{"line":149,"address":[],"length":0,"stats":{"Line":6}},{"line":151,"address":[],"length":0,"stats":{"Line":11}},{"line":152,"address":[],"length":0,"stats":{"Line":45}},{"line":153,"address":[],"length":0,"stats":{"Line":10}},{"line":156,"address":[],"length":0,"stats":{"Line":2}},{"line":158,"address":[],"length":0,"stats":{"Line":1}},{"line":160,"address":[],"length":0,"stats":{"Line":3}},{"line":165,"address":[],"length":0,"stats":{"Line":2}},{"line":166,"address":[],"length":0,"stats":{"Line":4}},{"line":167,"address":[],"length":0,"stats":{"Line":4}},{"line":169,"address":[],"length":0,"stats":{"Line":2}},{"line":170,"address":[],"length":0,"stats":{"Line":1}},{"line":171,"address":[],"length":0,"stats":{"Line":3}},{"line":172,"address":[],"length":0,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":3}},{"line":178,"address":[],"length":0,"stats":{"Line":6}},{"line":179,"address":[],"length":0,"stats":{"Line":22}},{"line":180,"address":[],"length":0,"stats":{"Line":4}},{"line":184,"address":[],"length":0,"stats":{"Line":1}}],"covered":46,"coverable":46},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","mod.rs"],"content":"pub mod activation;\npub mod linear;\npub mod module;\npub mod moe;\npub mod norm;\n\npub use activation::Activation;\npub use linear::{AllowedLinearRank, Linear};\npub use module::Module;\npub use norm::RMSNorm;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","module.rs"],"content":"use crate::tensor::TensorElem;\nuse std::fmt::Debug;\n\n/// A Module trait for Neural Network layers.\n///\n/// # Why is this needed?\n///\n/// The `Module` trait serves as the fundamental building block for all neural network components\n/// in `xla-rs`. It enforces a common interface that ensures:\n///\n/// 1.  **Thread Safety**: By requiring `Send` and `Sync`, we ensure that models can be safely\n///     shared across threads (e.g., for parallel inference or data loading).\n/// 2.  **Debuggability**: Requiring `Debug` ensures that the structure of any model can be\n///     easily inspected.\n/// 3.  **Extensibility**: While currently a marker trait, this abstraction allows us to generically\n///     implement features like parameter counting, device movement, and serialization for all layers\n///     in the future without breaking changes.\npub trait Module\u003cT: TensorElem\u003e: Debug + Send + Sync {}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[derive(Debug)]\n    struct MockModule;\n\n    impl Module\u003cf32\u003e for MockModule {}\n\n    #[test]\n    fn test_module_implementation() {\n        let module = MockModule;\n        // Just verify it compiles and runs\n        println!(\"{:?}\", module);\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","moe.rs"],"content":"//! # Mixture of Experts (MoE)\n//!\n//! This module implements a sparse Mixture of Experts (MoE) layer, a technique to scale model capacity\n//! without proportionally increasing computational cost.\n//!\n//! ## What is Mixture of Experts?\n//!\n//! In a standard dense model, every input token is processed by every parameter in the network.\n//! In an MoE model, the \"FeedForward\" block is replaced by a set of \"Experts\" (usually smaller FeedForward networks)\n//! and a \"Router\" (or Gate). For each token, the Router selects a small subset (Top-K) of experts to process it.\n//!\n//! $$ \\text{Output} = \\sum_{i \\in \\text{TopK}} w_i \\cdot \\text{Expert}_i(x) $$\n//!\n//! This allows the model to have a massive number of parameters (high capacity) while only using a fraction\n//! of them per token (low inference latency).\n//!\n//! ## Implementation Details\n//!\n//! This implementation provides:\n//! - **`TopKRouter`**: A learnable gating mechanism that projects inputs to expert logits and selects the top-k indices.\n//! - **`MoELayer`**: The container that holds the router and the list of experts.\n//! - **`Expert` Trait**: An abstraction for what constitutes an expert (typically an MLP).\n//!\n//! ### Routing Mechanism\n//!\n//! We use a standard Top-K routing mechanism:\n//! 1.  Compute logits: $H(x) = x \\cdot W_{gate}$\n//! 2.  Select Top-K: Identify the $k$ experts with the highest logits.\n//! 3.  Normalize: Apply Softmax to the selected logits to get routing weights.\n//! 4.  Dispatch: Send tokens to their respective experts.\n//! 5.  Combine: Weighted sum of expert outputs.\n//!\n//! ## Trade-offs and Design Decisions\n//!\n//! ### 1. Explicit Loops vs. Scatter/Gather\n//! **Decision**: We use explicit iteration and grouping (bucketing) of tokens per expert rather than\n//! optimized scatter/gather tensor operations.\n//!\n//! **Why?**\n//! - **Simplicity**: `xla-rs` is designed for clarity and education. Implementing efficient sparse scatter/gather\n//!   kernels is complex and hardware-specific.\n//! - **CPU Focus**: On CPU, the overhead of grouping tokens is often negligible compared to the matrix multiplications\n//!   inside the experts. Explicit grouping allows us to use standard dense matrix multiplication for each expert,\n//!   which is well-optimized.\n//!\n//! ### 2. Dynamic Control Flow\n//! **Decision**: The routing logic dynamically constructs batches for each expert at runtime.\n//!\n//! **Why?**\n//! - This avoids padding and wasted computation associated with fixed-size expert buffers (common in TPU/GPU implementations).\n//! - It handles load imbalance naturally (though extreme imbalance can still hurt performance due to stragglers).\n//!\n//! ### 3. Generic Experts\n//! **Decision**: Experts are generic modules implementing the `Expert` trait.\n//!\n//! **Why?**\n//! - Allows experimenting with different expert architectures (e.g., different activation functions,\n//!   or even nested MoEs) without changing the routing logic.\n\nuse crate::nn::{Linear, Module};\nuse crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n/// Top-K Router for Mixture of Experts.\n///\n/// Routes inputs to the top-k experts based on gate logits.\n#[derive(Debug)]\npub struct TopKRouter\u003cT: TensorElem\u003e {\n    pub gate: Linear\u003cT\u003e,\n    pub num_experts: usize,\n    pub k: usize,\n}\n\nimpl\u003cT: TensorElem + Float\u003e TopKRouter\u003cT\u003e {\n    /// Creates a new TopKRouter.\n    ///\n    /// # Arguments\n    ///\n    /// * `gate` - The linear layer used to compute routing logits.\n    /// * `num_experts` - Total number of experts.\n    /// * `k` - Number of experts to route to per token.\n    pub fn new(gate: Linear\u003cT\u003e, num_experts: usize, k: usize) -\u003e Self {\n        Self {\n            gate,\n            num_experts,\n            k,\n        }\n    }\n\n    /// Performs routing.\n    ///\n    /// # Returns\n    ///\n    /// A tuple containing:\n    /// * `weights` - The routing weights for the top-k experts.\n    /// * `indices` - The indices of the top-k experts.\n    pub fn forward(\n        \u0026self,\n        x: \u0026Tensor\u003cT, 3, Cpu\u003e,\n    ) -\u003e Result\u003c(Tensor\u003cT, 3, Cpu\u003e, Tensor\u003cusize, 3, Cpu\u003e)\u003e {\n        let logits = self.gate.forward(x)?;\n\n        let [b, s, n_e] = *logits.shape();\n\n        let mut weights_out = Tensor::zeros([b, s, self.k]);\n        let mut indices_out = Tensor::zeros([b, s, self.k]);\n\n        weights_out\n            .data_mut()\n            .par_chunks_mut(self.k)\n            .zip(indices_out.data_mut().par_chunks_mut(self.k))\n            .zip(logits.data().par_chunks(n_e))\n            .for_each(|((w_row, i_row), l_row)| {\n                let mut pairs: Vec\u003c(T, usize)\u003e = l_row\n                    .iter()\n                    .copied()\n                    .enumerate()\n                    .map(|(i, v)| (v, i))\n                    .collect();\n\n                pairs.sort_by(|a, b| b.0.partial_cmp(\u0026a.0).unwrap_or(std::cmp::Ordering::Equal));\n\n                let top_k = \u0026pairs[0..self.k];\n\n                let max_val = top_k[0].0;\n                let mut sum_exp = T::zero();\n                let mut exps = Vec::with_capacity(self.k);\n\n                for (val, _) in top_k {\n                    let exp_v = (*val - max_val).to_f32().unwrap().exp();\n                    let exp_v_t = T::from_f32(exp_v).unwrap();\n                    sum_exp += exp_v_t;\n                    exps.push(exp_v_t);\n                }\n\n                let inv_sum = T::one() / sum_exp;\n\n                for idx in 0..self.k {\n                    w_row[idx] = exps[idx] * inv_sum;\n                    i_row[idx] = top_k[idx].1;\n                }\n            });\n\n        Ok((weights_out, indices_out))\n    }\n}\n\n/// Mixture of Experts Layer.\n///\n/// Consists of a router and a set of experts.\n#[derive(Debug)]\npub struct MoELayer\u003cT: TensorElem, E: Module\u003cT\u003e\u003e {\n    pub router: TopKRouter\u003cT\u003e,\n    pub experts: Vec\u003cE\u003e,\n}\n\nimpl\u003cT: TensorElem + Float, E: Module\u003cT\u003e\u003e MoELayer\u003cT, E\u003e {\n    /// Creates a new MoELayer.\n    pub fn new(router: TopKRouter\u003cT\u003e, experts: Vec\u003cE\u003e) -\u003e Self {\n        Self { router, experts }\n    }\n\n    // Note: This forward signature assumes E (Expert) takes Tensor\u003cT, 3\u003e and returns Tensor\u003cT, 3\u003e.\n    // Since Module trait is generic and doesn't enforce forward signature (Rust traits can't easily enforce generic methods with varying ranks),\n    // we might need to assume E has a forward method or define a more specific Expert trait.\n    // For now, we'll implement it assuming E has a forward method compatible with MLP.\n    // But wait, Module trait is empty marker.\n    // We should probably add `forward` to Module or create `Expert` trait.\n    // For this refactor, I'll keep it simple and assume E is MLP-like but we can't call forward on generic E without a trait method.\n    // So I will define a trait `Expert` here or use `Module` if I update it.\n    // Let's update `Module` trait in a separate step if needed, or just define `Expert` trait here.\n\n    // Actually, to avoid changing `Module` trait too much right now, let's define a local trait or just rely on the fact that we moved it.\n    // But the plan said \"Make MoELayer generic over Expert\".\n    // Let's define `Expert` trait in this file for now.\n}\n\n/// Trait for Experts in MoE.\n///\n/// Experts must implement `Module` and provide a `forward` method accepting Rank 3 tensors.\npub trait Expert\u003cT: TensorElem\u003e: Module\u003cT\u003e {\n    fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e;\n}\n\nimpl\u003cT: TensorElem + Float, E: Expert\u003cT\u003e\u003e MoELayer\u003cT, E\u003e {\n    /// Performs the forward pass of the MoE layer.\n    ///\n    /// Routes inputs to experts and aggregates the results.\n    pub fn forward(\u0026self, x: \u0026Tensor\u003cT, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cT, 3, Cpu\u003e\u003e {\n        let [b, s, h] = *x.shape();\n        let (weights, indices) = self.router.forward(x)?;\n\n        let mut final_output = Tensor::zeros([b, s, h]);\n\n        let mut assignments: Vec\u003cVec\u003cusize\u003e\u003e = vec![vec![]; self.experts.len()];\n        let mut assignment_weights: Vec\u003cVec\u003cT\u003e\u003e = vec![vec![]; self.experts.len()];\n\n        let w_data = weights.data();\n        let i_data = indices.data();\n\n        for idx in 0..(b * s) {\n            for k_i in 0..self.router.k {\n                let expert_idx = i_data[idx * self.router.k + k_i];\n                let weight = w_data[idx * self.router.k + k_i];\n                assignments[expert_idx].push(idx);\n                assignment_weights[expert_idx].push(weight);\n            }\n        }\n\n        type ExpertResult\u003cT\u003e = Option\u003c(Vec\u003cusize\u003e, Tensor\u003cT, 2, Cpu\u003e, Vec\u003cT\u003e)\u003e;\n        let results: Vec\u003cExpertResult\u003cT\u003e\u003e = self\n            .experts\n            .par_iter()\n            .enumerate()\n            .map(|(e_idx, expert)| {\n                let indices: \u0026Vec\u003cusize\u003e = \u0026assignments[e_idx];\n                if indices.is_empty() {\n                    return None;\n                }\n\n                let num_samples = indices.len();\n                let mut input_data = Vec::with_capacity(num_samples * h);\n\n                for \u0026token_idx in indices {\n                    let start = token_idx * h;\n                    input_data.extend_from_slice(\u0026x.data()[start..start + h]);\n                }\n\n                let input_tensor = Tensor::new(input_data, [num_samples, h]).unwrap();\n\n                let input_3d = input_tensor.reshape([num_samples, 1, h]).unwrap();\n\n                let output_3d = expert.forward(\u0026input_3d).unwrap();\n                let output_2d = output_3d.reshape([num_samples, h]).unwrap();\n\n                Some((\n                    indices.clone(),\n                    output_2d,\n                    assignment_weights[e_idx].clone(),\n                ))\n            })\n            .collect();\n\n        let out_data = final_output.data_mut();\n\n        for (indices, output_tensor, weights) in results.into_iter().flatten() {\n            let out_vals = output_tensor.data();\n            for (i, \u0026token_idx) in indices.iter().enumerate() {\n                let weight = weights[i];\n                let out_offset = token_idx * h;\n                let val_offset = i * h;\n\n                for j in 0..h {\n                    out_data[out_offset + j] += out_vals[val_offset + j] * weight;\n                }\n            }\n        }\n\n        Ok(final_output)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    // Mock Expert\n    #[derive(Debug)]\n    struct MockExpert {\n        id: usize,\n    }\n\n    impl Module\u003cf32\u003e for MockExpert {}\n\n    impl Expert\u003cf32\u003e for MockExpert {\n        fn forward(\u0026self, x: \u0026Tensor\u003cf32, 3, Cpu\u003e) -\u003e Result\u003cTensor\u003cf32, 3, Cpu\u003e\u003e {\n            // Expert returns input * id\n            let scale = self.id as f32;\n            let out = x.map(|v| v * scale);\n            Ok(out)\n        }\n    }\n\n    #[test]\n    fn test_moe_forward() {\n        // 2 Experts, Top-1 Routing\n        // Input: [1, 2, 2] (Batch=1, Seq=2, Dim=2)\n        // Router Gate: Identity-like to force routing\n\n        // Input:\n        // [[1.0, 0.0],  -\u003e Should route to Expert 0 if gate favors index 0\n        //  [0.0, 1.0]]  -\u003e Should route to Expert 1 if gate favors index 1\n\n        let input_data = vec![1.0, 0.0, 0.0, 1.0];\n        let input = Tensor::\u003cf32, 3, Cpu\u003e::new(input_data, [1, 2, 2]).unwrap();\n\n        // Gate weights: Identity [2, 2]\n        // [1, 0]\n        // [0, 1]\n        let gate_w_data = vec![1.0, 0.0, 0.0, 1.0];\n        let gate_w = Tensor::\u003cf32, 2, Cpu\u003e::new(gate_w_data, [2, 2]).unwrap();\n        let gate = Linear::new(gate_w, None);\n\n        let router = TopKRouter::new(gate, 2, 1);\n\n        let experts = vec![\n            MockExpert { id: 10 }, // Expert 0 scales by 10\n            MockExpert { id: 20 }, // Expert 1 scales by 20\n        ];\n\n        let moe = MoELayer::new(router, experts);\n\n        let output = moe.forward(\u0026input).unwrap();\n\n        // Token 0: [1, 0] -\u003e Gate -\u003e [1, 0] -\u003e Top1 is Index 0 (Score 1.0).\n        // Softmax([1, 0]) -\u003e [0.73, 0.27]. Top1 weight approx 0.73?\n        // Wait, TopKRouter implementation:\n        // pairs sorted. top_k = pairs[0..k].\n        // max_val = top_k[0].0\n        // sum_exp...\n        // If k=1, max_val = val. exp(val - max_val) = exp(0) = 1.\n        // sum_exp = 1.\n        // weight = 1/1 = 1.\n        // So weight is 1.0 for top-1.\n\n        // Token 0 routes to Expert 0 (id 10). Input [1, 0]. Output [10, 0]. Weight 1.\n        // Final Token 0: [10, 0].\n\n        // Token 1: [0, 1] -\u003e Gate -\u003e [0, 1] -\u003e Top1 is Index 1 (Score 1.0).\n        // Weight 1.0.\n        // Token 1 routes to Expert 1 (id 20). Input [0, 1]. Output [0, 20]. Weight 1.\n        // Final Token 1: [0, 20].\n\n        let out_data = output.data();\n        assert!((out_data[0] - 10.0).abs() \u003c 1e-4);\n        assert!((out_data[1] - 0.0).abs() \u003c 1e-4);\n        assert!((out_data[2] - 0.0).abs() \u003c 1e-4);\n        assert!((out_data[3] - 20.0).abs() \u003c 1e-4);\n    }\n}\n","traces":[{"line":83,"address":[],"length":0,"stats":{"Line":1}},{"line":98,"address":[],"length":0,"stats":{"Line":1}},{"line":102,"address":[],"length":0,"stats":{"Line":4}},{"line":104,"address":[],"length":0,"stats":{"Line":4}},{"line":106,"address":[],"length":0,"stats":{"Line":4}},{"line":107,"address":[],"length":0,"stats":{"Line":4}},{"line":109,"address":[],"length":0,"stats":{"Line":1}},{"line":111,"address":[],"length":0,"stats":{"Line":2}},{"line":112,"address":[],"length":0,"stats":{"Line":4}},{"line":113,"address":[],"length":0,"stats":{"Line":4}},{"line":114,"address":[],"length":0,"stats":{"Line":3}},{"line":115,"address":[],"length":0,"stats":{"Line":6}},{"line":116,"address":[],"length":0,"stats":{"Line":2}},{"line":117,"address":[],"length":0,"stats":{"Line":2}},{"line":118,"address":[],"length":0,"stats":{"Line":2}},{"line":119,"address":[],"length":0,"stats":{"Line":10}},{"line":120,"address":[],"length":0,"stats":{"Line":2}},{"line":122,"address":[],"length":0,"stats":{"Line":14}},{"line":124,"address":[],"length":0,"stats":{"Line":4}},{"line":126,"address":[],"length":0,"stats":{"Line":4}},{"line":127,"address":[],"length":0,"stats":{"Line":4}},{"line":128,"address":[],"length":0,"stats":{"Line":6}},{"line":130,"address":[],"length":0,"stats":{"Line":8}},{"line":131,"address":[],"length":0,"stats":{"Line":12}},{"line":132,"address":[],"length":0,"stats":{"Line":10}},{"line":133,"address":[],"length":0,"stats":{"Line":4}},{"line":134,"address":[],"length":0,"stats":{"Line":4}},{"line":137,"address":[],"length":0,"stats":{"Line":4}},{"line":139,"address":[],"length":0,"stats":{"Line":6}},{"line":140,"address":[],"length":0,"stats":{"Line":6}},{"line":141,"address":[],"length":0,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":1}},{"line":160,"address":[],"length":0,"stats":{"Line":1}},{"line":190,"address":[],"length":0,"stats":{"Line":1}},{"line":191,"address":[],"length":0,"stats":{"Line":4}},{"line":192,"address":[],"length":0,"stats":{"Line":5}},{"line":194,"address":[],"length":0,"stats":{"Line":4}},{"line":196,"address":[],"length":0,"stats":{"Line":6}},{"line":197,"address":[],"length":0,"stats":{"Line":6}},{"line":199,"address":[],"length":0,"stats":{"Line":3}},{"line":200,"address":[],"length":0,"stats":{"Line":3}},{"line":202,"address":[],"length":0,"stats":{"Line":3}},{"line":203,"address":[],"length":0,"stats":{"Line":6}},{"line":204,"address":[],"length":0,"stats":{"Line":6}},{"line":205,"address":[],"length":0,"stats":{"Line":6}},{"line":206,"address":[],"length":0,"stats":{"Line":8}},{"line":207,"address":[],"length":0,"stats":{"Line":4}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":3}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":216,"address":[],"length":0,"stats":{"Line":3}},{"line":217,"address":[],"length":0,"stats":{"Line":6}},{"line":218,"address":[],"length":0,"stats":{"Line":4}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":6}},{"line":223,"address":[],"length":0,"stats":{"Line":6}},{"line":225,"address":[],"length":0,"stats":{"Line":8}},{"line":226,"address":[],"length":0,"stats":{"Line":6}},{"line":227,"address":[],"length":0,"stats":{"Line":10}},{"line":230,"address":[],"length":0,"stats":{"Line":10}},{"line":232,"address":[],"length":0,"stats":{"Line":10}},{"line":234,"address":[],"length":0,"stats":{"Line":10}},{"line":235,"address":[],"length":0,"stats":{"Line":10}},{"line":237,"address":[],"length":0,"stats":{"Line":2}},{"line":238,"address":[],"length":0,"stats":{"Line":6}},{"line":239,"address":[],"length":0,"stats":{"Line":4}},{"line":240,"address":[],"length":0,"stats":{"Line":2}},{"line":245,"address":[],"length":0,"stats":{"Line":3}},{"line":247,"address":[],"length":0,"stats":{"Line":9}},{"line":248,"address":[],"length":0,"stats":{"Line":6}},{"line":249,"address":[],"length":0,"stats":{"Line":8}},{"line":250,"address":[],"length":0,"stats":{"Line":4}},{"line":251,"address":[],"length":0,"stats":{"Line":4}},{"line":252,"address":[],"length":0,"stats":{"Line":4}},{"line":254,"address":[],"length":0,"stats":{"Line":10}},{"line":255,"address":[],"length":0,"stats":{"Line":8}},{"line":260,"address":[],"length":0,"stats":{"Line":1}}],"covered":75,"coverable":77},{"path":["/","Users","blitz","my-oss","xla-rs","src","nn","norm.rs"],"content":"use crate::tensor::{Cpu, Result, Tensor, TensorElem};\nuse num_traits::Float;\nuse rayon::prelude::*;\n\n/// RMSNorm (Root Mean Square Layer Normalization).\n///\n/// Normalizes the input tensor using the root mean square of the elements.\n/// Used in modern LLM architectures like Gemma and Llama.\n#[derive(Debug)]\npub struct RMSNorm\u003cT: TensorElem\u003e {\n    pub weight: Tensor\u003cT, 1, Cpu\u003e,\n    pub eps: T,\n}\n\nimpl\u003cT: TensorElem + Float\u003e RMSNorm\u003cT\u003e {\n    /// Creates a new RMSNorm layer.\n    ///\n    /// # Arguments\n    ///\n    /// * `weight` - The scale weights of shape `[features]`.\n    /// * `eps` - A small constant for numerical stability.\n    pub fn new(weight: Tensor\u003cT, 1, Cpu\u003e, eps: T) -\u003e Self {\n        Self { weight, eps }\n    }\n\n    /// Performs the forward pass of RMSNorm.\n    ///\n    /// Normalizes the input over the last dimension.\n    pub fn forward\u003cconst RANK: usize\u003e(\n        \u0026self,\n        x: \u0026Tensor\u003cT, RANK, Cpu\u003e,\n    ) -\u003e Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e {\n        let shape = x.shape();\n        let last_dim = shape[RANK - 1];\n        if last_dim != self.weight.shape()[0] {\n            return Err(crate::tensor::TensorError::ShapeMismatch {\n                expected: vec![last_dim],\n                got: vec![self.weight.shape()[0]],\n            });\n        }\n\n        let mut out = Tensor::zeros(*shape);\n\n        out.data_mut()\n            .par_chunks_mut(last_dim)\n            .zip(x.data().par_chunks(last_dim))\n            .for_each(|(out_row, in_row)| {\n                let mut sum_sq = T::zero();\n                for \u0026val in in_row {\n                    sum_sq += val * val;\n                }\n                let mean_sq = sum_sq / T::from_usize(last_dim).unwrap();\n                let rsqrt = T::one() / (mean_sq + self.eps).sqrt();\n\n                for i in 0..last_dim {\n                    out_row[i] = in_row[i] * rsqrt * self.weight.data()[i];\n                }\n            });\n\n        Ok(out)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::tensor::Tensor;\n\n    #[test]\n    fn test_rmsnorm_forward() {\n        // Input: [1.0, 2.0, 3.0]\n        // RMS = sqrt((1+4+9)/3) = sqrt(14/3) = sqrt(4.666) approx 2.1602\n        // Norm: [1/2.16, 2/2.16, 3/2.16] -\u003e [0.4629, 0.9258, 1.3887]\n        // Weight: [1, 1, 1] -\u003e Output same as norm\n\n        let input_data = vec![1.0, 2.0, 3.0];\n        let input = Tensor::\u003cf32, 1, Cpu\u003e::new(input_data, [3]).unwrap();\n\n        let weight = Tensor::\u003cf32, 1, Cpu\u003e::ones([3]);\n        let norm = RMSNorm::new(weight, 1e-5); // Small eps\n\n        let output = norm.forward(\u0026input).unwrap();\n        let out_data = output.data();\n\n        // Expected values calculation\n        let rms = (14.0f32 / 3.0).sqrt();\n        let expected = vec![1.0 / rms, 2.0 / rms, 3.0 / rms];\n\n        for (got, exp) in out_data.iter().zip(expected.iter()) {\n            assert!((got - exp).abs() \u003c 1e-4);\n        }\n    }\n\n    #[test]\n    fn test_rmsnorm_shape_mismatch() {\n        let weight = Tensor::\u003cf32, 1, Cpu\u003e::ones([4]); // Expect last dim 4\n        let norm = RMSNorm::new(weight, 1e-5);\n\n        let input = Tensor::\u003cf32, 2, Cpu\u003e::zeros([2, 3]); // Last dim 3\n        let res = norm.forward(\u0026input);\n        assert!(res.is_err());\n    }\n}\n","traces":[{"line":22,"address":[],"length":0,"stats":{"Line":7}},{"line":29,"address":[],"length":0,"stats":{"Line":9}},{"line":33,"address":[],"length":0,"stats":{"Line":27}},{"line":34,"address":[],"length":0,"stats":{"Line":18}},{"line":35,"address":[],"length":0,"stats":{"Line":27}},{"line":36,"address":[],"length":0,"stats":{"Line":1}},{"line":37,"address":[],"length":0,"stats":{"Line":3}},{"line":38,"address":[],"length":0,"stats":{"Line":3}},{"line":42,"address":[],"length":0,"stats":{"Line":24}},{"line":44,"address":[],"length":0,"stats":{"Line":8}},{"line":45,"address":[],"length":0,"stats":{"Line":16}},{"line":46,"address":[],"length":0,"stats":{"Line":32}},{"line":47,"address":[],"length":0,"stats":{"Line":23}},{"line":48,"address":[],"length":0,"stats":{"Line":30}},{"line":49,"address":[],"length":0,"stats":{"Line":2712}},{"line":50,"address":[],"length":0,"stats":{"Line":1798}},{"line":52,"address":[],"length":0,"stats":{"Line":60}},{"line":53,"address":[],"length":0,"stats":{"Line":45}},{"line":55,"address":[],"length":0,"stats":{"Line":1813}},{"line":56,"address":[],"length":0,"stats":{"Line":3596}},{"line":60,"address":[],"length":0,"stats":{"Line":8}}],"covered":21,"coverable":21},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","device.rs"],"content":"//! Device abstraction for Tensor storage.\n//!\n//! This module defines the `Device` trait and the `Cpu` device implementation.\n//! Devices determine where tensor data is allocated and how operations are executed.\n//!\n//! # ML Context\n//!\n//! In machine learning frameworks, a \"Device\" represents the hardware accelerator where\n//! computation happens. Common devices include:\n//! - **CPU**: Central Processing Unit (host memory). Good for sequential logic and small models.\n//! - **GPU**: Graphics Processing Unit (device memory). Excellent for massive parallel matrix operations.\n//! - **TPU**: Tensor Processing Unit. Specialized hardware for ML workloads.\n//!\n//! Abstracting the device allows the same neural network code to run on a laptop (CPU)\n//! or a cluster of GPUs without modification.\n\nuse crate::tensor::{Storage, TensorElem};\nuse std::fmt::Debug;\n\n/// A trait representing the underlying storage device for a Tensor.\n///\n/// Devices determine where the data is stored (e.g., CPU, GPU) and how it is accessed.\n/// Currently, only `Cpu` is implemented.\n///\n/// # Design\n///\n/// This trait is designed to be extensible. Future implementations could include `Cuda` or `Mps`\n/// devices. The `Storage` associated type allows each device to define its own memory container\n/// (e.g., `Vec\u003cT\u003e` for CPU, `CudaBuffer\u003cT\u003e` for GPU).\npub trait Device: Clone + Debug + PartialEq + Send + Sync {\n    /// The type of storage used by this device.\n    type Storage\u003cT\u003e: Storage\u003cT\u003e\n    where\n        T: TensorElem;\n\n    /// Returns the name of the device.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::tensor::{Cpu, Device};\n    /// let device = Cpu;\n    /// assert_eq!(device.name(), \"CPU\");\n    /// ```\n    fn name(\u0026self) -\u003e \u0026'static str;\n}\n\n/// A CPU Device.\n///\n/// Represents the standard system CPU. Data is stored in system RAM using `Vec\u003cT\u003e`.\n/// This is the default device for all tensors.\n///\n/// # Performance\n///\n/// Operations on the CPU are generally slower than GPU for large matrix multiplications,\n/// but `xla-rs` uses `rayon` to parallelize operations across all available CPU cores,\n/// providing decent performance for development and inference on smaller models.\n#[derive(Clone, Debug, PartialEq)]\npub struct Cpu;\n\nimpl Device for Cpu {\n    type Storage\u003cT\u003e\n        = Vec\u003cT\u003e\n    where\n        T: TensorElem;\n\n    fn name(\u0026self) -\u003e \u0026'static str {\n        \"CPU\"\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_cpu_device_name() {\n        let device = Cpu;\n        assert_eq!(device.name(), \"CPU\");\n    }\n\n    #[test]\n    fn test_cpu_device_traits() {\n        let device = Cpu;\n        let device_clone = device.clone();\n        assert_eq!(device, device_clone);\n        assert_eq!(format!(\"{:?}\", device), \"Cpu\");\n    }\n}\n","traces":[{"line":67,"address":[],"length":0,"stats":{"Line":2}},{"line":68,"address":[],"length":0,"stats":{"Line":2}}],"covered":2,"coverable":2},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","mod.rs"],"content":"//! Core Tensor implementation.\n//!\n//! This module defines the `Tensor` struct, which is the central data structure in `xla-rs`.\n//! It supports N-dimensional arrays, automatic differentiation (via the `autograd` module),\n//! and various mathematical operations.\n//!\n//! # Key Components\n//!\n//! - [`Tensor`]: The main struct representing an N-dimensional array.\n//! - [`TensorError`]: Error type for tensor operations.\n//! - [`TensorElem`]: Trait bound for elements that can be stored in a tensor.\n//!\n//! # ML Context\n//!\n//! Tensors are the fundamental data structure in deep learning. They generalize scalars (0D),\n//! vectors (1D), and matrices (2D) to N dimensions.\n//!\n//! - **0D**: Scalar (loss value).\n//! - **1D**: Vector (bias term, embedding).\n//! - **2D**: Matrix (weights, grayscale image).\n//! - **3D**: (RGB image, sequence of vectors).\n//! - **4D**: (Batch of RGB images).\n//!\n//! In `xla-rs`, tensors are strongly typed by element type `T` and rank `RANK`.\n//! This allows for some compile-time safety and optimization.\n//!\n//! # Examples\n//!\n//! ```rust\n//! use xla_rs::tensor::Tensor;\n//!\n//! let data = vec![1.0, 2.0, 3.0, 4.0];\n//! let tensor = Tensor::\u003cf32, 2\u003e::new(data, [2, 2]).unwrap();\n//! assert_eq!(tensor.shape(), \u0026[2, 2]);\n//! ```\n\nuse num_traits::{FromPrimitive, Num, NumAssign, ToPrimitive};\nuse std::fmt::Debug;\nuse thiserror::Error;\n\npub mod device;\npub mod ops;\npub mod storage;\n\npub use device::{Cpu, Device};\npub use storage::Storage;\n\n/// Error type for Tensor operations.\n#[derive(Error, Debug)]\npub enum TensorError {\n    /// The shape of the data does not match the expected shape.\n    #[error(\"Shape mismatch: expected {expected:?}, got {got:?}\")]\n    ShapeMismatch {\n        expected: Vec\u003cusize\u003e,\n        got: Vec\u003cusize\u003e,\n    },\n    /// Broadcasting is not possible between the given shapes.\n    #[error(\"Incompatible shapes for broadcasting: {0:?} and {1:?}\")]\n    BroadcastError(Vec\u003cusize\u003e, Vec\u003cusize\u003e),\n    /// An index is out of bounds for the given shape.\n    #[error(\"Index out of bounds: index {index:?} for shape {shape:?}\")]\n    IndexOutOfBounds {\n        index: Vec\u003cusize\u003e,\n        shape: Vec\u003cusize\u003e,\n    },\n    /// Operations between tensors on different devices are not allowed.\n    #[error(\"Device mismatch\")]\n    DeviceMismatch,\n    /// The requested operation is not supported (e.g., for a specific rank or type).\n    #[error(\"Unsupported operation: {0}\")]\n    Unsupported(String),\n}\n\npub type Result\u003cT\u003e = std::result::Result\u003cT, TensorError\u003e;\n\n/// Trait bound for elements that can be stored in a Tensor.\n///\n/// # Requirements\n/// - `Copy + Clone`: Essential for efficient storage in contiguous memory (e.g., `Vec\u003cT\u003e`) and fast element access.\n/// - `Num + ...`: Provides necessary numeric operations for tensor math.\n/// - `Send + Sync`: Required for parallel execution via `rayon`.\npub trait TensorElem:\n    Num + NumAssign + Copy + Clone + Debug + Send + Sync + FromPrimitive + ToPrimitive + PartialOrd\n{\n}\n\nimpl\u003cT\u003e TensorElem for T where\n    T: Num\n        + NumAssign\n        + Copy\n        + Clone\n        + Debug\n        + Send\n        + Sync\n        + FromPrimitive\n        + ToPrimitive\n        + PartialOrd\n{\n}\n\n/// The core Tensor struct.\n///\n/// Represents an N-dimensional array of elements.\n///\n/// # Generics\n///\n/// - `T`: The element type (must implement `TensorElem`).\n/// - `RANK`: The number of dimensions (const generic).\n/// - `D`: The device where data is stored (defaults to `Cpu`).\n/// # Design Philosophy: `const RANK` vs `const SHAPE`\n///\n/// The `Tensor` struct uses `const RANK: usize` rather than encoding the full shape in the type system\n/// (e.g., `Tensor\u003cT, [32, 128]\u003e`). This is a deliberate trade-off to balance **correctness** with **usability**.\n///\n/// **Why not `const SHAPE`?**\n/// - **Dynamic Batching:** Deep learning models often need to handle variable batch sizes (e.g., training with batch size 32,\n///   inference with batch size 1). Encoding shape in the type would require re-instantiating the model for every batch size.\n/// - **Complexity:** Operations like `reshape` or `matmul` would require complex type-level arithmetic, making compiler\n///   errors difficult to read and the API rigid.\n///\n/// **The Trade-off:**\n/// - **Pros:** We gain the flexibility to handle variable sequence lengths and batch sizes without recompilation.\n///   Function signatures remain readable (e.g., `fn forward(x: Tensor\u003cf32, 2\u003e)`).\n/// - **Cons:** Shape mismatches (e.g., multiplying `[32, 10]` by `[5, 20]`) are caught at runtime rather than compile-time.\n///\n/// This approach aligns with the industry standard for most deep learning frameworks (like PyTorch, TensorFlow, and many Rust crates),\n/// prioritizing the flexibility required for real-world model training and inference.\n#[derive(Clone)]\npub struct Tensor\u003cT, const RANK: usize, D: Device = Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    shape: [usize; RANK],\n    strides: [usize; RANK],\n    data: D::Storage\u003cT\u003e,\n    device: D,\n}\n\nimpl\u003cT, const RANK: usize\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    /// Creates a new Tensor from a vector of data and a shape.\n    ///\n    /// # Arguments\n    ///\n    /// * `data` - A flat vector containing the tensor elements.\n    /// * `shape` - An array representing the dimensions of the tensor.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::ShapeMismatch` if the length of `data` does not match the product of `shape`.\n    pub fn new(data: Vec\u003cT\u003e, shape: [usize; RANK]) -\u003e Result\u003cSelf\u003e {\n        let size: usize = shape.iter().product();\n        if data.len() != size {\n            return Err(TensorError::ShapeMismatch {\n                expected: vec![size],\n                got: vec![data.len()],\n            });\n        }\n\n        let strides = compute_strides(\u0026shape);\n        Ok(Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        })\n    }\n\n    /// Creates a new Tensor filled with zeros.\n    ///\n    /// # Arguments\n    ///\n    /// * `shape` - An array representing the dimensions of the tensor.\n    pub fn zeros(shape: [usize; RANK]) -\u003e Self {\n        let size: usize = shape.iter().product();\n        let data = vec![T::zero(); size];\n        let strides = compute_strides(\u0026shape);\n        Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        }\n    }\n\n    /// Creates a new Tensor filled with ones.\n    ///\n    /// # Arguments\n    ///\n    /// * `shape` - An array representing the dimensions of the tensor.\n    pub fn ones(shape: [usize; RANK]) -\u003e Self {\n        let size: usize = shape.iter().product();\n        let data = vec![T::one(); size];\n        let strides = compute_strides(\u0026shape);\n        Self {\n            shape,\n            strides,\n            data,\n            device: Cpu,\n        }\n    }\n\n    /// Reshapes the tensor to a new shape.\n    ///\n    /// The number of elements must remain the same.\n    ///\n    /// # Arguments\n    ///\n    /// * `new_shape` - The target shape.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::ShapeMismatch` if the total number of elements in `new_shape`\n    /// does not match the current size of the tensor.\n    pub fn reshape\u003cconst NEW_RANK: usize\u003e(\n        self,\n        new_shape: [usize; NEW_RANK],\n    ) -\u003e Result\u003cTensor\u003cT, NEW_RANK, Cpu\u003e\u003e {\n        let current_size: usize = self.shape.iter().product();\n        let new_size: usize = new_shape.iter().product();\n\n        if current_size != new_size {\n            return Err(TensorError::ShapeMismatch {\n                expected: vec![current_size],\n                got: vec![new_size],\n            });\n        }\n\n        let strides = compute_strides(\u0026new_shape);\n        Ok(Tensor {\n            shape: new_shape,\n            strides,\n            data: self.data,\n            device: self.device,\n        })\n    }\n}\n\n/// Computes the strides for a given shape.\n///\n/// Strides represent the number of elements to skip in memory to move to the next element\n/// along a specific dimension. This implementation assumes a row-major (C-style) memory layout.\n///\n/// # Arguments\n///\n/// * `shape` - The shape of the tensor.\n///\n/// # Returns\n///\n/// An array of strides corresponding to the input shape.\nfn compute_strides\u003cconst RANK: usize\u003e(shape: \u0026[usize; RANK]) -\u003e [usize; RANK] {\n    let mut strides = [0; RANK];\n    let mut stride = 1;\n    for i in (0..RANK).rev() {\n        strides[i] = stride;\n        stride *= shape[i];\n    }\n    strides\n}\n\nimpl\u003cT, const RANK: usize, D: Device\u003e Tensor\u003cT, RANK, D\u003e\nwhere\n    T: TensorElem,\n{\n    /// Returns the shape of the tensor.\n    ///\n    /// The shape is an array of length `RANK` representing the size of each dimension.\n    pub fn shape(\u0026self) -\u003e \u0026[usize; RANK] {\n        \u0026self.shape\n    }\n\n    /// Returns the strides of the tensor.\n    ///\n    /// Strides represent the number of elements to skip in memory to move to the next element\n    /// along a specific dimension.\n    pub fn strides(\u0026self) -\u003e \u0026[usize; RANK] {\n        \u0026self.strides\n    }\n\n    /// Returns a reference to the underlying data as a slice.\n    pub fn data(\u0026self) -\u003e \u0026[T] {\n        self.data.as_slice()\n    }\n\n    /// Returns a mutable reference to the underlying data as a slice.\n    ///\n    /// # Warning\n    ///\n    /// Modifying the data directly can be dangerous if you violate invariants (though `Tensor`\n    /// doesn't have many invariants on the values themselves). Use with caution.\n    pub fn data_mut(\u0026mut self) -\u003e \u0026mut [T] {\n        self.data.as_mut_slice()\n    }\n\n    /// Returns the total number of elements in the tensor.\n    ///\n    /// This is equal to the product of the dimensions in the shape.\n    pub fn size(\u0026self) -\u003e usize {\n        self.shape.iter().product()\n    }\n}\n\nimpl\u003cT, const RANK: usize, D: Device\u003e Debug for Tensor\u003cT, RANK, D\u003e\nwhere\n    T: TensorElem,\n{\n    fn fmt(\u0026self, f: \u0026mut std::fmt::Formatter\u003c'_\u003e) -\u003e std::fmt::Result {\n        f.debug_struct(\"Tensor\")\n            .field(\"shape\", \u0026self.shape)\n            .field(\"device\", \u0026self.device.name())\n            .field(\"data_len\", \u0026self.data.len())\n            .finish()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_tensor_creation() {\n        // Positive case\n        let data = vec![1.0, 2.0, 3.0, 4.0];\n        let tensor = Tensor::\u003cf32, 2\u003e::new(data.clone(), [2, 2]).unwrap();\n        assert_eq!(tensor.shape(), \u0026[2, 2]);\n        assert_eq!(tensor.data(), \u0026data[..]);\n\n        // Negative case: Size mismatch\n        let err = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 2.0, 3.0], [2, 2]);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_zeros_ones() {\n        let zeros = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        assert_eq!(zeros.data(), \u0026[0.0; 6]);\n\n        let ones = Tensor::\u003cf32, 2\u003e::ones([2, 3]);\n        assert_eq!(ones.data(), \u0026[1.0; 6]);\n    }\n\n    #[test]\n    fn test_reshape() {\n        let tensor = Tensor::\u003cf32, 2\u003e::zeros([2, 3]); // 6 elements\n\n        // Valid reshape\n        let reshaped = tensor.reshape([3, 2]).unwrap();\n        assert_eq!(reshaped.shape(), \u0026[3, 2]);\n\n        // Invalid reshape\n        let err = reshaped.clone().reshape([4, 2]); // 8 elements\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_arithmetic() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let b = Tensor::\u003cf32, 1\u003e::new(vec![3.0, 4.0], [2]).unwrap();\n\n        // Add\n        let c = (\u0026a + \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[4.0, 6.0]);\n\n        // Mul\n        let d = (\u0026a * \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[3.0, 8.0]);\n\n        // Mismatch\n        let _e = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let f = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let err = \u0026a + \u0026f;\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_matmul_2d() {\n        // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n        let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n        let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n        let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2]);\n\n        // Row 0: 1*7 + 2*9 + 3*2 = 7 + 18 + 6 = 31\n        // Row 0, Col 1: 1*8 + 2*1 + 3*3 = 8 + 2 + 9 = 19\n        // Row 1: 4*7 + 5*9 + 6*2 = 28 + 45 + 12 = 85\n        // Row 1, Col 1: 4*8 + 5*1 + 6*3 = 32 + 5 + 18 = 55\n        assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n    }\n\n    #[test]\n    fn test_matmul_broadcast_error() {\n        let a = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        let b = Tensor::\u003cf32, 2\u003e::zeros([4, 2]); // K mismatch (3 vs 4)\n\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_transpose() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n        // [ 1 2 3 ]\n        // [ 4 5 6 ]\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[3, 2]);\n        // [ 1 4 ]\n        // [ 2 5 ]\n        // [ 3 6 ]\n        assert_eq!(t_t.data(), \u0026[1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_axes() {\n        // Rank 4 tensor [B, S, H, D] -\u003e [B, H, S, D]\n        // Shape: [1, 2, 2, 2] -\u003e [1, 2, 2, 2] for simplicity but distinct values\n        let data: Vec\u003cf32\u003e = (0..8).map(|i| i as f32).collect();\n\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 2]).unwrap();\n\n        let permuted = t.transpose_axes(1, 2).unwrap();\n        assert_eq!(permuted.shape(), \u0026[1, 2, 2, 2]); // H, S swapped but sizes same\n\n        assert_eq!(permuted.data(), \u0026[0.0, 1.0, 4.0, 5.0, 2.0, 3.0, 6.0, 7.0]);\n    }\n\n    #[test]\n    fn test_macro() {\n        let t = tensor!([1.0, 2.0, 3.0, 4.0], [2, 2]);\n        assert_eq!(t.shape(), \u0026[2, 2]);\n        assert_eq!(t.data(), \u0026[1.0, 2.0, 3.0, 4.0]);\n    }\n\n    #[test]\n    fn test_tensor_accessors() {\n        let mut t = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        assert_eq!(t.size(), 6);\n        assert_eq!(t.strides(), \u0026[3, 1]);\n\n        // Test data_mut\n        {\n            let data = t.data_mut();\n            data[0] = 1.0;\n        }\n        assert_eq!(t.data()[0], 1.0);\n    }\n\n    #[test]\n    fn test_compute_strides() {\n        let shape = [2, 3, 4];\n        let strides = compute_strides(\u0026shape);\n        // Stride for dim 2 (last) is 1\n        // Stride for dim 1 is 4\n        // Stride for dim 0 is 3 * 4 = 12\n        assert_eq!(strides, [12, 4, 1]);\n    }\n\n    #[test]\n    fn test_tensor_error_display() {\n        let err = TensorError::ShapeMismatch {\n            expected: vec![2, 2],\n            got: vec![4],\n        };\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Shape mismatch: expected [2, 2], got [4]\"\n        );\n\n        let err = TensorError::BroadcastError(vec![2, 3], vec![2, 2]);\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Incompatible shapes for broadcasting: [2, 3] and [2, 2]\"\n        );\n\n        let err = TensorError::IndexOutOfBounds {\n            index: vec![3],\n            shape: vec![2],\n        };\n        assert_eq!(\n            format!(\"{}\", err),\n            \"Index out of bounds: index [3] for shape [2]\"\n        );\n\n        let err = TensorError::DeviceMismatch;\n        assert_eq!(format!(\"{}\", err), \"Device mismatch\");\n\n        let err = TensorError::Unsupported(\"foo\".to_string());\n        assert_eq!(format!(\"{}\", err), \"Unsupported operation: foo\");\n    }\n\n    #[test]\n    fn test_tensor_clone() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let t2 = t.clone();\n        assert_eq!(t.data(), t2.data());\n        assert_eq!(t.shape(), t2.shape());\n    }\n\n    #[test]\n    fn test_tensor_debug() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let debug_str = format!(\"{:?}\", t);\n        assert!(debug_str.contains(\"Tensor\"));\n        assert!(debug_str.contains(\"shape\"));\n        assert!(debug_str.contains(\"device\"));\n        assert!(debug_str.contains(\"CPU\"));\n    }\n}\n","traces":[{"line":153,"address":[],"length":0,"stats":{"Line":60119}},{"line":154,"address":[],"length":0,"stats":{"Line":300595}},{"line":155,"address":[],"length":0,"stats":{"Line":120238}},{"line":156,"address":[],"length":0,"stats":{"Line":1}},{"line":157,"address":[],"length":0,"stats":{"Line":3}},{"line":158,"address":[],"length":0,"stats":{"Line":2}},{"line":162,"address":[],"length":0,"stats":{"Line":180354}},{"line":163,"address":[],"length":0,"stats":{"Line":60118}},{"line":164,"address":[],"length":0,"stats":{"Line":120236}},{"line":165,"address":[],"length":0,"stats":{"Line":120236}},{"line":166,"address":[],"length":0,"stats":{"Line":60118}},{"line":167,"address":[],"length":0,"stats":{"Line":60118}},{"line":176,"address":[],"length":0,"stats":{"Line":320210}},{"line":177,"address":[],"length":0,"stats":{"Line":1601050}},{"line":178,"address":[],"length":0,"stats":{"Line":1280840}},{"line":179,"address":[],"length":0,"stats":{"Line":960630}},{"line":193,"address":[],"length":0,"stats":{"Line":20054}},{"line":194,"address":[],"length":0,"stats":{"Line":100270}},{"line":195,"address":[],"length":0,"stats":{"Line":80216}},{"line":196,"address":[],"length":0,"stats":{"Line":60162}},{"line":217,"address":[],"length":0,"stats":{"Line":159}},{"line":221,"address":[],"length":0,"stats":{"Line":795}},{"line":222,"address":[],"length":0,"stats":{"Line":795}},{"line":224,"address":[],"length":0,"stats":{"Line":159}},{"line":225,"address":[],"length":0,"stats":{"Line":1}},{"line":226,"address":[],"length":0,"stats":{"Line":3}},{"line":227,"address":[],"length":0,"stats":{"Line":1}},{"line":231,"address":[],"length":0,"stats":{"Line":474}},{"line":232,"address":[],"length":0,"stats":{"Line":158}},{"line":233,"address":[],"length":0,"stats":{"Line":316}},{"line":234,"address":[],"length":0,"stats":{"Line":316}},{"line":235,"address":[],"length":0,"stats":{"Line":158}},{"line":236,"address":[],"length":0,"stats":{"Line":158}},{"line":253,"address":[],"length":0,"stats":{"Line":600710}},{"line":254,"address":[],"length":0,"stats":{"Line":1201420}},{"line":255,"address":[],"length":0,"stats":{"Line":1201420}},{"line":256,"address":[],"length":0,"stats":{"Line":3604642}},{"line":257,"address":[],"length":0,"stats":{"Line":1201611}},{"line":258,"address":[],"length":0,"stats":{"Line":1201611}},{"line":260,"address":[],"length":0,"stats":{"Line":600710}},{"line":270,"address":[],"length":0,"stats":{"Line":20223}},{"line":271,"address":[],"length":0,"stats":{"Line":20223}},{"line":278,"address":[],"length":0,"stats":{"Line":2}},{"line":279,"address":[],"length":0,"stats":{"Line":2}},{"line":283,"address":[],"length":0,"stats":{"Line":101714}},{"line":284,"address":[],"length":0,"stats":{"Line":101714}},{"line":293,"address":[],"length":0,"stats":{"Line":80053}},{"line":294,"address":[],"length":0,"stats":{"Line":80053}},{"line":300,"address":[],"length":0,"stats":{"Line":1}},{"line":301,"address":[],"length":0,"stats":{"Line":3}},{"line":309,"address":[],"length":0,"stats":{"Line":1}},{"line":310,"address":[],"length":0,"stats":{"Line":5}},{"line":311,"address":[],"length":0,"stats":{"Line":4}},{"line":312,"address":[],"length":0,"stats":{"Line":4}},{"line":313,"address":[],"length":0,"stats":{"Line":3}}],"covered":55,"coverable":55},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","ops.rs"],"content":"//! Tensor operations.\n//!\n//! This module implements various mathematical operations for Tensors, including\n//! element-wise arithmetic (Add, Sub, Mul, Div), matrix multiplication, and broadcasting.\n//! It uses `rayon` for parallel execution on the CPU.\n\nuse super::{Cpu, Result, Tensor, TensorElem, TensorError};\nuse rayon::prelude::*;\nuse std::ops::{Add, Div, Mul, Sub};\n\n/// Implements a binary arithmetic operation trait (e.g., `Add`, `Sub`) for `\u0026Tensor`.\n///\n/// This macro handles the boilerplate of checking shape compatibility, creating a new\n/// output tensor, and performing the element-wise operation in parallel using `rayon`.\n///\n/// # Arguments\n///\n/// * `$trait` - The trait to implement (e.g., `Add`).\n/// * `$method` - The method name of the trait (e.g., `add`).\nmacro_rules! impl_bin_op {\n    ($trait:ident, $method:ident) =\u003e {\n        impl\u003cT, const RANK: usize\u003e $trait for \u0026Tensor\u003cT, RANK, Cpu\u003e\n        where\n            T: TensorElem,\n        {\n            type Output = Result\u003cTensor\u003cT, RANK, Cpu\u003e\u003e;\n\n            fn $method(self, rhs: Self) -\u003e Self::Output {\n                if self.shape != rhs.shape {\n                    return Err(TensorError::ShapeMismatch {\n                        expected: self.shape.to_vec(),\n                        got: rhs.shape.to_vec(),\n                    });\n                }\n\n                let mut out = Tensor::zeros(self.shape);\n                // Seamless parallelism using rayon\n                out.data\n                    .as_mut_slice()\n                    .par_iter_mut()\n                    .zip(self.data.as_slice().par_iter())\n                    .zip(rhs.data.as_slice().par_iter())\n                    .for_each(|((o, a), b)| {\n                        *o = a.$method(*b);\n                    });\n\n                Ok(out)\n            }\n        }\n    };\n}\n\nimpl_bin_op!(Add, add);\nimpl_bin_op!(Sub, sub);\nimpl_bin_op!(Mul, mul);\nimpl_bin_op!(Div, div);\n\nimpl\u003cT, const RANK: usize\u003e Tensor\u003cT, RANK, Cpu\u003e\nwhere\n    T: TensorElem,\n{\n    /// Applies a function element-wise to the tensor.\n    ///\n    /// Creates a new tensor with the same shape, where each element is the result of applying\n    /// the closure `f` to the corresponding element in the input tensor.\n    ///\n    /// # Arguments\n    ///\n    /// * `f` - A closure that takes an element of type `T` and returns a value of type `T`.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use xla_rs::tensor::Tensor;\n    /// let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n    /// let squared = t.map(|x| x * x);\n    /// assert_eq!(squared.data(), \u0026[1.0, 4.0, 9.0]);\n    /// ```\n    pub fn map\u003cF\u003e(\u0026self, f: F) -\u003e Self\n    where\n        F: Fn(T) -\u003e T + Sync + Send,\n    {\n        let mut out = Tensor::zeros(self.shape);\n        out.data\n            .as_mut_slice()\n            .par_iter_mut()\n            .zip(self.data.as_slice().par_iter())\n            .for_each(|(o, i)| *o = f(*i));\n        out\n    }\n\n    /// Matrix Multiplication.\n    /// Supports:\n    /// - 2D x 2D: [M, K] x [K, N] -\u003e [M, N]\n    /// - 3D x 3D: [B, M, K] x [B, K, N] -\u003e [B, M, N] (Batched Matmul)\n    ///\n    /// Performs matrix multiplication on the last two dimensions of the tensors.\n    /// If the rank is greater than 2, the leading dimensions are treated as batch dimensions.\n    /// This is known as **Batched Matrix Multiplication**.\n    ///\n    /// # Mathematical Definition\n    ///\n    /// For tensors of rank $N$, this operation corresponds to the Einstein summation:\n    /// `...mk,...kn-\u003e...mn`\n    ///\n    /// # Examples\n    ///\n    /// - **Rank 2 (Matrix x Matrix)**: `[M, K] x [K, N] -\u003e [M, N]`\n    /// - **Rank 3 (Batch x Matrix x Matrix)**: `[B, M, K] x [B, K, N] -\u003e [B, M, N]`\n    /// - **Rank 4**: `[B, H, M, K] x [B, H, K, N] -\u003e [B, H, M, N]`\n    pub fn matmul(\u0026self, rhs: \u0026Self) -\u003e Result\u003cSelf\u003e {\n        // Compile-time check\n        const { assert!(RANK \u003e= 2, \"Matmul requires rank \u003e= 2\") };\n        self.matmul_impl(rhs)\n    }\n\n    /// Internal implementation of Matrix Multiplication.\n    fn matmul_impl(\u0026self, rhs: \u0026Self) -\u003e Result\u003cSelf\u003e {\n        let m = self.shape[RANK - 2];\n        let k = self.shape[RANK - 1];\n        let k2 = rhs.shape[RANK - 2];\n        let n = rhs.shape[RANK - 1];\n\n        // Check batch dimensions\n        if self.shape[..RANK - 2] != rhs.shape[..RANK - 2] {\n            return Err(TensorError::ShapeMismatch {\n                expected: self.shape.to_vec(),\n                got: rhs.shape.to_vec(),\n            });\n        }\n\n        let mut out_shape = self.shape;\n        out_shape[RANK - 2] = m;\n        out_shape[RANK - 1] = n;\n\n        // Delegate to the kernel\n        // This is where you would swap in a BLAS call or other accelerator\n        let out_data = xla_rs_kernels::cpu_matmul(\n            self.data.as_slice(),\n            rhs.data.as_slice(),\n            \u0026self.shape,\n            \u0026rhs.shape,\n        )\n        .map_err(|e| match e {\n            xla_rs_kernels::KernelError::ShapeMismatch { expected, got } =\u003e {\n                TensorError::ShapeMismatch { expected, got }\n            }\n        })?;\n\n        let strides = crate::tensor::compute_strides(\u0026out_shape);\n        Ok(Tensor {\n            shape: out_shape,\n            strides,\n            data: out_data,\n            device: Cpu,\n        })\n    }\n\n    /// Transposes the tensor.\n    ///\n    /// Swaps the last two dimensions.\n    /// - For 2D tensors (matrices), this is a standard matrix transpose.\n    /// - For N-D tensors, it swaps the last two axes (e.g., [B, M, N] -\u003e [B, N, M]).\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::Unsupported` if the tensor rank is less than 2.\n    pub fn transpose(\u0026self) -\u003e Result\u003cSelf\u003e {\n        if RANK \u003c 2 {\n            return Err(TensorError::Unsupported(\n                \"Transpose requires rank \u003e= 2\".into(),\n            ));\n        }\n\n        let mut new_shape = self.shape;\n        new_shape.swap(RANK - 1, RANK - 2);\n\n        // Delegate to the kernel\n        let out_data = xla_rs_kernels::cpu_transpose(self.data.as_slice(), \u0026self.shape).map_err(\n            |e| match e {\n                xla_rs_kernels::KernelError::ShapeMismatch { expected, got } =\u003e {\n                    TensorError::ShapeMismatch { expected, got }\n                }\n            },\n        )?;\n\n        let strides = crate::tensor::compute_strides(\u0026new_shape);\n        Ok(Tensor {\n            shape: new_shape,\n            strides,\n            data: out_data,\n            device: Cpu,\n        })\n    }\n\n    /// Transposes two specific axes of the tensor.\n    ///\n    /// This operation creates a new tensor with the data physically permuted to match the new shape.\n    ///\n    /// # Arguments\n    ///\n    /// * `ax1` - The first axis to swap.\n    /// * `ax2` - The second axis to swap.\n    ///\n    /// # Errors\n    ///\n    /// Returns `TensorError::IndexOutOfBounds` if `ax1` or `ax2` are out of bounds.\n    /// Returns `TensorError::Unsupported` for complex permutations not yet optimized.\n    pub fn transpose_axes(\u0026self, ax1: usize, ax2: usize) -\u003e Result\u003cSelf\u003e {\n        if ax1 \u003e= RANK || ax2 \u003e= RANK {\n            return Err(TensorError::IndexOutOfBounds {\n                index: vec![ax1, ax2],\n                shape: self.shape.to_vec(),\n            });\n        }\n        if ax1 == ax2 {\n            return Ok(self.clone());\n        }\n\n        let mut new_shape = self.shape;\n        new_shape.swap(ax1, ax2);\n\n        // Compute new strides based on old strides\n        // But since we own data, we can't just change strides without permuting data if we want it contiguous.\n        // My Tensor struct enforces contiguous storage (Vec\u003cT\u003e).\n        // So we must physically move data.\n\n        let mut out = Tensor::zeros(new_shape);\n\n        // Generic permute is hard. Implementing for specific cases used in Attention.\n        // Case: [B, S, H, D] -\u003e [B, H, S, D] (swap 1 and 2) where RANK=4.\n\n        if RANK == 4 \u0026\u0026 ((ax1 == 1 \u0026\u0026 ax2 == 2) || (ax1 == 2 \u0026\u0026 ax2 == 1)) {\n            let [_, _, _, _] = self.shape[0..4].try_into().unwrap();\n            // Input: B, S, H, D. Output: B, H, S, D.\n            // We want to write to [b, h, s, d].\n            // Iterate output\n            let out_s = out.strides;\n            let in_s = self.strides;\n\n            // We can use a generic loop with recursion or specific nested loops.\n            // Nested loops for 4D.\n            // Parallelize on B.\n\n            // Check if ax1/ax2 match expectation.\n            // The shape in `self` is [B, S, H, D] (if coming from reshape).\n            // If we swap 1 and 2, we get [B, H, S, D].\n\n            // But let's use the strides to be generic for 4D.\n\n            let out_ptr = out.data.as_mut_slice();\n            let in_ptr = self.data.as_slice();\n\n            // Parallelize outer dim\n            out_ptr\n                .par_chunks_mut(out_s[0])\n                .enumerate()\n                .for_each(|(i, batch_chunk)| {\n                    // i is batch index\n                    // batch_chunk is [H, S, D] size flattened\n                    // We need to iterate over H, S, D\n                    let new_dim1 = new_shape[1]; // H\n                    let new_dim2 = new_shape[2]; // S\n                    let new_dim3 = new_shape[3]; // D\n\n                    for j in 0..new_dim1 {\n                        for k in 0..new_dim2 {\n                            for l in 0..new_dim3 {\n                                // Output index: i, j, k, l (linear: i*s0 + j*s1 + k*s2 + l*s3)\n                                // This chunk corresponds to `i`. so offset is j*out_s[1] + ...\n                                let out_idx = j * out_s[1] + k * out_s[2] + l * out_s[3];\n\n                                // Input index: i, k, j, l (swapped j and k compared to output dims mapping)\n                                // Wait, we swapped axes.\n                                // Input indices:\n                                // idx[ax1] = out_idx[ax2]\n                                // idx[ax2] = out_idx[ax1]\n                                // indices: [i, k, j, l] if we swapped 1 and 2.\n                                // Input stride:\n                                let in_idx = i * in_s[0] + k * in_s[1] + j * in_s[2] + l * in_s[3];\n\n                                batch_chunk[out_idx] = in_ptr[in_idx];\n                            }\n                        }\n                    }\n                });\n            Ok(out)\n        } else {\n            Err(TensorError::Unsupported(format!(\n                \"General transpose_axes not impl for rank {} axes {},{}\",\n                RANK, ax1, ax2\n            )))\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_arithmetic() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let b = Tensor::\u003cf32, 1\u003e::new(vec![3.0, 4.0], [2]).unwrap();\n\n        // Add\n        let c = (\u0026a + \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[4.0, 6.0]);\n\n        // Sub\n        let c = (\u0026a - \u0026b).unwrap();\n        assert_eq!(c.data(), \u0026[-2.0, -2.0]);\n\n        // Mul\n        let d = (\u0026a * \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[3.0, 8.0]);\n\n        // Div\n        let d = (\u0026a / \u0026b).unwrap();\n        assert_eq!(d.data(), \u0026[1.0 / 3.0, 2.0 / 4.0]);\n\n        // Mismatch\n        let _e = Tensor::\u003cf32, 1\u003e::new(vec![1.0], [1]).unwrap();\n        let f = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let err = \u0026a + \u0026f;\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_map() {\n        let a = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0, 3.0], [3]).unwrap();\n        let b = a.map(|x| x * 2.0);\n        assert_eq!(b.data(), \u0026[2.0, 4.0, 6.0]);\n    }\n\n    #[test]\n    fn test_matmul_2d() {\n        // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n        let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n        let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n        let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2]);\n\n        // Row 0: 1*7 + 2*9 + 3*2 = 7 + 18 + 6 = 31\n        // Row 0, Col 1: 1*8 + 2*1 + 3*3 = 8 + 2 + 9 = 19\n        // Row 1: 4*7 + 5*9 + 6*2 = 28 + 45 + 12 = 85\n        // Row 1, Col 1: 4*8 + 5*1 + 6*3 = 32 + 5 + 18 = 55\n        assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n    }\n\n    #[test]\n    fn test_matmul_3d() {\n        // Batched Matmul: [B, M, K] x [B, K, N] -\u003e [B, M, N]\n        // B=2, M=2, K=2, N=2\n        // Batch 1: Identity * Identity = Identity\n        // Batch 2: 2*Identity * 3*Identity = 6*Identity\n\n        let batch1_a = vec![1.0, 0.0, 0.0, 1.0]; // Identity\n        let batch2_a = vec![2.0, 0.0, 0.0, 2.0]; // 2*Identity\n        let mut a_data = batch1_a;\n        a_data.extend(batch2_a);\n        let a = Tensor::\u003cf32, 3\u003e::new(a_data, [2, 2, 2]).unwrap();\n\n        let batch1_b = vec![1.0, 0.0, 0.0, 1.0]; // Identity\n        let batch2_b = vec![3.0, 0.0, 0.0, 3.0]; // 3*Identity\n        let mut b_data = batch1_b;\n        b_data.extend(batch2_b);\n        let b = Tensor::\u003cf32, 3\u003e::new(b_data, [2, 2, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[2, 2, 2]);\n\n        let expected_batch1 = vec![1.0, 0.0, 0.0, 1.0];\n        let expected_batch2 = vec![6.0, 0.0, 0.0, 6.0];\n        let mut expected = expected_batch1;\n        expected.extend(expected_batch2);\n\n        assert_eq!(c.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_matmul_broadcast_error() {\n        let a = Tensor::\u003cf32, 2\u003e::zeros([2, 3]);\n        let b = Tensor::\u003cf32, 2\u003e::zeros([4, 2]); // K mismatch (3 vs 4)\n\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n\n    #[test]\n    fn test_transpose() {\n        let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n        let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n        // [ 1 2 3 ]\n        // [ 4 5 6 ]\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[3, 2]);\n        // [ 1 4 ]\n        // [ 2 5 ]\n        // [ 3 6 ]\n        assert_eq!(t_t.data(), \u0026[1.0, 4.0, 2.0, 5.0, 3.0, 6.0]);\n    }\n\n    #[test]\n    fn test_transpose_axes() {\n        // Rank 4 tensor [B, S, H, D] -\u003e [B, H, S, D]\n        // Shape: [1, 2, 2, 2] -\u003e [1, 2, 2, 2] for simplicity but distinct values\n        let data: Vec\u003cf32\u003e = (0..8).map(|i| i as f32).collect();\n\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 2]).unwrap();\n\n        let permuted = t.transpose_axes(1, 2).unwrap();\n        assert_eq!(permuted.shape(), \u0026[1, 2, 2, 2]); // H, S swapped but sizes same\n\n        assert_eq!(permuted.data(), \u0026[0.0, 1.0, 4.0, 5.0, 2.0, 3.0, 6.0, 7.0]);\n    }\n\n    #[test]\n    fn test_matmul_4d() {\n        // [B, S, M, K] x [B, S, K, N] -\u003e [B, S, M, N]\n        // Shape: [1, 2, 2, 2] x [1, 2, 2, 2] -\u003e [1, 2, 2, 2]\n        // Batch 1, Seq 1: Identity * Identity = Identity\n        // Batch 1, Seq 2: 2*Identity * 3*Identity = 6*Identity\n\n        let s1_a = vec![1.0, 0.0, 0.0, 1.0];\n        let s2_a = vec![2.0, 0.0, 0.0, 2.0];\n        let mut a_data = s1_a;\n        a_data.extend(s2_a);\n        let a = Tensor::\u003cf32, 4\u003e::new(a_data, [1, 2, 2, 2]).unwrap();\n\n        let s1_b = vec![1.0, 0.0, 0.0, 1.0];\n        let s2_b = vec![3.0, 0.0, 0.0, 3.0];\n        let mut b_data = s1_b;\n        b_data.extend(s2_b);\n        let b = Tensor::\u003cf32, 4\u003e::new(b_data, [1, 2, 2, 2]).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026[1, 2, 2, 2]);\n\n        let expected_s1 = vec![1.0, 0.0, 0.0, 1.0];\n        let expected_s2 = vec![6.0, 0.0, 0.0, 6.0];\n        let mut expected = expected_s1;\n        expected.extend(expected_s2);\n\n        assert_eq!(c.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_matmul_10d() {\n        // 10D Tensor\n        // Shape: [1, ..., 1, 2, 2] (8 ones, then 2, 2)\n        let shape = [1, 1, 1, 1, 1, 1, 1, 1, 2, 2];\n        let data = vec![1.0, 2.0, 3.0, 4.0]; // [1 2; 3 4]\n        let a = Tensor::\u003cf32, 10\u003e::new(data.clone(), shape).unwrap();\n        let b = Tensor::\u003cf32, 10\u003e::new(data, shape).unwrap();\n\n        let c = a.matmul(\u0026b).unwrap();\n        assert_eq!(c.shape(), \u0026shape);\n\n        // [1 2; 3 4] * [1 2; 3 4] = [7 10; 15 22]\n        assert_eq!(c.data(), \u0026[7.0, 10.0, 15.0, 22.0]);\n    }\n\n    #[test]\n    fn test_transpose_4d() {\n        // [B, S, M, N] -\u003e [B, S, N, M]\n        // [1, 2, 2, 3] -\u003e [1, 2, 3, 2]\n        let data: Vec\u003cf32\u003e = (0..12).map(|i| i as f32).collect();\n        let t = Tensor::\u003cf32, 4\u003e::new(data, [1, 2, 2, 3]).unwrap();\n\n        let t_t = t.transpose().unwrap();\n        assert_eq!(t_t.shape(), \u0026[1, 2, 3, 2]);\n\n        // First matrix (0..6):\n        // [0 1 2]\n        // [3 4 5]\n        // Transpose:\n        // [0 3]\n        // [1 4]\n        // [2 5]\n        // -\u003e 0, 3, 1, 4, 2, 5\n\n        // Second matrix (6..12):\n        // [6 7 8]\n        // [9 10 11]\n        // Transpose:\n        // [6 9]\n        // [7 10]\n        // [8 11]\n        // -\u003e 6, 9, 7, 10, 8, 11\n\n        let expected = vec![0.0, 3.0, 1.0, 4.0, 2.0, 5.0, 6.0, 9.0, 7.0, 10.0, 8.0, 11.0];\n        assert_eq!(t_t.data(), \u0026expected[..]);\n    }\n\n    #[test]\n    fn test_transpose_10d() {\n        // 10D Tensor\n        // Shape: [1, ..., 1, 2, 3]\n        let shape = [1, 1, 1, 1, 1, 1, 1, 1, 2, 3];\n        let data: Vec\u003cf32\u003e = (0..6).map(|i| i as f32).collect();\n        let t = Tensor::\u003cf32, 10\u003e::new(data, shape).unwrap();\n\n        let t_t = t.transpose().unwrap();\n        let expected_shape = [1, 1, 1, 1, 1, 1, 1, 1, 3, 2];\n        assert_eq!(t_t.shape(), \u0026expected_shape);\n\n        // [0 1 2]\n        // [3 4 5]\n        // -\u003e\n        // [0 3]\n        // [1 4]\n        // [2 5]\n        assert_eq!(t_t.data(), \u0026[0.0, 3.0, 1.0, 4.0, 2.0, 5.0]);\n    }\n\n    #[test]\n    fn test_transpose_error() {\n        let t = Tensor::\u003cf32, 1\u003e::new(vec![1.0, 2.0], [2]).unwrap();\n        let err = t.transpose();\n        assert!(matches!(err, Err(TensorError::Unsupported(_))));\n    }\n\n    #[test]\n    fn test_transpose_axes_error() {\n        let t = Tensor::\u003cf32, 2\u003e::zeros([2, 2]);\n        let err = t.transpose_axes(0, 2); // 2 is out of bounds\n        assert!(matches!(err, Err(TensorError::IndexOutOfBounds { .. })));\n    }\n\n    #[test]\n    fn test_transpose_axes_identity() {\n        let t = Tensor::\u003cf32, 2\u003e::zeros([2, 2]);\n        let t2 = t.transpose_axes(0, 0).unwrap();\n        assert_eq!(t.shape(), t2.shape());\n    }\n\n    #[test]\n    fn test_transpose_axes_unsupported() {\n        let t = Tensor::\u003cf32, 3\u003e::zeros([2, 2, 2]);\n        // 3D transpose axes not fully implemented in the simplified logic\n        let err = t.transpose_axes(0, 1);\n        assert!(matches!(err, Err(TensorError::Unsupported(_))));\n    }\n\n    #[test]\n    fn test_matmul_batch_mismatch() {\n        let a = Tensor::\u003cf32, 3\u003e::zeros([2, 2, 2]);\n        let b = Tensor::\u003cf32, 3\u003e::zeros([3, 2, 2]); // Batch size 3 vs 2\n        let err = a.matmul(\u0026b);\n        assert!(matches!(err, Err(TensorError::ShapeMismatch { .. })));\n    }\n}\n","traces":[{"line":28,"address":[],"length":0,"stats":{"Line":240086}},{"line":29,"address":[],"length":0,"stats":{"Line":240086}},{"line":30,"address":[],"length":0,"stats":{"Line":2}},{"line":31,"address":[],"length":0,"stats":{"Line":6}},{"line":32,"address":[],"length":0,"stats":{"Line":2}},{"line":36,"address":[],"length":0,"stats":{"Line":720252}},{"line":38,"address":[],"length":0,"stats":{"Line":240084}},{"line":39,"address":[],"length":0,"stats":{"Line":240084}},{"line":40,"address":[],"length":0,"stats":{"Line":240084}},{"line":41,"address":[],"length":0,"stats":{"Line":720252}},{"line":42,"address":[],"length":0,"stats":{"Line":720252}},{"line":43,"address":[],"length":0,"stats":{"Line":601595}},{"line":44,"address":[],"length":0,"stats":{"Line":361511}},{"line":47,"address":[],"length":0,"stats":{"Line":240084}},{"line":79,"address":[],"length":0,"stats":{"Line":80023}},{"line":83,"address":[],"length":0,"stats":{"Line":240069}},{"line":84,"address":[],"length":0,"stats":{"Line":80023}},{"line":87,"address":[],"length":0,"stats":{"Line":240069}},{"line":88,"address":[],"length":0,"stats":{"Line":280924}},{"line":89,"address":[],"length":0,"stats":{"Line":80023}},{"line":111,"address":[],"length":0,"stats":{"Line":120101}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":360303}},{"line":118,"address":[],"length":0,"stats":{"Line":120101}},{"line":119,"address":[],"length":0,"stats":{"Line":240202}},{"line":120,"address":[],"length":0,"stats":{"Line":240202}},{"line":121,"address":[],"length":0,"stats":{"Line":240202}},{"line":122,"address":[],"length":0,"stats":{"Line":240202}},{"line":125,"address":[],"length":0,"stats":{"Line":240202}},{"line":126,"address":[],"length":0,"stats":{"Line":1}},{"line":127,"address":[],"length":0,"stats":{"Line":3}},{"line":128,"address":[],"length":0,"stats":{"Line":1}},{"line":132,"address":[],"length":0,"stats":{"Line":240200}},{"line":133,"address":[],"length":0,"stats":{"Line":120100}},{"line":134,"address":[],"length":0,"stats":{"Line":120100}},{"line":139,"address":[],"length":0,"stats":{"Line":120100}},{"line":140,"address":[],"length":0,"stats":{"Line":120100}},{"line":141,"address":[],"length":0,"stats":{"Line":120100}},{"line":142,"address":[],"length":0,"stats":{"Line":120100}},{"line":144,"address":[],"length":0,"stats":{"Line":120102}},{"line":145,"address":[],"length":0,"stats":{"Line":4}},{"line":146,"address":[],"length":0,"stats":{"Line":2}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":360294}},{"line":152,"address":[],"length":0,"stats":{"Line":120098}},{"line":153,"address":[],"length":0,"stats":{"Line":240196}},{"line":154,"address":[],"length":0,"stats":{"Line":240196}},{"line":155,"address":[],"length":0,"stats":{"Line":120098}},{"line":156,"address":[],"length":0,"stats":{"Line":120098}},{"line":169,"address":[],"length":0,"stats":{"Line":80072}},{"line":170,"address":[],"length":0,"stats":{"Line":80072}},{"line":171,"address":[],"length":0,"stats":{"Line":1}},{"line":172,"address":[],"length":0,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":160142}},{"line":177,"address":[],"length":0,"stats":{"Line":320284}},{"line":180,"address":[],"length":0,"stats":{"Line":400355}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":189,"address":[],"length":0,"stats":{"Line":240213}},{"line":190,"address":[],"length":0,"stats":{"Line":80071}},{"line":191,"address":[],"length":0,"stats":{"Line":160142}},{"line":192,"address":[],"length":0,"stats":{"Line":160142}},{"line":193,"address":[],"length":0,"stats":{"Line":80071}},{"line":194,"address":[],"length":0,"stats":{"Line":80071}},{"line":211,"address":[],"length":0,"stats":{"Line":37}},{"line":212,"address":[],"length":0,"stats":{"Line":74}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":214,"address":[],"length":0,"stats":{"Line":4}},{"line":215,"address":[],"length":0,"stats":{"Line":1}},{"line":218,"address":[],"length":0,"stats":{"Line":36}},{"line":219,"address":[],"length":0,"stats":{"Line":1}},{"line":222,"address":[],"length":0,"stats":{"Line":70}},{"line":223,"address":[],"length":0,"stats":{"Line":140}},{"line":230,"address":[],"length":0,"stats":{"Line":105}},{"line":235,"address":[],"length":0,"stats":{"Line":103}},{"line":236,"address":[],"length":0,"stats":{"Line":68}},{"line":240,"address":[],"length":0,"stats":{"Line":68}},{"line":241,"address":[],"length":0,"stats":{"Line":68}},{"line":253,"address":[],"length":0,"stats":{"Line":102}},{"line":254,"address":[],"length":0,"stats":{"Line":102}},{"line":257,"address":[],"length":0,"stats":{"Line":34}},{"line":258,"address":[],"length":0,"stats":{"Line":68}},{"line":260,"address":[],"length":0,"stats":{"Line":68}},{"line":264,"address":[],"length":0,"stats":{"Line":68}},{"line":265,"address":[],"length":0,"stats":{"Line":68}},{"line":266,"address":[],"length":0,"stats":{"Line":68}},{"line":268,"address":[],"length":0,"stats":{"Line":116}},{"line":269,"address":[],"length":0,"stats":{"Line":266}},{"line":270,"address":[],"length":0,"stats":{"Line":3064}},{"line":273,"address":[],"length":0,"stats":{"Line":7200}},{"line":282,"address":[],"length":0,"stats":{"Line":8640}},{"line":284,"address":[],"length":0,"stats":{"Line":1440}},{"line":289,"address":[],"length":0,"stats":{"Line":34}},{"line":291,"address":[],"length":0,"stats":{"Line":1}},{"line":292,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":1}}],"covered":92,"coverable":98},{"path":["/","Users","blitz","my-oss","xla-rs","src","tensor","storage.rs"],"content":"//! Storage abstraction for Tensors.\n//!\n//! This module defines the `Storage` trait, which abstracts over the underlying data container.\n//!\n//! # ML Context\n//!\n//! Tensors need to store their elements somewhere. \"Storage\" is the container that holds\n//! the raw data.\n//! - **Contiguous Memory**: Most tensor operations (like matrix multiplication) rely on data\n//!   being stored contiguously in memory for cache efficiency and SIMD usage.\n//! - **Abstraction**: By abstracting storage, we can support different backends (CPU `Vec`,\n//!   GPU buffers, mmap files) without changing the high-level Tensor API.\n\nuse crate::tensor::TensorElem;\nuse std::fmt::Debug;\n\n/// A trait for the underlying data storage.\n///\n/// Abstracts over the container used to hold tensor data.\n/// For `Cpu` device, this is typically `Vec\u003cT\u003e`.\n///\n/// # Requirements\n///\n/// Implementations must provide access to the raw data as slices. This allows\n/// efficient interaction with low-level linear algebra routines.\npub trait Storage\u003cT\u003e: Clone + Debug + Send + Sync {\n    /// Returns the data as an immutable slice.\n    fn as_slice(\u0026self) -\u003e \u0026[T];\n\n    /// Returns the data as a mutable slice.\n    fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [T];\n\n    /// Returns the number of elements in the storage.\n    fn len(\u0026self) -\u003e usize;\n\n    /// Returns `true` if the storage contains no elements.\n    fn is_empty(\u0026self) -\u003e bool {\n        self.len() == 0\n    }\n\n    /// Copies data from a slice into the storage.\n    ///\n    /// # Arguments\n    ///\n    /// * `src` - The source slice to copy from.\n    fn copy_from_slice(\u0026mut self, src: \u0026[T])\n    where\n        T: Copy,\n    {\n        self.as_mut_slice().copy_from_slice(src);\n    }\n}\n\nimpl\u003cT: TensorElem\u003e Storage\u003cT\u003e for Vec\u003cT\u003e {\n    fn as_slice(\u0026self) -\u003e \u0026[T] {\n        self\n    }\n    fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [T] {\n        self\n    }\n    fn len(\u0026self) -\u003e usize {\n        self.len()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_vec_storage() {\n        let mut storage = vec![1.0, 2.0, 3.0];\n\n        // Test as_slice\n        assert_eq!(storage.as_slice(), \u0026[1.0, 2.0, 3.0]);\n\n        // Test len\n        assert_eq!(storage.len(), 3);\n        assert!(!storage.is_empty());\n\n        // Test as_mut_slice\n        storage.as_mut_slice()[0] = 10.0;\n        assert_eq!(storage.as_slice(), \u0026[10.0, 2.0, 3.0]);\n\n        // Test copy_from_slice\n        storage.copy_from_slice(\u0026[4.0, 5.0, 6.0]);\n        assert_eq!(storage.as_slice(), \u0026[4.0, 5.0, 6.0]);\n    }\n\n    #[test]\n    fn test_empty_storage() {\n        let storage: Vec\u003cf32\u003e = vec![];\n        assert!(storage.is_empty());\n        assert_eq!(storage.len(), 0);\n    }\n\n    #[derive(Clone, Debug)]\n    struct MockStorage {\n        data: Vec\u003cf32\u003e,\n    }\n\n    impl Storage\u003cf32\u003e for MockStorage {\n        fn as_slice(\u0026self) -\u003e \u0026[f32] {\n            \u0026self.data\n        }\n        fn as_mut_slice(\u0026mut self) -\u003e \u0026mut [f32] {\n            \u0026mut self.data\n        }\n        fn len(\u0026self) -\u003e usize {\n            self.data.len()\n        }\n    }\n\n    #[test]\n    fn test_storage_defaults() {\n        let mut storage = MockStorage {\n            data: vec![1.0, 2.0],\n        };\n        // Test default is_empty\n        assert!(!storage.is_empty());\n\n        let empty = MockStorage { data: vec![] };\n        assert!(empty.is_empty());\n\n        // Test default copy_from_slice\n        storage.copy_from_slice(\u0026[3.0, 4.0]);\n        assert_eq!(storage.as_slice(), \u0026[3.0, 4.0]);\n    }\n}\n","traces":[{"line":37,"address":[],"length":0,"stats":{"Line":2}},{"line":38,"address":[],"length":0,"stats":{"Line":2}},{"line":46,"address":[],"length":0,"stats":{"Line":2}},{"line":50,"address":[],"length":0,"stats":{"Line":6}},{"line":55,"address":[],"length":0,"stats":{"Line":101714}},{"line":56,"address":[],"length":0,"stats":{"Line":101714}},{"line":58,"address":[],"length":0,"stats":{"Line":80054}},{"line":59,"address":[],"length":0,"stats":{"Line":80054}},{"line":61,"address":[],"length":0,"stats":{"Line":1}},{"line":62,"address":[],"length":0,"stats":{"Line":2}}],"covered":10,"coverable":10},{"path":["/","Users","blitz","my-oss","xla-rs","test_link.rs"],"content":"extern crate xla_rs; fn main() {}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","autograd_tests.rs"],"content":"use xla_rs::autograd::Variable;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_scalar_autograd() {\n    // f(x) = x^2 + 3x\n    // f'(x) = 2x + 3\n    // Let x = 2.0\n    // f(2) = 4 + 6 = 10\n    // f'(2) = 4 + 3 = 7\n\n    let x_data = Tensor::\u003cf32, 1\u003e::new(vec![2.0], [1]).unwrap();\n    let x = Variable::new(x_data);\n\n    // x^2 = x * x\n    let x_sq = x.clone() * x.clone();\n\n    // 3x\n    let three = Variable::new(Tensor::\u003cf32, 1\u003e::new(vec![3.0], [1]).unwrap());\n    let three_x = three * x.clone();\n\n    // y = x^2 + 3x\n    let y = x_sq + three_x;\n\n    assert_eq!(y.data.data()[0], 10.0);\n\n    y.backward();\n\n    // Check grad of x\n    // x contributes to x_sq (2x) and three_x (3)\n    // grad should be 7.0\n\n    let x_grad = x.grad.borrow();\n    assert!(x_grad.is_some());\n    assert_eq!(x_grad.as_ref().unwrap().data()[0], 7.0);\n}\n\n#[test]\nfn test_matmul_autograd() {\n    // C = A @ B\n    // A: [1, 2] = [1, 2]\n    // B: [2, 1] = [3, 4]\n    // C: [1, 1] = 1*3 + 2*4 = 3 + 8 = 11\n\n    // dC/dA = B^T = [3, 4]\n    // dC/dB = A^T = [1, 2]\n\n    let a_data = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 2.0], [1, 2]).unwrap();\n    let b_data = Tensor::\u003cf32, 2\u003e::new(vec![3.0, 4.0], [2, 1]).unwrap();\n\n    let a = Variable::new(a_data);\n    let b = Variable::new(b_data);\n\n    let c = a.matmul(\u0026b).unwrap();\n\n    assert_eq!(c.data.data()[0], 11.0);\n\n    c.backward();\n\n    let a_grad = a.grad.borrow();\n    let b_grad = b.grad.borrow();\n\n    assert!(a_grad.is_some());\n    assert!(b_grad.is_some());\n\n    // Check A grad: should be B^T = [3, 4]\n    assert_eq!(a_grad.as_ref().unwrap().data(), \u0026[3.0, 4.0]);\n\n    // Check B grad: should be A^T = [1, 2]\n    assert_eq!(b_grad.as_ref().unwrap().data(), \u0026[1.0, 2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","book_tests.rs"],"content":"use doc_comment::doctest;\n\ndoctest!(\"../book/src/chapter_06_gemma.md\");\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_01_setup.rs"],"content":"#[test]\nfn test_environment_setup() {\n    println!(\"Welcome to xla-rs!\");\n    println!(\"If you see this, your Rust environment is correctly set up.\");\n\n    // Verify we can allocate a vector\n    let v: Vec\u003cf32\u003e = vec![1.0, 2.0, 3.0];\n    assert_eq!(v.len(), 3);\n\n    // Verify rayon is working\n    use rayon::prelude::*;\n    let sum: f32 = v.par_iter().sum();\n    assert_eq!(sum, 6.0);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_02_tensors.rs"],"content":"use xla_rs::tensor::Tensor;\n\n#[test]\nfn test_tensor_basics() {\n    // 1. Create a 2x3 tensor\n    let data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n    let t = Tensor::\u003cf32, 2\u003e::new(data, [2, 3]).unwrap();\n\n    assert_eq!(t.shape(), \u0026[2, 3]);\n    assert_eq!(t.strides(), \u0026[3, 1]); // Row-major\n}\n\n#[test]\nfn test_broadcasting() {\n    // A: [2, 2]\n    let a = Tensor::\u003cf32, 2\u003e::ones([2, 2]);\n    // Broadcasting is not yet implemented, so we manually broadcast for now\n    let b_expanded = Tensor::\u003cf32, 2\u003e::new(vec![1.0, 1.0, 1.0, 1.0], [2, 2]).unwrap();\n\n    let c = (\u0026a + \u0026b_expanded).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2]);\n    assert_eq!(c.data(), \u0026[2.0, 2.0, 2.0, 2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_03_autograd.rs"],"content":"use xla_rs::autograd::Variable;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_autograd_simple() {\n    // a = 2, b = 3\n    let a_data = Tensor::\u003cf32, 1\u003e::new(vec![2.0], [1]).unwrap();\n    let b_data = Tensor::\u003cf32, 1\u003e::new(vec![3.0], [1]).unwrap();\n\n    let a = Variable::new(a_data);\n    let b = Variable::new(b_data);\n\n    // c = a * b = 6\n    let c = a.clone() * b.clone();\n\n    assert_eq!(c.data.data(), \u0026[6.0]);\n\n    // Backward\n    c.backward();\n\n    // da = b = 3\n    let a_grad = a.grad.borrow();\n    assert_eq!(a_grad.as_ref().unwrap().data(), \u0026[3.0]);\n\n    // db = a = 2\n    let b_grad = b.grad.borrow();\n    assert_eq!(b_grad.as_ref().unwrap().data(), \u0026[2.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_04_nn.rs"],"content":"use xla_rs::nn::Linear;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_linear_forward() {\n    // Weight: [2, 2] (all ones)\n    let weight = Tensor::\u003cf32, 2\u003e::ones([2, 2]);\n    // Bias: [2] (all ones)\n    let bias = Tensor::\u003cf32, 1\u003e::ones([2]);\n\n    let linear = Linear::new(weight, Some(bias));\n\n    // Input: [1, 2] (all ones)\n    let input = Tensor::\u003cf32, 2\u003e::ones([1, 2]);\n\n    // Output = Input * Weight^T + Bias\n    // [1, 1] * [1, 1]^T + [1, 1]\n    // [1*1 + 1*1] + 1 = 3\n\n    let output = linear.forward(\u0026input).unwrap();\n\n    assert_eq!(output.shape(), \u0026[1, 2]);\n    assert_eq!(output.data(), \u0026[3.0, 3.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_05_attention.rs"],"content":"use xla_rs::models::gemma::attention::MultiHeadAttention;\nuse xla_rs::models::gemma::rope::precompute_freqs_cis;\nuse xla_rs::nn::Linear;\nuse xla_rs::tensor::Tensor;\n\n#[test]\nfn test_attention_forward() {\n    let dim = 16;\n    let num_heads = 4;\n    let num_kv_heads = 4; // Standard MHA for simplicity\n    let head_dim = 4;\n\n    // Create dummy projections (Identity-like would be ideal, but random/ones is fine for shape check)\n    let q_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let k_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let v_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n    let o_proj = Linear::new(Tensor::\u003cf32, 2\u003e::ones([dim, dim]), None);\n\n    let attn = MultiHeadAttention::new(\n        dim,\n        num_heads,\n        num_kv_heads,\n        head_dim,\n        q_proj,\n        k_proj,\n        v_proj,\n        o_proj,\n    );\n\n    // Input: [Batch=1, Seq=2, Dim=16]\n    let x = Tensor::\u003cf32, 3\u003e::ones([1, 2, dim]);\n\n    // RoPE freqs\n    let (cos, sin) = precompute_freqs_cis(head_dim, 10, 10000.0);\n\n    let output = attn.forward(\u0026x, \u0026cos, \u0026sin, None).unwrap();\n\n    assert_eq!(output.shape(), \u0026[1, 2, dim]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","chapter_06_gemma.rs"],"content":"use xla_rs::models::gemma::attention::MultiHeadAttention;\nuse xla_rs::models::gemma::rope::precompute_freqs_cis;\nuse xla_rs::models::gemma::{GemmaBlock, GemmaConfig, GemmaModel, MLP};\nuse xla_rs::nn::{Linear, RMSNorm};\nuse xla_rs::tensor::Tensor;\n\nfn create_dummy_model(config: \u0026GemmaConfig) -\u003e GemmaModel\u003cf32\u003e {\n    let mut layers = Vec::new();\n\n    for _ in 0..config.num_hidden_layers {\n        let dim = config.hidden_size;\n\n        let kv_dim = config.num_key_value_heads * config.head_dim;\n        let attn = MultiHeadAttention::new(\n            dim,\n            config.num_attention_heads,\n            config.num_key_value_heads,\n            config.head_dim,\n            Linear::new(Tensor::ones([dim, dim]), None),\n            Linear::new(Tensor::ones([kv_dim, dim]), None),\n            Linear::new(Tensor::ones([kv_dim, dim]), None),\n            Linear::new(Tensor::ones([dim, dim]), None),\n        );\n\n        let mlp = MLP {\n            gate_proj: Linear::new(Tensor::ones([config.intermediate_size, dim]), None),\n            up_proj: Linear::new(Tensor::ones([config.intermediate_size, dim]), None),\n            down_proj: Linear::new(Tensor::ones([dim, config.intermediate_size]), None),\n        };\n\n        layers.push(GemmaBlock {\n            self_attn: attn,\n            mlp,\n            input_layernorm: RMSNorm::new(Tensor::ones([dim]), config.rms_norm_eps),\n            post_attention_layernorm: RMSNorm::new(Tensor::ones([dim]), config.rms_norm_eps),\n        });\n    }\n\n    GemmaModel {\n        layers,\n        norm: RMSNorm::new(Tensor::ones([config.hidden_size]), config.rms_norm_eps),\n    }\n}\n\n#[test]\nfn test_gemma_forward() {\n    let config = GemmaConfig::tiny_test();\n    let model = create_dummy_model(\u0026config);\n\n    // Input: [Batch=1, Seq=2, Dim=64]\n    let x = Tensor::\u003cf32, 3\u003e::ones([1, 2, config.hidden_size]);\n\n    // RoPE\n    let (cos, sin) = precompute_freqs_cis(config.head_dim, 10, 10000.0);\n\n    // Step 1: Norm\n    let norm_x = model.layers[0].input_layernorm.forward(\u0026x).unwrap();\n    assert_eq!(norm_x.shape(), \u0026[1, 2, 64]);\n\n    // Step 2: Attn\n    let attn_out = model.layers[0]\n        .self_attn\n        .forward(\u0026norm_x, \u0026cos, \u0026sin, None)\n        .unwrap();\n    assert_eq!(attn_out.shape(), \u0026[1, 2, 64]);\n\n    // Step 3: Residual\n    let x2 = (\u0026x + \u0026attn_out).unwrap();\n\n    // Step 4: Post Norm\n    let norm_x2 = model.layers[0]\n        .post_attention_layernorm\n        .forward(\u0026x2)\n        .unwrap();\n    assert_eq!(norm_x2.shape(), \u0026[1, 2, 64]);\n\n    // Step 5: MLP\n    let mlp_out = model.layers[0].mlp.forward(\u0026norm_x2).unwrap();\n    assert_eq!(mlp_out.shape(), \u0026[1, 2, 64]);\n\n    // Full forward\n    let output = model.forward(\u0026x, \u0026cos, \u0026sin, None).unwrap();\n    assert_eq!(output.shape(), \u0026[1, 2, config.hidden_size]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","matmul_verification.rs"],"content":"use xla_rs::tensor::Tensor;\n\n#[test]\nfn test_matmul_2d_verification() {\n    // A: [2, 3], B: [3, 2] -\u003e C: [2, 2]\n    let a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0];\n    let a = Tensor::\u003cf32, 2\u003e::new(a_data, [2, 3]).unwrap();\n\n    let b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0];\n    let b = Tensor::\u003cf32, 2\u003e::new(b_data, [3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2]);\n    assert_eq!(c.data(), \u0026[31.0, 19.0, 85.0, 55.0]);\n}\n\n#[test]\nfn test_matmul_3d_verification() {\n    // Batch size 2\n    // Batch 0: Same as 2D test\n    // Batch 1: All ones\n    // A: [2, 2, 3]\n    // B: [2, 3, 2]\n    // C: [2, 2, 2]\n\n    let mut a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // Batch 0\n    // Batch 1: [1, 1, 1; 1, 1, 1] (2x3)\n    a_data.extend_from_slice(\u0026[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]);\n\n    let mut b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0]; // Batch 0\n    // Batch 1: [1, 1; 1, 1; 1, 1] (3x2)\n    b_data.extend_from_slice(\u0026[1.0, 1.0, 1.0, 1.0, 1.0, 1.0]);\n\n    let a = Tensor::\u003cf32, 3\u003e::new(a_data, [2, 2, 3]).unwrap();\n    let b = Tensor::\u003cf32, 3\u003e::new(b_data, [2, 3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[2, 2, 2]);\n\n    let c_data = c.data();\n    // Batch 0 check\n    assert_eq!(\u0026c_data[0..4], \u0026[31.0, 19.0, 85.0, 55.0]);\n    // Batch 1 check: 1*1 + 1*1 + 1*1 = 3\n    assert_eq!(\u0026c_data[4..8], \u0026[3.0, 3.0, 3.0, 3.0]);\n}\n\n#[test]\nfn test_matmul_4d_verification() {\n    // Rank 4: [Batch, Heads, M, K] x [Batch, Heads, K, N] -\u003e [Batch, Heads, M, N]\n    // Shape: [1, 2, 2, 3] x [1, 2, 3, 2] -\u003e [1, 2, 2, 2]\n\n    // Batch 0, Head 0:\n    // A: [[1, 2, 3], [4, 5, 6]] (2x3)\n    // B: [[7, 8], [9, 1], [2, 3]] (3x2)\n    // Expected: [[31, 19], [85, 55]]\n\n    // Batch 0, Head 1:\n    // A: All ones (2x3)\n    // B: All ones (3x2)\n    // Expected: [[3, 3], [3, 3]]\n\n    let mut a_data = vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0]; // Head 0\n    a_data.extend_from_slice(\u0026[1.0; 6]); // Head 1\n\n    let mut b_data = vec![7.0, 8.0, 9.0, 1.0, 2.0, 3.0]; // Head 0\n    b_data.extend_from_slice(\u0026[1.0; 6]); // Head 1\n\n    let a = Tensor::\u003cf32, 4\u003e::new(a_data, [1, 2, 2, 3]).unwrap();\n    let b = Tensor::\u003cf32, 4\u003e::new(b_data, [1, 2, 3, 2]).unwrap();\n\n    let c = a.matmul(\u0026b).unwrap();\n    assert_eq!(c.shape(), \u0026[1, 2, 2, 2]);\n\n    let c_data = c.data();\n    // Head 0 check\n    assert_eq!(\u0026c_data[0..4], \u0026[31.0, 19.0, 85.0, 55.0]);\n    // Head 1 check\n    assert_eq!(\u0026c_data[4..8], \u0026[3.0, 3.0, 3.0, 3.0]);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","blitz","my-oss","xla-rs","tests","nn_integration_test.rs"],"content":"#![allow(unused)]\nuse std::cell::RefCell;\nuse std::rc::Rc;\nuse xla_rs::autograd::{GraphNode, Variable};\nuse xla_rs::tensor::{Cpu, Tensor, TensorElem};\n\n// --- Sigmoid Activation ---\n\n#[derive(Debug)]\nstruct SigmoidNode\u003cT: TensorElem, const RANK: usize\u003e {\n    input: Tensor\u003cT, RANK, Cpu\u003e,\n    output: Tensor\u003cT, RANK, Cpu\u003e,\n    input_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    out_grad: Rc\u003cRefCell\u003cOption\u003cTensor\u003cT, RANK, Cpu\u003e\u003e\u003e\u003e,\n    parents: Vec\u003cRc\u003cdyn GraphNode\u003e\u003e,\n}\n\nimpl\u003cT: TensorElem + num_traits::Float, const RANK: usize\u003e GraphNode for SigmoidNode\u003cT, RANK\u003e {\n    fn backward(\u0026self) {\n        if let Some(grad) = self.out_grad.borrow().as_ref() {\n            // dL/dx = dL/dy * y * (1 - y)\n            let one = T::one();\n            let y = \u0026self.output;\n\n            // y * (1 - y)\n            // We need to do this element-wise.\n            // Since we don't have extensive tensor ops, we'll use map.\n\n            let derivative = y.map(|val| val * (one - val));\n            let dx = (grad * \u0026derivative).unwrap();\n\n            let mut input_grad = self.input_grad.borrow_mut();\n            if let Some(g) = input_grad.as_mut() {\n                *g = (g as \u0026Tensor\u003cT, RANK, Cpu\u003e + \u0026dx).unwrap(); // Add to existing gradient\n            } else {\n                *input_grad = Some(dx);\n            }\n        }\n    }\n\n    fn parents(\u0026self) -\u003e Vec\u003cRc\u003cdyn GraphNode\u003e\u003e {\n        self.parents.clone()\n    }\n}\n\nfn sigmoid\u003cconst RANK: usize\u003e(x: \u0026Variable\u003cf32, RANK\u003e) -\u003e Variable\u003cf32, RANK\u003e {\n    let data = x.data.map(|v| 1.0 / (1.0 + (-v).exp()));\n\n    let mut parents = Vec::new();\n    if let Some(p) = \u0026x.node {\n        parents.push(p.clone());\n    }\n\n    let out_grad = Rc::new(RefCell::new(None));\n\n    let node = Rc::new(SigmoidNode {\n        input: x.data.clone(),\n        output: data.clone(),\n        input_grad: x.grad.clone(),\n        out_grad: out_grad.clone(),\n        parents,\n    });\n\n    Variable {\n        data,\n        grad: out_grad,\n        node: Some(node),\n    }\n}\n\n// --- Linear Layer ---\n\nstruct Linear {\n    weight: Variable\u003cf32, 2\u003e,\n    bias: Variable\u003cf32, 2\u003e,\n}\n\nimpl Linear {\n    fn new(in_features: usize, out_features: usize) -\u003e Self {\n        // Pseudo-random initialization\n        let w_data = (0..in_features * out_features)\n            .map(|i| {\n                let s = (i + 1) as f32;\n                ((s * 12.9898).sin() * 43758.5453).fract() - 0.5\n            })\n            .collect();\n        let weight = Variable::new(Tensor::new(w_data, [in_features, out_features]).unwrap());\n\n        // Bias shape [1, out_features] for easy addition with batch_size=1\n        let b_data = vec![0.0; out_features];\n        let bias = Variable::new(Tensor::new(b_data, [1, out_features]).unwrap());\n\n        Self { weight, bias }\n    }\n\n    fn forward(\u0026self, x: \u0026Variable\u003cf32, 2\u003e) -\u003e Variable\u003cf32, 2\u003e {\n        // y = x @ W + b\n        // x: [B, In], W: [In, Out] -\u003e [B, Out]\n        let xw = x.matmul(\u0026self.weight).unwrap();\n        xw + self.bias.clone()\n    }\n}\n\n#[test]\nfn test_xor_training() {\n    // XOR Problem\n    // Inputs: (0,0), (0,1), (1,0), (1,1)\n    // Targets: 0, 1, 1, 0\n\n    // We'll use inputs with an extra 1.0 for bias:\n    // (0,0,1), (0,1,1), (1,0,1), (1,1,1)\n\n    let inputs = vec![\n        vec![0.0, 0.0],\n        vec![0.0, 1.0],\n        vec![1.0, 0.0],\n        vec![1.0, 1.0],\n    ];\n\n    let targets = vec![vec![0.0], vec![1.0], vec![1.0], vec![0.0]];\n\n    // Network: 2 inputs -\u003e 4 hidden -\u003e 1 output\n    let mut l1 = Linear::new(2, 4);\n    let mut l2 = Linear::new(4, 1);\n\n    let lr = 0.5;\n    let epochs = 5000;\n\n    for epoch in 0..epochs {\n        let mut total_loss = 0.0;\n\n        for (x_vec, t_vec) in inputs.iter().zip(targets.iter()) {\n            // Prepare input\n            let x = Variable::new(Tensor::new(x_vec.clone(), [1, 2]).unwrap());\n            let t = Variable::new(Tensor::new(t_vec.clone(), [1, 1]).unwrap());\n\n            // Forward\n            let h1 = l1.forward(\u0026x);\n            let h1_act = sigmoid(\u0026h1);\n\n            let h2 = l2.forward(\u0026h1_act);\n            let pred = sigmoid(\u0026h2); // Sigmoid output for 0/1\n\n            // Loss = (pred - target)^2\n            let neg_one = Variable::new(Tensor::new(vec![-1.0], [1, 1]).unwrap());\n            let neg_target = t * neg_one;\n            let diff = pred.clone() + neg_target;\n            let loss = diff.clone() * diff.clone();\n\n            total_loss += loss.data.data()[0];\n\n            // Backward\n            loss.backward();\n\n            // Update weights\n            update_param(\u0026mut l1.weight, lr);\n            update_param(\u0026mut l1.bias, lr);\n            update_param(\u0026mut l2.weight, lr);\n            update_param(\u0026mut l2.bias, lr);\n\n            // Zero gradients\n            zero_grad(\u0026l1.weight);\n            zero_grad(\u0026l1.bias);\n            zero_grad(\u0026l2.weight);\n            zero_grad(\u0026l2.bias);\n        }\n\n        if epoch % 500 == 0 {\n            println!(\"Epoch {}: Loss = {}\", epoch, total_loss);\n        }\n    }\n\n    // Verify predictions\n    println!(\"Predictions:\");\n    for (x_vec, t_vec) in inputs.iter().zip(targets.iter()) {\n        let x = Variable::new(Tensor::new(x_vec.clone(), [1, 2]).unwrap());\n        let h1 = l1.forward(\u0026x);\n        let h1_act = sigmoid(\u0026h1);\n        let h2 = l2.forward(\u0026h1_act);\n        let pred = sigmoid(\u0026h2);\n\n        let val = pred.data.data()[0];\n        println!(\"Input: {:?}, Target: {:?}, Pred: {}\", x_vec, t_vec, val);\n\n        let target = t_vec[0];\n        assert!((val - target).abs() \u003c 0.4);\n    }\n}\n\nfn update_param\u003cconst RANK: usize\u003e(var: \u0026mut Variable\u003cf32, RANK\u003e, lr: f32) {\n    let data = var.data.data_mut();\n    if let Some(grad) = var.grad.borrow().as_ref() {\n        let grad_data = grad.data();\n        for (w, g) in data.iter_mut().zip(grad_data.iter()) {\n            *w -= lr * g;\n        }\n    }\n}\n\nfn zero_grad\u003cconst RANK: usize\u003e(var: \u0026Variable\u003cf32, RANK\u003e) {\n    *var.grad.borrow_mut() = None;\n}\n","traces":[],"covered":0,"coverable":0}]};
    </script>
    <script crossorigin>/** @license React v16.13.1
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
'use strict';(function(d,r){"object"===typeof exports&&"undefined"!==typeof module?r(exports):"function"===typeof define&&define.amd?define(["exports"],r):(d=d||self,r(d.React={}))})(this,function(d){function r(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function w(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function da(){}function L(a,b,c){this.props=a;this.context=b;this.refs=ba;this.updater=c||ca}function ea(a,b,c){var g,e={},fa=null,d=null;if(null!=b)for(g in void 0!==b.ref&&(d=b.ref),void 0!==b.key&&(fa=""+b.key),b)ha.call(b,g)&&!ia.hasOwnProperty(g)&&(e[g]=b[g]);var h=arguments.length-2;if(1===h)e.children=c;else if(1<h){for(var k=Array(h),f=0;f<h;f++)k[f]=arguments[f+2];e.children=k}if(a&&a.defaultProps)for(g in h=a.defaultProps,
h)void 0===e[g]&&(e[g]=h[g]);return{$$typeof:x,type:a,key:fa,ref:d,props:e,_owner:M.current}}function va(a,b){return{$$typeof:x,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function N(a){return"object"===typeof a&&null!==a&&a.$$typeof===x}function wa(a){var b={"=":"=0",":":"=2"};return"$"+(""+a).replace(/[=:]/g,function(a){return b[a]})}function ja(a,b,c,g){if(C.length){var e=C.pop();e.result=a;e.keyPrefix=b;e.func=c;e.context=g;e.count=0;return e}return{result:a,keyPrefix:b,func:c,
context:g,count:0}}function ka(a){a.result=null;a.keyPrefix=null;a.func=null;a.context=null;a.count=0;10>C.length&&C.push(a)}function O(a,b,c,g){var e=typeof a;if("undefined"===e||"boolean"===e)a=null;var d=!1;if(null===a)d=!0;else switch(e){case "string":case "number":d=!0;break;case "object":switch(a.$$typeof){case x:case xa:d=!0}}if(d)return c(g,a,""===b?"."+P(a,0):b),1;d=0;b=""===b?".":b+":";if(Array.isArray(a))for(var f=0;f<a.length;f++){e=a[f];var h=b+P(e,f);d+=O(e,h,c,g)}else if(null===a||
"object"!==typeof a?h=null:(h=la&&a[la]||a["@@iterator"],h="function"===typeof h?h:null),"function"===typeof h)for(a=h.call(a),f=0;!(e=a.next()).done;)e=e.value,h=b+P(e,f++),d+=O(e,h,c,g);else if("object"===e)throw c=""+a,Error(r(31,"[object Object]"===c?"object with keys {"+Object.keys(a).join(", ")+"}":c,""));return d}function Q(a,b,c){return null==a?0:O(a,"",b,c)}function P(a,b){return"object"===typeof a&&null!==a&&null!=a.key?wa(a.key):b.toString(36)}function ya(a,b,c){a.func.call(a.context,b,
a.count++)}function za(a,b,c){var g=a.result,e=a.keyPrefix;a=a.func.call(a.context,b,a.count++);Array.isArray(a)?R(a,g,c,function(a){return a}):null!=a&&(N(a)&&(a=va(a,e+(!a.key||b&&b.key===a.key?"":(""+a.key).replace(ma,"$&/")+"/")+c)),g.push(a))}function R(a,b,c,g,e){var d="";null!=c&&(d=(""+c).replace(ma,"$&/")+"/");b=ja(b,d,g,e);Q(a,za,b);ka(b)}function t(){var a=na.current;if(null===a)throw Error(r(321));return a}function S(a,b){var c=a.length;a.push(b);a:for(;;){var g=c-1>>>1,e=a[g];if(void 0!==
e&&0<D(e,b))a[g]=b,a[c]=e,c=g;else break a}}function n(a){a=a[0];return void 0===a?null:a}function E(a){var b=a[0];if(void 0!==b){var c=a.pop();if(c!==b){a[0]=c;a:for(var g=0,e=a.length;g<e;){var d=2*(g+1)-1,f=a[d],h=d+1,k=a[h];if(void 0!==f&&0>D(f,c))void 0!==k&&0>D(k,f)?(a[g]=k,a[h]=c,g=h):(a[g]=f,a[d]=c,g=d);else if(void 0!==k&&0>D(k,c))a[g]=k,a[h]=c,g=h;else break a}}return b}return null}function D(a,b){var c=a.sortIndex-b.sortIndex;return 0!==c?c:a.id-b.id}function F(a){for(var b=n(u);null!==
b;){if(null===b.callback)E(u);else if(b.startTime<=a)E(u),b.sortIndex=b.expirationTime,S(p,b);else break;b=n(u)}}function T(a){y=!1;F(a);if(!v)if(null!==n(p))v=!0,z(U);else{var b=n(u);null!==b&&G(T,b.startTime-a)}}function U(a,b){v=!1;y&&(y=!1,V());H=!0;var c=m;try{F(b);for(l=n(p);null!==l&&(!(l.expirationTime>b)||a&&!W());){var g=l.callback;if(null!==g){l.callback=null;m=l.priorityLevel;var e=g(l.expirationTime<=b);b=q();"function"===typeof e?l.callback=e:l===n(p)&&E(p);F(b)}else E(p);l=n(p)}if(null!==
l)var d=!0;else{var f=n(u);null!==f&&G(T,f.startTime-b);d=!1}return d}finally{l=null,m=c,H=!1}}function oa(a){switch(a){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1E4;default:return 5E3}}var f="function"===typeof Symbol&&Symbol.for,x=f?Symbol.for("react.element"):60103,xa=f?Symbol.for("react.portal"):60106,Aa=f?Symbol.for("react.fragment"):60107,Ba=f?Symbol.for("react.strict_mode"):60108,Ca=f?Symbol.for("react.profiler"):60114,Da=f?Symbol.for("react.provider"):60109,
Ea=f?Symbol.for("react.context"):60110,Fa=f?Symbol.for("react.forward_ref"):60112,Ga=f?Symbol.for("react.suspense"):60113,Ha=f?Symbol.for("react.memo"):60115,Ia=f?Symbol.for("react.lazy"):60116,la="function"===typeof Symbol&&Symbol.iterator,pa=Object.getOwnPropertySymbols,Ja=Object.prototype.hasOwnProperty,Ka=Object.prototype.propertyIsEnumerable,I=function(){try{if(!Object.assign)return!1;var a=new String("abc");a[5]="de";if("5"===Object.getOwnPropertyNames(a)[0])return!1;var b={};for(a=0;10>a;a++)b["_"+
String.fromCharCode(a)]=a;if("0123456789"!==Object.getOwnPropertyNames(b).map(function(a){return b[a]}).join(""))return!1;var c={};"abcdefghijklmnopqrst".split("").forEach(function(a){c[a]=a});return"abcdefghijklmnopqrst"!==Object.keys(Object.assign({},c)).join("")?!1:!0}catch(g){return!1}}()?Object.assign:function(a,b){if(null===a||void 0===a)throw new TypeError("Object.assign cannot be called with null or undefined");var c=Object(a);for(var g,e=1;e<arguments.length;e++){var d=Object(arguments[e]);
for(var f in d)Ja.call(d,f)&&(c[f]=d[f]);if(pa){g=pa(d);for(var h=0;h<g.length;h++)Ka.call(d,g[h])&&(c[g[h]]=d[g[h]])}}return c},ca={isMounted:function(a){return!1},enqueueForceUpdate:function(a,b,c){},enqueueReplaceState:function(a,b,c,d){},enqueueSetState:function(a,b,c,d){}},ba={};w.prototype.isReactComponent={};w.prototype.setState=function(a,b){if("object"!==typeof a&&"function"!==typeof a&&null!=a)throw Error(r(85));this.updater.enqueueSetState(this,a,b,"setState")};w.prototype.forceUpdate=
function(a){this.updater.enqueueForceUpdate(this,a,"forceUpdate")};da.prototype=w.prototype;f=L.prototype=new da;f.constructor=L;I(f,w.prototype);f.isPureReactComponent=!0;var M={current:null},ha=Object.prototype.hasOwnProperty,ia={key:!0,ref:!0,__self:!0,__source:!0},ma=/\/+/g,C=[],na={current:null},X;if("undefined"===typeof window||"function"!==typeof MessageChannel){var A=null,qa=null,ra=function(){if(null!==A)try{var a=q();A(!0,a);A=null}catch(b){throw setTimeout(ra,0),b;}},La=Date.now();var q=
function(){return Date.now()-La};var z=function(a){null!==A?setTimeout(z,0,a):(A=a,setTimeout(ra,0))};var G=function(a,b){qa=setTimeout(a,b)};var V=function(){clearTimeout(qa)};var W=function(){return!1};f=X=function(){}}else{var Y=window.performance,sa=window.Date,Ma=window.setTimeout,Na=window.clearTimeout;"undefined"!==typeof console&&(f=window.cancelAnimationFrame,"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),
"function"!==typeof f&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"));if("object"===typeof Y&&"function"===typeof Y.now)q=function(){return Y.now()};else{var Oa=sa.now();q=function(){return sa.now()-Oa}}var J=!1,K=null,Z=-1,ta=5,ua=0;W=function(){return q()>=ua};f=function(){};X=function(a){0>a||125<a?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):
ta=0<a?Math.floor(1E3/a):5};var B=new MessageChannel,aa=B.port2;B.port1.onmessage=function(){if(null!==K){var a=q();ua=a+ta;try{K(!0,a)?aa.postMessage(null):(J=!1,K=null)}catch(b){throw aa.postMessage(null),b;}}else J=!1};z=function(a){K=a;J||(J=!0,aa.postMessage(null))};G=function(a,b){Z=Ma(function(){a(q())},b)};V=function(){Na(Z);Z=-1}}var p=[],u=[],Pa=1,l=null,m=3,H=!1,v=!1,y=!1,Qa=0;B={ReactCurrentDispatcher:na,ReactCurrentOwner:M,IsSomeRendererActing:{current:!1},assign:I};I(B,{Scheduler:{__proto__:null,
unstable_ImmediatePriority:1,unstable_UserBlockingPriority:2,unstable_NormalPriority:3,unstable_IdlePriority:5,unstable_LowPriority:4,unstable_runWithPriority:function(a,b){switch(a){case 1:case 2:case 3:case 4:case 5:break;default:a=3}var c=m;m=a;try{return b()}finally{m=c}},unstable_next:function(a){switch(m){case 1:case 2:case 3:var b=3;break;default:b=m}var c=m;m=b;try{return a()}finally{m=c}},unstable_scheduleCallback:function(a,b,c){var d=q();if("object"===typeof c&&null!==c){var e=c.delay;
e="number"===typeof e&&0<e?d+e:d;c="number"===typeof c.timeout?c.timeout:oa(a)}else c=oa(a),e=d;c=e+c;a={id:Pa++,callback:b,priorityLevel:a,startTime:e,expirationTime:c,sortIndex:-1};e>d?(a.sortIndex=e,S(u,a),null===n(p)&&a===n(u)&&(y?V():y=!0,G(T,e-d))):(a.sortIndex=c,S(p,a),v||H||(v=!0,z(U)));return a},unstable_cancelCallback:function(a){a.callback=null},unstable_wrapCallback:function(a){var b=m;return function(){var c=m;m=b;try{return a.apply(this,arguments)}finally{m=c}}},unstable_getCurrentPriorityLevel:function(){return m},
unstable_shouldYield:function(){var a=q();F(a);var b=n(p);return b!==l&&null!==l&&null!==b&&null!==b.callback&&b.startTime<=a&&b.expirationTime<l.expirationTime||W()},unstable_requestPaint:f,unstable_continueExecution:function(){v||H||(v=!0,z(U))},unstable_pauseExecution:function(){},unstable_getFirstCallbackNode:function(){return n(p)},get unstable_now(){return q},get unstable_forceFrameRate(){return X},unstable_Profiling:null},SchedulerTracing:{__proto__:null,__interactionsRef:null,__subscriberRef:null,
unstable_clear:function(a){return a()},unstable_getCurrent:function(){return null},unstable_getThreadID:function(){return++Qa},unstable_trace:function(a,b,c){return c()},unstable_wrap:function(a){return a},unstable_subscribe:function(a){},unstable_unsubscribe:function(a){}}});d.Children={map:function(a,b,c){if(null==a)return a;var d=[];R(a,d,null,b,c);return d},forEach:function(a,b,c){if(null==a)return a;b=ja(null,null,b,c);Q(a,ya,b);ka(b)},count:function(a){return Q(a,function(){return null},null)},
toArray:function(a){var b=[];R(a,b,null,function(a){return a});return b},only:function(a){if(!N(a))throw Error(r(143));return a}};d.Component=w;d.Fragment=Aa;d.Profiler=Ca;d.PureComponent=L;d.StrictMode=Ba;d.Suspense=Ga;d.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=B;d.cloneElement=function(a,b,c){if(null===a||void 0===a)throw Error(r(267,a));var d=I({},a.props),e=a.key,f=a.ref,m=a._owner;if(null!=b){void 0!==b.ref&&(f=b.ref,m=M.current);void 0!==b.key&&(e=""+b.key);if(a.type&&a.type.defaultProps)var h=
a.type.defaultProps;for(k in b)ha.call(b,k)&&!ia.hasOwnProperty(k)&&(d[k]=void 0===b[k]&&void 0!==h?h[k]:b[k])}var k=arguments.length-2;if(1===k)d.children=c;else if(1<k){h=Array(k);for(var l=0;l<k;l++)h[l]=arguments[l+2];d.children=h}return{$$typeof:x,type:a.type,key:e,ref:f,props:d,_owner:m}};d.createContext=function(a,b){void 0===b&&(b=null);a={$$typeof:Ea,_calculateChangedBits:b,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null};a.Provider={$$typeof:Da,_context:a};return a.Consumer=
a};d.createElement=ea;d.createFactory=function(a){var b=ea.bind(null,a);b.type=a;return b};d.createRef=function(){return{current:null}};d.forwardRef=function(a){return{$$typeof:Fa,render:a}};d.isValidElement=N;d.lazy=function(a){return{$$typeof:Ia,_ctor:a,_status:-1,_result:null}};d.memo=function(a,b){return{$$typeof:Ha,type:a,compare:void 0===b?null:b}};d.useCallback=function(a,b){return t().useCallback(a,b)};d.useContext=function(a,b){return t().useContext(a,b)};d.useDebugValue=function(a,b){};
d.useEffect=function(a,b){return t().useEffect(a,b)};d.useImperativeHandle=function(a,b,c){return t().useImperativeHandle(a,b,c)};d.useLayoutEffect=function(a,b){return t().useLayoutEffect(a,b)};d.useMemo=function(a,b){return t().useMemo(a,b)};d.useReducer=function(a,b,c){return t().useReducer(a,b,c)};d.useRef=function(a){return t().useRef(a)};d.useState=function(a){return t().useState(a)};d.version="16.13.1"});
</script>
    <script crossorigin>/** @license React v16.13.1
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */
/*
 Modernizr 3.0.0pre (Custom Build) | MIT
*/
'use strict';(function(I,ea){"object"===typeof exports&&"undefined"!==typeof module?ea(exports,require("react")):"function"===typeof define&&define.amd?define(["exports","react"],ea):(I=I||self,ea(I.ReactDOM={},I.React))})(this,function(I,ea){function k(a){for(var b="https://reactjs.org/docs/error-decoder.html?invariant="+a,c=1;c<arguments.length;c++)b+="&args[]="+encodeURIComponent(arguments[c]);return"Minified React error #"+a+"; visit "+b+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}
function ji(a,b,c,d,e,f,g,h,m){yb=!1;gc=null;ki.apply(li,arguments)}function mi(a,b,c,d,e,f,g,h,m){ji.apply(this,arguments);if(yb){if(yb){var n=gc;yb=!1;gc=null}else throw Error(k(198));hc||(hc=!0,pd=n)}}function lf(a,b,c){var d=a.type||"unknown-event";a.currentTarget=mf(c);mi(d,b,void 0,a);a.currentTarget=null}function nf(){if(ic)for(var a in cb){var b=cb[a],c=ic.indexOf(a);if(!(-1<c))throw Error(k(96,a));if(!jc[c]){if(!b.extractEvents)throw Error(k(97,a));jc[c]=b;c=b.eventTypes;for(var d in c){var e=
void 0;var f=c[d],g=b,h=d;if(qd.hasOwnProperty(h))throw Error(k(99,h));qd[h]=f;var m=f.phasedRegistrationNames;if(m){for(e in m)m.hasOwnProperty(e)&&of(m[e],g,h);e=!0}else f.registrationName?(of(f.registrationName,g,h),e=!0):e=!1;if(!e)throw Error(k(98,d,a));}}}}function of(a,b,c){if(db[a])throw Error(k(100,a));db[a]=b;rd[a]=b.eventTypes[c].dependencies}function pf(a){var b=!1,c;for(c in a)if(a.hasOwnProperty(c)){var d=a[c];if(!cb.hasOwnProperty(c)||cb[c]!==d){if(cb[c])throw Error(k(102,c));cb[c]=
d;b=!0}}b&&nf()}function qf(a){if(a=rf(a)){if("function"!==typeof sd)throw Error(k(280));var b=a.stateNode;b&&(b=td(b),sd(a.stateNode,a.type,b))}}function sf(a){eb?fb?fb.push(a):fb=[a]:eb=a}function tf(){if(eb){var a=eb,b=fb;fb=eb=null;qf(a);if(b)for(a=0;a<b.length;a++)qf(b[a])}}function ud(){if(null!==eb||null!==fb)vd(),tf()}function uf(a,b,c){if(wd)return a(b,c);wd=!0;try{return vf(a,b,c)}finally{wd=!1,ud()}}function ni(a){if(wf.call(xf,a))return!0;if(wf.call(yf,a))return!1;if(oi.test(a))return xf[a]=
!0;yf[a]=!0;return!1}function pi(a,b,c,d){if(null!==c&&0===c.type)return!1;switch(typeof b){case "function":case "symbol":return!0;case "boolean":if(d)return!1;if(null!==c)return!c.acceptsBooleans;a=a.toLowerCase().slice(0,5);return"data-"!==a&&"aria-"!==a;default:return!1}}function qi(a,b,c,d){if(null===b||"undefined"===typeof b||pi(a,b,c,d))return!0;if(d)return!1;if(null!==c)switch(c.type){case 3:return!b;case 4:return!1===b;case 5:return isNaN(b);case 6:return isNaN(b)||1>b}return!1}function L(a,
b,c,d,e,f){this.acceptsBooleans=2===b||3===b||4===b;this.attributeName=d;this.attributeNamespace=e;this.mustUseProperty=c;this.propertyName=a;this.type=b;this.sanitizeURL=f}function xd(a,b,c,d){var e=E.hasOwnProperty(b)?E[b]:null;var f=null!==e?0===e.type:d?!1:!(2<b.length)||"o"!==b[0]&&"O"!==b[0]||"n"!==b[1]&&"N"!==b[1]?!1:!0;f||(qi(b,c,e,d)&&(c=null),d||null===e?ni(b)&&(null===c?a.removeAttribute(b):a.setAttribute(b,""+c)):e.mustUseProperty?a[e.propertyName]=null===c?3===e.type?!1:"":c:(b=e.attributeName,
d=e.attributeNamespace,null===c?a.removeAttribute(b):(e=e.type,c=3===e||4===e&&!0===c?"":""+c,d?a.setAttributeNS(d,b,c):a.setAttribute(b,c))))}function zb(a){if(null===a||"object"!==typeof a)return null;a=zf&&a[zf]||a["@@iterator"];return"function"===typeof a?a:null}function ri(a){if(-1===a._status){a._status=0;var b=a._ctor;b=b();a._result=b;b.then(function(b){0===a._status&&(b=b.default,a._status=1,a._result=b)},function(b){0===a._status&&(a._status=2,a._result=b)})}}function na(a){if(null==a)return null;
if("function"===typeof a)return a.displayName||a.name||null;if("string"===typeof a)return a;switch(a){case Ma:return"Fragment";case gb:return"Portal";case kc:return"Profiler";case Af:return"StrictMode";case lc:return"Suspense";case yd:return"SuspenseList"}if("object"===typeof a)switch(a.$$typeof){case Bf:return"Context.Consumer";case Cf:return"Context.Provider";case zd:var b=a.render;b=b.displayName||b.name||"";return a.displayName||(""!==b?"ForwardRef("+b+")":"ForwardRef");case Ad:return na(a.type);
case Df:return na(a.render);case Ef:if(a=1===a._status?a._result:null)return na(a)}return null}function Bd(a){var b="";do{a:switch(a.tag){case 3:case 4:case 6:case 7:case 10:case 9:var c="";break a;default:var d=a._debugOwner,e=a._debugSource,f=na(a.type);c=null;d&&(c=na(d.type));d=f;f="";e?f=" (at "+e.fileName.replace(si,"")+":"+e.lineNumber+")":c&&(f=" (created by "+c+")");c="\n    in "+(d||"Unknown")+f}b+=c;a=a.return}while(a);return b}function va(a){switch(typeof a){case "boolean":case "number":case "object":case "string":case "undefined":return a;
default:return""}}function Ff(a){var b=a.type;return(a=a.nodeName)&&"input"===a.toLowerCase()&&("checkbox"===b||"radio"===b)}function ti(a){var b=Ff(a)?"checked":"value",c=Object.getOwnPropertyDescriptor(a.constructor.prototype,b),d=""+a[b];if(!a.hasOwnProperty(b)&&"undefined"!==typeof c&&"function"===typeof c.get&&"function"===typeof c.set){var e=c.get,f=c.set;Object.defineProperty(a,b,{configurable:!0,get:function(){return e.call(this)},set:function(a){d=""+a;f.call(this,a)}});Object.defineProperty(a,
b,{enumerable:c.enumerable});return{getValue:function(){return d},setValue:function(a){d=""+a},stopTracking:function(){a._valueTracker=null;delete a[b]}}}}function mc(a){a._valueTracker||(a._valueTracker=ti(a))}function Gf(a){if(!a)return!1;var b=a._valueTracker;if(!b)return!0;var c=b.getValue();var d="";a&&(d=Ff(a)?a.checked?"true":"false":a.value);a=d;return a!==c?(b.setValue(a),!0):!1}function Cd(a,b){var c=b.checked;return M({},b,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=
c?c:a._wrapperState.initialChecked})}function Hf(a,b){var c=null==b.defaultValue?"":b.defaultValue,d=null!=b.checked?b.checked:b.defaultChecked;c=va(null!=b.value?b.value:c);a._wrapperState={initialChecked:d,initialValue:c,controlled:"checkbox"===b.type||"radio"===b.type?null!=b.checked:null!=b.value}}function If(a,b){b=b.checked;null!=b&&xd(a,"checked",b,!1)}function Dd(a,b){If(a,b);var c=va(b.value),d=b.type;if(null!=c)if("number"===d){if(0===c&&""===a.value||a.value!=c)a.value=""+c}else a.value!==
""+c&&(a.value=""+c);else if("submit"===d||"reset"===d){a.removeAttribute("value");return}b.hasOwnProperty("value")?Ed(a,b.type,c):b.hasOwnProperty("defaultValue")&&Ed(a,b.type,va(b.defaultValue));null==b.checked&&null!=b.defaultChecked&&(a.defaultChecked=!!b.defaultChecked)}function Jf(a,b,c){if(b.hasOwnProperty("value")||b.hasOwnProperty("defaultValue")){var d=b.type;if(!("submit"!==d&&"reset"!==d||void 0!==b.value&&null!==b.value))return;b=""+a._wrapperState.initialValue;c||b===a.value||(a.value=
b);a.defaultValue=b}c=a.name;""!==c&&(a.name="");a.defaultChecked=!!a._wrapperState.initialChecked;""!==c&&(a.name=c)}function Ed(a,b,c){if("number"!==b||a.ownerDocument.activeElement!==a)null==c?a.defaultValue=""+a._wrapperState.initialValue:a.defaultValue!==""+c&&(a.defaultValue=""+c)}function ui(a){var b="";ea.Children.forEach(a,function(a){null!=a&&(b+=a)});return b}function Fd(a,b){a=M({children:void 0},b);if(b=ui(b.children))a.children=b;return a}function hb(a,b,c,d){a=a.options;if(b){b={};
for(var e=0;e<c.length;e++)b["$"+c[e]]=!0;for(c=0;c<a.length;c++)e=b.hasOwnProperty("$"+a[c].value),a[c].selected!==e&&(a[c].selected=e),e&&d&&(a[c].defaultSelected=!0)}else{c=""+va(c);b=null;for(e=0;e<a.length;e++){if(a[e].value===c){a[e].selected=!0;d&&(a[e].defaultSelected=!0);return}null!==b||a[e].disabled||(b=a[e])}null!==b&&(b.selected=!0)}}function Gd(a,b){if(null!=b.dangerouslySetInnerHTML)throw Error(k(91));return M({},b,{value:void 0,defaultValue:void 0,children:""+a._wrapperState.initialValue})}
function Kf(a,b){var c=b.value;if(null==c){c=b.children;b=b.defaultValue;if(null!=c){if(null!=b)throw Error(k(92));if(Array.isArray(c)){if(!(1>=c.length))throw Error(k(93));c=c[0]}b=c}null==b&&(b="");c=b}a._wrapperState={initialValue:va(c)}}function Lf(a,b){var c=va(b.value),d=va(b.defaultValue);null!=c&&(c=""+c,c!==a.value&&(a.value=c),null==b.defaultValue&&a.defaultValue!==c&&(a.defaultValue=c));null!=d&&(a.defaultValue=""+d)}function Mf(a,b){b=a.textContent;b===a._wrapperState.initialValue&&""!==
b&&null!==b&&(a.value=b)}function Nf(a){switch(a){case "svg":return"http://www.w3.org/2000/svg";case "math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function Hd(a,b){return null==a||"http://www.w3.org/1999/xhtml"===a?Nf(b):"http://www.w3.org/2000/svg"===a&&"foreignObject"===b?"http://www.w3.org/1999/xhtml":a}function nc(a,b){var c={};c[a.toLowerCase()]=b.toLowerCase();c["Webkit"+a]="webkit"+b;c["Moz"+a]="moz"+b;return c}function oc(a){if(Id[a])return Id[a];
if(!ib[a])return a;var b=ib[a],c;for(c in b)if(b.hasOwnProperty(c)&&c in Of)return Id[a]=b[c];return a}function Jd(a){var b=Pf.get(a);void 0===b&&(b=new Map,Pf.set(a,b));return b}function Na(a){var b=a,c=a;if(a.alternate)for(;b.return;)b=b.return;else{a=b;do b=a,0!==(b.effectTag&1026)&&(c=b.return),a=b.return;while(a)}return 3===b.tag?c:null}function Qf(a){if(13===a.tag){var b=a.memoizedState;null===b&&(a=a.alternate,null!==a&&(b=a.memoizedState));if(null!==b)return b.dehydrated}return null}function Rf(a){if(Na(a)!==
a)throw Error(k(188));}function vi(a){var b=a.alternate;if(!b){b=Na(a);if(null===b)throw Error(k(188));return b!==a?null:a}for(var c=a,d=b;;){var e=c.return;if(null===e)break;var f=e.alternate;if(null===f){d=e.return;if(null!==d){c=d;continue}break}if(e.child===f.child){for(f=e.child;f;){if(f===c)return Rf(e),a;if(f===d)return Rf(e),b;f=f.sibling}throw Error(k(188));}if(c.return!==d.return)c=e,d=f;else{for(var g=!1,h=e.child;h;){if(h===c){g=!0;c=e;d=f;break}if(h===d){g=!0;d=e;c=f;break}h=h.sibling}if(!g){for(h=
f.child;h;){if(h===c){g=!0;c=f;d=e;break}if(h===d){g=!0;d=f;c=e;break}h=h.sibling}if(!g)throw Error(k(189));}}if(c.alternate!==d)throw Error(k(190));}if(3!==c.tag)throw Error(k(188));return c.stateNode.current===c?a:b}function Sf(a){a=vi(a);if(!a)return null;for(var b=a;;){if(5===b.tag||6===b.tag)return b;if(b.child)b.child.return=b,b=b.child;else{if(b===a)break;for(;!b.sibling;){if(!b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}}return null}function jb(a,b){if(null==
b)throw Error(k(30));if(null==a)return b;if(Array.isArray(a)){if(Array.isArray(b))return a.push.apply(a,b),a;a.push(b);return a}return Array.isArray(b)?[a].concat(b):[a,b]}function Kd(a,b,c){Array.isArray(a)?a.forEach(b,c):a&&b.call(c,a)}function pc(a){null!==a&&(Ab=jb(Ab,a));a=Ab;Ab=null;if(a){Kd(a,wi);if(Ab)throw Error(k(95));if(hc)throw a=pd,hc=!1,pd=null,a;}}function Ld(a){a=a.target||a.srcElement||window;a.correspondingUseElement&&(a=a.correspondingUseElement);return 3===a.nodeType?a.parentNode:
a}function Tf(a){if(!wa)return!1;a="on"+a;var b=a in document;b||(b=document.createElement("div"),b.setAttribute(a,"return;"),b="function"===typeof b[a]);return b}function Uf(a){a.topLevelType=null;a.nativeEvent=null;a.targetInst=null;a.ancestors.length=0;10>qc.length&&qc.push(a)}function Vf(a,b,c,d){if(qc.length){var e=qc.pop();e.topLevelType=a;e.eventSystemFlags=d;e.nativeEvent=b;e.targetInst=c;return e}return{topLevelType:a,eventSystemFlags:d,nativeEvent:b,targetInst:c,ancestors:[]}}function Wf(a){var b=
a.targetInst,c=b;do{if(!c){a.ancestors.push(c);break}var d=c;if(3===d.tag)d=d.stateNode.containerInfo;else{for(;d.return;)d=d.return;d=3!==d.tag?null:d.stateNode.containerInfo}if(!d)break;b=c.tag;5!==b&&6!==b||a.ancestors.push(c);c=Bb(d)}while(c);for(c=0;c<a.ancestors.length;c++){b=a.ancestors[c];var e=Ld(a.nativeEvent);d=a.topLevelType;var f=a.nativeEvent,g=a.eventSystemFlags;0===c&&(g|=64);for(var h=null,m=0;m<jc.length;m++){var n=jc[m];n&&(n=n.extractEvents(d,b,f,e,g))&&(h=jb(h,n))}pc(h)}}function Md(a,
b,c){if(!c.has(a)){switch(a){case "scroll":Cb(b,"scroll",!0);break;case "focus":case "blur":Cb(b,"focus",!0);Cb(b,"blur",!0);c.set("blur",null);c.set("focus",null);break;case "cancel":case "close":Tf(a)&&Cb(b,a,!0);break;case "invalid":case "submit":case "reset":break;default:-1===Db.indexOf(a)&&w(a,b)}c.set(a,null)}}function xi(a,b){var c=Jd(b);Nd.forEach(function(a){Md(a,b,c)});yi.forEach(function(a){Md(a,b,c)})}function Od(a,b,c,d,e){return{blockedOn:a,topLevelType:b,eventSystemFlags:c|32,nativeEvent:e,
container:d}}function Xf(a,b){switch(a){case "focus":case "blur":xa=null;break;case "dragenter":case "dragleave":ya=null;break;case "mouseover":case "mouseout":za=null;break;case "pointerover":case "pointerout":Eb.delete(b.pointerId);break;case "gotpointercapture":case "lostpointercapture":Fb.delete(b.pointerId)}}function Gb(a,b,c,d,e,f){if(null===a||a.nativeEvent!==f)return a=Od(b,c,d,e,f),null!==b&&(b=Hb(b),null!==b&&Yf(b)),a;a.eventSystemFlags|=d;return a}function zi(a,b,c,d,e){switch(b){case "focus":return xa=
Gb(xa,a,b,c,d,e),!0;case "dragenter":return ya=Gb(ya,a,b,c,d,e),!0;case "mouseover":return za=Gb(za,a,b,c,d,e),!0;case "pointerover":var f=e.pointerId;Eb.set(f,Gb(Eb.get(f)||null,a,b,c,d,e));return!0;case "gotpointercapture":return f=e.pointerId,Fb.set(f,Gb(Fb.get(f)||null,a,b,c,d,e)),!0}return!1}function Ai(a){var b=Bb(a.target);if(null!==b){var c=Na(b);if(null!==c)if(b=c.tag,13===b){if(b=Qf(c),null!==b){a.blockedOn=b;Pd(a.priority,function(){Bi(c)});return}}else if(3===b&&c.stateNode.hydrate){a.blockedOn=
3===c.tag?c.stateNode.containerInfo:null;return}}a.blockedOn=null}function rc(a){if(null!==a.blockedOn)return!1;var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);if(null!==b){var c=Hb(b);null!==c&&Yf(c);a.blockedOn=b;return!1}return!0}function Zf(a,b,c){rc(a)&&c.delete(b)}function Ci(){for(Rd=!1;0<fa.length;){var a=fa[0];if(null!==a.blockedOn){a=Hb(a.blockedOn);null!==a&&Di(a);break}var b=Qd(a.topLevelType,a.eventSystemFlags,a.container,a.nativeEvent);null!==b?a.blockedOn=b:fa.shift()}null!==
xa&&rc(xa)&&(xa=null);null!==ya&&rc(ya)&&(ya=null);null!==za&&rc(za)&&(za=null);Eb.forEach(Zf);Fb.forEach(Zf)}function Ib(a,b){a.blockedOn===b&&(a.blockedOn=null,Rd||(Rd=!0,$f(ag,Ci)))}function bg(a){if(0<fa.length){Ib(fa[0],a);for(var b=1;b<fa.length;b++){var c=fa[b];c.blockedOn===a&&(c.blockedOn=null)}}null!==xa&&Ib(xa,a);null!==ya&&Ib(ya,a);null!==za&&Ib(za,a);b=function(b){return Ib(b,a)};Eb.forEach(b);Fb.forEach(b);for(b=0;b<Jb.length;b++)c=Jb[b],c.blockedOn===a&&(c.blockedOn=null);for(;0<Jb.length&&
(b=Jb[0],null===b.blockedOn);)Ai(b),null===b.blockedOn&&Jb.shift()}function Sd(a,b){for(var c=0;c<a.length;c+=2){var d=a[c],e=a[c+1],f="on"+(e[0].toUpperCase()+e.slice(1));f={phasedRegistrationNames:{bubbled:f,captured:f+"Capture"},dependencies:[d],eventPriority:b};Td.set(d,b);cg.set(d,f);dg[e]=f}}function w(a,b){Cb(b,a,!1)}function Cb(a,b,c){var d=Td.get(b);switch(void 0===d?2:d){case 0:d=Ei.bind(null,b,1,a);break;case 1:d=Fi.bind(null,b,1,a);break;default:d=sc.bind(null,b,1,a)}c?a.addEventListener(b,
d,!0):a.addEventListener(b,d,!1)}function Ei(a,b,c,d){Oa||vd();var e=sc,f=Oa;Oa=!0;try{eg(e,a,b,c,d)}finally{(Oa=f)||ud()}}function Fi(a,b,c,d){Gi(Hi,sc.bind(null,a,b,c,d))}function sc(a,b,c,d){if(tc)if(0<fa.length&&-1<Nd.indexOf(a))a=Od(null,a,b,c,d),fa.push(a);else{var e=Qd(a,b,c,d);if(null===e)Xf(a,d);else if(-1<Nd.indexOf(a))a=Od(e,a,b,c,d),fa.push(a);else if(!zi(e,a,b,c,d)){Xf(a,d);a=Vf(a,d,null,b);try{uf(Wf,a)}finally{Uf(a)}}}}function Qd(a,b,c,d){c=Ld(d);c=Bb(c);if(null!==c){var e=Na(c);if(null===
e)c=null;else{var f=e.tag;if(13===f){c=Qf(e);if(null!==c)return c;c=null}else if(3===f){if(e.stateNode.hydrate)return 3===e.tag?e.stateNode.containerInfo:null;c=null}else e!==c&&(c=null)}}a=Vf(a,d,c,b);try{uf(Wf,a)}finally{Uf(a)}return null}function fg(a,b,c){return null==b||"boolean"===typeof b||""===b?"":c||"number"!==typeof b||0===b||Kb.hasOwnProperty(a)&&Kb[a]?(""+b).trim():b+"px"}function gg(a,b){a=a.style;for(var c in b)if(b.hasOwnProperty(c)){var d=0===c.indexOf("--"),e=fg(c,b[c],d);"float"===
c&&(c="cssFloat");d?a.setProperty(c,e):a[c]=e}}function Ud(a,b){if(b){if(Ii[a]&&(null!=b.children||null!=b.dangerouslySetInnerHTML))throw Error(k(137,a,""));if(null!=b.dangerouslySetInnerHTML){if(null!=b.children)throw Error(k(60));if(!("object"===typeof b.dangerouslySetInnerHTML&&"__html"in b.dangerouslySetInnerHTML))throw Error(k(61));}if(null!=b.style&&"object"!==typeof b.style)throw Error(k(62,""));}}function Vd(a,b){if(-1===a.indexOf("-"))return"string"===typeof b.is;switch(a){case "annotation-xml":case "color-profile":case "font-face":case "font-face-src":case "font-face-uri":case "font-face-format":case "font-face-name":case "missing-glyph":return!1;
default:return!0}}function oa(a,b){a=9===a.nodeType||11===a.nodeType?a:a.ownerDocument;var c=Jd(a);b=rd[b];for(var d=0;d<b.length;d++)Md(b[d],a,c)}function uc(){}function Wd(a){a=a||("undefined"!==typeof document?document:void 0);if("undefined"===typeof a)return null;try{return a.activeElement||a.body}catch(b){return a.body}}function hg(a){for(;a&&a.firstChild;)a=a.firstChild;return a}function ig(a,b){var c=hg(a);a=0;for(var d;c;){if(3===c.nodeType){d=a+c.textContent.length;if(a<=b&&d>=b)return{node:c,
offset:b-a};a=d}a:{for(;c;){if(c.nextSibling){c=c.nextSibling;break a}c=c.parentNode}c=void 0}c=hg(c)}}function jg(a,b){return a&&b?a===b?!0:a&&3===a.nodeType?!1:b&&3===b.nodeType?jg(a,b.parentNode):"contains"in a?a.contains(b):a.compareDocumentPosition?!!(a.compareDocumentPosition(b)&16):!1:!1}function kg(){for(var a=window,b=Wd();b instanceof a.HTMLIFrameElement;){try{var c="string"===typeof b.contentWindow.location.href}catch(d){c=!1}if(c)a=b.contentWindow;else break;b=Wd(a.document)}return b}
function Xd(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return b&&("input"===b&&("text"===a.type||"search"===a.type||"tel"===a.type||"url"===a.type||"password"===a.type)||"textarea"===b||"true"===a.contentEditable)}function lg(a,b){switch(a){case "button":case "input":case "select":case "textarea":return!!b.autoFocus}return!1}function Yd(a,b){return"textarea"===a||"option"===a||"noscript"===a||"string"===typeof b.children||"number"===typeof b.children||"object"===typeof b.dangerouslySetInnerHTML&&
null!==b.dangerouslySetInnerHTML&&null!=b.dangerouslySetInnerHTML.__html}function kb(a){for(;null!=a;a=a.nextSibling){var b=a.nodeType;if(1===b||3===b)break}return a}function mg(a){a=a.previousSibling;for(var b=0;a;){if(8===a.nodeType){var c=a.data;if(c===ng||c===Zd||c===$d){if(0===b)return a;b--}else c===og&&b++}a=a.previousSibling}return null}function Bb(a){var b=a[Aa];if(b)return b;for(var c=a.parentNode;c;){if(b=c[Lb]||c[Aa]){c=b.alternate;if(null!==b.child||null!==c&&null!==c.child)for(a=mg(a);null!==
a;){if(c=a[Aa])return c;a=mg(a)}return b}a=c;c=a.parentNode}return null}function Hb(a){a=a[Aa]||a[Lb];return!a||5!==a.tag&&6!==a.tag&&13!==a.tag&&3!==a.tag?null:a}function Pa(a){if(5===a.tag||6===a.tag)return a.stateNode;throw Error(k(33));}function ae(a){return a[vc]||null}function pa(a){do a=a.return;while(a&&5!==a.tag);return a?a:null}function pg(a,b){var c=a.stateNode;if(!c)return null;var d=td(c);if(!d)return null;c=d[b];a:switch(b){case "onClick":case "onClickCapture":case "onDoubleClick":case "onDoubleClickCapture":case "onMouseDown":case "onMouseDownCapture":case "onMouseMove":case "onMouseMoveCapture":case "onMouseUp":case "onMouseUpCapture":case "onMouseEnter":(d=
!d.disabled)||(a=a.type,d=!("button"===a||"input"===a||"select"===a||"textarea"===a));a=!d;break a;default:a=!1}if(a)return null;if(c&&"function"!==typeof c)throw Error(k(231,b,typeof c));return c}function qg(a,b,c){if(b=pg(a,c.dispatchConfig.phasedRegistrationNames[b]))c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a)}function Ji(a){if(a&&a.dispatchConfig.phasedRegistrationNames){for(var b=a._targetInst,c=[];b;)c.push(b),b=pa(b);for(b=c.length;0<b--;)qg(c[b],
"captured",a);for(b=0;b<c.length;b++)qg(c[b],"bubbled",a)}}function be(a,b,c){a&&c&&c.dispatchConfig.registrationName&&(b=pg(a,c.dispatchConfig.registrationName))&&(c._dispatchListeners=jb(c._dispatchListeners,b),c._dispatchInstances=jb(c._dispatchInstances,a))}function Ki(a){a&&a.dispatchConfig.registrationName&&be(a._targetInst,null,a)}function lb(a){Kd(a,Ji)}function rg(){if(wc)return wc;var a,b=ce,c=b.length,d,e="value"in Ba?Ba.value:Ba.textContent,f=e.length;for(a=0;a<c&&b[a]===e[a];a++);var g=
c-a;for(d=1;d<=g&&b[c-d]===e[f-d];d++);return wc=e.slice(a,1<d?1-d:void 0)}function xc(){return!0}function yc(){return!1}function R(a,b,c,d){this.dispatchConfig=a;this._targetInst=b;this.nativeEvent=c;a=this.constructor.Interface;for(var e in a)a.hasOwnProperty(e)&&((b=a[e])?this[e]=b(c):"target"===e?this.target=d:this[e]=c[e]);this.isDefaultPrevented=(null!=c.defaultPrevented?c.defaultPrevented:!1===c.returnValue)?xc:yc;this.isPropagationStopped=yc;return this}function Li(a,b,c,d){if(this.eventPool.length){var e=
this.eventPool.pop();this.call(e,a,b,c,d);return e}return new this(a,b,c,d)}function Mi(a){if(!(a instanceof this))throw Error(k(279));a.destructor();10>this.eventPool.length&&this.eventPool.push(a)}function sg(a){a.eventPool=[];a.getPooled=Li;a.release=Mi}function tg(a,b){switch(a){case "keyup":return-1!==Ni.indexOf(b.keyCode);case "keydown":return 229!==b.keyCode;case "keypress":case "mousedown":case "blur":return!0;default:return!1}}function ug(a){a=a.detail;return"object"===typeof a&&"data"in
a?a.data:null}function Oi(a,b){switch(a){case "compositionend":return ug(b);case "keypress":if(32!==b.which)return null;vg=!0;return wg;case "textInput":return a=b.data,a===wg&&vg?null:a;default:return null}}function Pi(a,b){if(mb)return"compositionend"===a||!de&&tg(a,b)?(a=rg(),wc=ce=Ba=null,mb=!1,a):null;switch(a){case "paste":return null;case "keypress":if(!(b.ctrlKey||b.altKey||b.metaKey)||b.ctrlKey&&b.altKey){if(b.char&&1<b.char.length)return b.char;if(b.which)return String.fromCharCode(b.which)}return null;
case "compositionend":return xg&&"ko"!==b.locale?null:b.data;default:return null}}function yg(a){var b=a&&a.nodeName&&a.nodeName.toLowerCase();return"input"===b?!!Qi[a.type]:"textarea"===b?!0:!1}function zg(a,b,c){a=R.getPooled(Ag.change,a,b,c);a.type="change";sf(c);lb(a);return a}function Ri(a){pc(a)}function zc(a){var b=Pa(a);if(Gf(b))return a}function Si(a,b){if("change"===a)return b}function Bg(){Mb&&(Mb.detachEvent("onpropertychange",Cg),Nb=Mb=null)}function Cg(a){if("value"===a.propertyName&&
zc(Nb))if(a=zg(Nb,a,Ld(a)),Oa)pc(a);else{Oa=!0;try{ee(Ri,a)}finally{Oa=!1,ud()}}}function Ti(a,b,c){"focus"===a?(Bg(),Mb=b,Nb=c,Mb.attachEvent("onpropertychange",Cg)):"blur"===a&&Bg()}function Ui(a,b){if("selectionchange"===a||"keyup"===a||"keydown"===a)return zc(Nb)}function Vi(a,b){if("click"===a)return zc(b)}function Wi(a,b){if("input"===a||"change"===a)return zc(b)}function Xi(a){var b=this.nativeEvent;return b.getModifierState?b.getModifierState(a):(a=Yi[a])?!!b[a]:!1}function fe(a){return Xi}
function Zi(a,b){return a===b&&(0!==a||1/a===1/b)||a!==a&&b!==b}function Ob(a,b){if(Qa(a,b))return!0;if("object"!==typeof a||null===a||"object"!==typeof b||null===b)return!1;var c=Object.keys(a),d=Object.keys(b);if(c.length!==d.length)return!1;for(d=0;d<c.length;d++)if(!$i.call(b,c[d])||!Qa(a[c[d]],b[c[d]]))return!1;return!0}function Dg(a,b){var c=b.window===b?b.document:9===b.nodeType?b:b.ownerDocument;if(ge||null==nb||nb!==Wd(c))return null;c=nb;"selectionStart"in c&&Xd(c)?c={start:c.selectionStart,
end:c.selectionEnd}:(c=(c.ownerDocument&&c.ownerDocument.defaultView||window).getSelection(),c={anchorNode:c.anchorNode,anchorOffset:c.anchorOffset,focusNode:c.focusNode,focusOffset:c.focusOffset});return Pb&&Ob(Pb,c)?null:(Pb=c,a=R.getPooled(Eg.select,he,a,b),a.type="select",a.target=nb,lb(a),a)}function Ac(a){var b=a.keyCode;"charCode"in a?(a=a.charCode,0===a&&13===b&&(a=13)):a=b;10===a&&(a=13);return 32<=a||13===a?a:0}function q(a,b){0>ob||(a.current=ie[ob],ie[ob]=null,ob--)}function y(a,b,c){ob++;
ie[ob]=a.current;a.current=b}function pb(a,b){var c=a.type.contextTypes;if(!c)return Ca;var d=a.stateNode;if(d&&d.__reactInternalMemoizedUnmaskedChildContext===b)return d.__reactInternalMemoizedMaskedChildContext;var e={},f;for(f in c)e[f]=b[f];d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=b,a.__reactInternalMemoizedMaskedChildContext=e);return e}function N(a){a=a.childContextTypes;return null!==a&&void 0!==a}function Fg(a,b,c){if(B.current!==Ca)throw Error(k(168));y(B,b);y(G,c)}
function Gg(a,b,c){var d=a.stateNode;a=b.childContextTypes;if("function"!==typeof d.getChildContext)return c;d=d.getChildContext();for(var e in d)if(!(e in a))throw Error(k(108,na(b)||"Unknown",e));return M({},c,{},d)}function Bc(a){a=(a=a.stateNode)&&a.__reactInternalMemoizedMergedChildContext||Ca;Ra=B.current;y(B,a);y(G,G.current);return!0}function Hg(a,b,c){var d=a.stateNode;if(!d)throw Error(k(169));c?(a=Gg(a,b,Ra),d.__reactInternalMemoizedMergedChildContext=a,q(G),q(B),y(B,a)):q(G);y(G,c)}function Cc(){switch(aj()){case Dc:return 99;
case Ig:return 98;case Jg:return 97;case Kg:return 96;case Lg:return 95;default:throw Error(k(332));}}function Mg(a){switch(a){case 99:return Dc;case 98:return Ig;case 97:return Jg;case 96:return Kg;case 95:return Lg;default:throw Error(k(332));}}function Da(a,b){a=Mg(a);return bj(a,b)}function Ng(a,b,c){a=Mg(a);return je(a,b,c)}function Og(a){null===qa?(qa=[a],Ec=je(Dc,Pg)):qa.push(a);return Qg}function ha(){if(null!==Ec){var a=Ec;Ec=null;Rg(a)}Pg()}function Pg(){if(!ke&&null!==qa){ke=!0;var a=0;
try{var b=qa;Da(99,function(){for(;a<b.length;a++){var c=b[a];do c=c(!0);while(null!==c)}});qa=null}catch(c){throw null!==qa&&(qa=qa.slice(a+1)),je(Dc,ha),c;}finally{ke=!1}}}function Fc(a,b,c){c/=10;return 1073741821-(((1073741821-a+b/10)/c|0)+1)*c}function aa(a,b){if(a&&a.defaultProps){b=M({},b);a=a.defaultProps;for(var c in a)void 0===b[c]&&(b[c]=a[c])}return b}function le(){Gc=qb=Hc=null}function me(a){var b=Ic.current;q(Ic);a.type._context._currentValue=b}function Sg(a,b){for(;null!==a;){var c=
a.alternate;if(a.childExpirationTime<b)a.childExpirationTime=b,null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);else if(null!==c&&c.childExpirationTime<b)c.childExpirationTime=b;else break;a=a.return}}function rb(a,b){Hc=a;Gc=qb=null;a=a.dependencies;null!==a&&null!==a.firstContext&&(a.expirationTime>=b&&(ia=!0),a.firstContext=null)}function W(a,b){if(Gc!==a&&!1!==b&&0!==b){if("number"!==typeof b||1073741823===b)Gc=a,b=1073741823;b={context:a,observedBits:b,next:null};if(null===qb){if(null===
Hc)throw Error(k(308));qb=b;Hc.dependencies={expirationTime:0,firstContext:b,responders:null}}else qb=qb.next=b}return a._currentValue}function ne(a){a.updateQueue={baseState:a.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oe(a,b){a=a.updateQueue;b.updateQueue===a&&(b.updateQueue={baseState:a.baseState,baseQueue:a.baseQueue,shared:a.shared,effects:a.effects})}function Ea(a,b){a={expirationTime:a,suspenseConfig:b,tag:Tg,payload:null,callback:null,next:null};return a.next=
a}function Fa(a,b){a=a.updateQueue;if(null!==a){a=a.shared;var c=a.pending;null===c?b.next=b:(b.next=c.next,c.next=b);a.pending=b}}function Ug(a,b){var c=a.alternate;null!==c&&oe(c,a);a=a.updateQueue;c=a.baseQueue;null===c?(a.baseQueue=b.next=b,b.next=b):(b.next=c.next,c.next=b)}function Qb(a,b,c,d){var e=a.updateQueue;Ga=!1;var f=e.baseQueue,g=e.shared.pending;if(null!==g){if(null!==f){var h=f.next;f.next=g.next;g.next=h}f=g;e.shared.pending=null;h=a.alternate;null!==h&&(h=h.updateQueue,null!==h&&
(h.baseQueue=g))}if(null!==f){h=f.next;var m=e.baseState,n=0,k=null,ba=null,l=null;if(null!==h){var p=h;do{g=p.expirationTime;if(g<d){var t={expirationTime:p.expirationTime,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null};null===l?(ba=l=t,k=m):l=l.next=t;g>n&&(n=g)}else{null!==l&&(l=l.next={expirationTime:1073741823,suspenseConfig:p.suspenseConfig,tag:p.tag,payload:p.payload,callback:p.callback,next:null});Vg(g,p.suspenseConfig);a:{var q=a,r=p;g=b;t=c;switch(r.tag){case 1:q=
r.payload;if("function"===typeof q){m=q.call(t,m,g);break a}m=q;break a;case 3:q.effectTag=q.effectTag&-4097|64;case Tg:q=r.payload;g="function"===typeof q?q.call(t,m,g):q;if(null===g||void 0===g)break a;m=M({},m,g);break a;case Jc:Ga=!0}}null!==p.callback&&(a.effectTag|=32,g=e.effects,null===g?e.effects=[p]:g.push(p))}p=p.next;if(null===p||p===h)if(g=e.shared.pending,null===g)break;else p=f.next=g.next,g.next=h,e.baseQueue=f=g,e.shared.pending=null}while(1)}null===l?k=m:l.next=ba;e.baseState=k;e.baseQueue=
l;Kc(n);a.expirationTime=n;a.memoizedState=m}}function Wg(a,b,c){a=b.effects;b.effects=null;if(null!==a)for(b=0;b<a.length;b++){var d=a[b],e=d.callback;if(null!==e){d.callback=null;d=e;e=c;if("function"!==typeof d)throw Error(k(191,d));d.call(e)}}}function Lc(a,b,c,d){b=a.memoizedState;c=c(d,b);c=null===c||void 0===c?b:M({},b,c);a.memoizedState=c;0===a.expirationTime&&(a.updateQueue.baseState=c)}function Xg(a,b,c,d,e,f,g){a=a.stateNode;return"function"===typeof a.shouldComponentUpdate?a.shouldComponentUpdate(d,
f,g):b.prototype&&b.prototype.isPureReactComponent?!Ob(c,d)||!Ob(e,f):!0}function Yg(a,b,c){var d=!1,e=Ca;var f=b.contextType;"object"===typeof f&&null!==f?f=W(f):(e=N(b)?Ra:B.current,d=b.contextTypes,f=(d=null!==d&&void 0!==d)?pb(a,e):Ca);b=new b(c,f);a.memoizedState=null!==b.state&&void 0!==b.state?b.state:null;b.updater=Mc;a.stateNode=b;b._reactInternalFiber=a;d&&(a=a.stateNode,a.__reactInternalMemoizedUnmaskedChildContext=e,a.__reactInternalMemoizedMaskedChildContext=f);return b}function Zg(a,
b,c,d){a=b.state;"function"===typeof b.componentWillReceiveProps&&b.componentWillReceiveProps(c,d);"function"===typeof b.UNSAFE_componentWillReceiveProps&&b.UNSAFE_componentWillReceiveProps(c,d);b.state!==a&&Mc.enqueueReplaceState(b,b.state,null)}function pe(a,b,c,d){var e=a.stateNode;e.props=c;e.state=a.memoizedState;e.refs=$g;ne(a);var f=b.contextType;"object"===typeof f&&null!==f?e.context=W(f):(f=N(b)?Ra:B.current,e.context=pb(a,f));Qb(a,c,e,d);e.state=a.memoizedState;f=b.getDerivedStateFromProps;
"function"===typeof f&&(Lc(a,b,f,c),e.state=a.memoizedState);"function"===typeof b.getDerivedStateFromProps||"function"===typeof e.getSnapshotBeforeUpdate||"function"!==typeof e.UNSAFE_componentWillMount&&"function"!==typeof e.componentWillMount||(b=e.state,"function"===typeof e.componentWillMount&&e.componentWillMount(),"function"===typeof e.UNSAFE_componentWillMount&&e.UNSAFE_componentWillMount(),b!==e.state&&Mc.enqueueReplaceState(e,e.state,null),Qb(a,c,e,d),e.state=a.memoizedState);"function"===
typeof e.componentDidMount&&(a.effectTag|=4)}function Rb(a,b,c){a=c.ref;if(null!==a&&"function"!==typeof a&&"object"!==typeof a){if(c._owner){c=c._owner;if(c){if(1!==c.tag)throw Error(k(309));var d=c.stateNode}if(!d)throw Error(k(147,a));var e=""+a;if(null!==b&&null!==b.ref&&"function"===typeof b.ref&&b.ref._stringRef===e)return b.ref;b=function(a){var b=d.refs;b===$g&&(b=d.refs={});null===a?delete b[e]:b[e]=a};b._stringRef=e;return b}if("string"!==typeof a)throw Error(k(284));if(!c._owner)throw Error(k(290,
a));}return a}function Nc(a,b){if("textarea"!==a.type)throw Error(k(31,"[object Object]"===Object.prototype.toString.call(b)?"object with keys {"+Object.keys(b).join(", ")+"}":b,""));}function ah(a){function b(b,c){if(a){var d=b.lastEffect;null!==d?(d.nextEffect=c,b.lastEffect=c):b.firstEffect=b.lastEffect=c;c.nextEffect=null;c.effectTag=8}}function c(c,d){if(!a)return null;for(;null!==d;)b(c,d),d=d.sibling;return null}function d(a,b){for(a=new Map;null!==b;)null!==b.key?a.set(b.key,b):a.set(b.index,
b),b=b.sibling;return a}function e(a,b){a=Sa(a,b);a.index=0;a.sibling=null;return a}function f(b,c,d){b.index=d;if(!a)return c;d=b.alternate;if(null!==d)return d=d.index,d<c?(b.effectTag=2,c):d;b.effectTag=2;return c}function g(b){a&&null===b.alternate&&(b.effectTag=2);return b}function h(a,b,c,d){if(null===b||6!==b.tag)return b=qe(c,a.mode,d),b.return=a,b;b=e(b,c);b.return=a;return b}function m(a,b,c,d){if(null!==b&&b.elementType===c.type)return d=e(b,c.props),d.ref=Rb(a,b,c),d.return=a,d;d=Oc(c.type,
c.key,c.props,null,a.mode,d);d.ref=Rb(a,b,c);d.return=a;return d}function n(a,b,c,d){if(null===b||4!==b.tag||b.stateNode.containerInfo!==c.containerInfo||b.stateNode.implementation!==c.implementation)return b=re(c,a.mode,d),b.return=a,b;b=e(b,c.children||[]);b.return=a;return b}function l(a,b,c,d,f){if(null===b||7!==b.tag)return b=Ha(c,a.mode,d,f),b.return=a,b;b=e(b,c);b.return=a;return b}function ba(a,b,c){if("string"===typeof b||"number"===typeof b)return b=qe(""+b,a.mode,c),b.return=a,b;if("object"===
typeof b&&null!==b){switch(b.$$typeof){case Pc:return c=Oc(b.type,b.key,b.props,null,a.mode,c),c.ref=Rb(a,null,b),c.return=a,c;case gb:return b=re(b,a.mode,c),b.return=a,b}if(Qc(b)||zb(b))return b=Ha(b,a.mode,c,null),b.return=a,b;Nc(a,b)}return null}function p(a,b,c,d){var e=null!==b?b.key:null;if("string"===typeof c||"number"===typeof c)return null!==e?null:h(a,b,""+c,d);if("object"===typeof c&&null!==c){switch(c.$$typeof){case Pc:return c.key===e?c.type===Ma?l(a,b,c.props.children,d,e):m(a,b,c,
d):null;case gb:return c.key===e?n(a,b,c,d):null}if(Qc(c)||zb(c))return null!==e?null:l(a,b,c,d,null);Nc(a,c)}return null}function t(a,b,c,d,e){if("string"===typeof d||"number"===typeof d)return a=a.get(c)||null,h(b,a,""+d,e);if("object"===typeof d&&null!==d){switch(d.$$typeof){case Pc:return a=a.get(null===d.key?c:d.key)||null,d.type===Ma?l(b,a,d.props.children,e,d.key):m(b,a,d,e);case gb:return a=a.get(null===d.key?c:d.key)||null,n(b,a,d,e)}if(Qc(d)||zb(d))return a=a.get(c)||null,l(b,a,d,e,null);
Nc(b,d)}return null}function q(e,g,h,m){for(var n=null,k=null,l=g,r=g=0,C=null;null!==l&&r<h.length;r++){l.index>r?(C=l,l=null):C=l.sibling;var O=p(e,l,h[r],m);if(null===O){null===l&&(l=C);break}a&&l&&null===O.alternate&&b(e,l);g=f(O,g,r);null===k?n=O:k.sibling=O;k=O;l=C}if(r===h.length)return c(e,l),n;if(null===l){for(;r<h.length;r++)l=ba(e,h[r],m),null!==l&&(g=f(l,g,r),null===k?n=l:k.sibling=l,k=l);return n}for(l=d(e,l);r<h.length;r++)C=t(l,e,r,h[r],m),null!==C&&(a&&null!==C.alternate&&l.delete(null===
C.key?r:C.key),g=f(C,g,r),null===k?n=C:k.sibling=C,k=C);a&&l.forEach(function(a){return b(e,a)});return n}function w(e,g,h,n){var m=zb(h);if("function"!==typeof m)throw Error(k(150));h=m.call(h);if(null==h)throw Error(k(151));for(var l=m=null,r=g,C=g=0,O=null,v=h.next();null!==r&&!v.done;C++,v=h.next()){r.index>C?(O=r,r=null):O=r.sibling;var q=p(e,r,v.value,n);if(null===q){null===r&&(r=O);break}a&&r&&null===q.alternate&&b(e,r);g=f(q,g,C);null===l?m=q:l.sibling=q;l=q;r=O}if(v.done)return c(e,r),m;
if(null===r){for(;!v.done;C++,v=h.next())v=ba(e,v.value,n),null!==v&&(g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);return m}for(r=d(e,r);!v.done;C++,v=h.next())v=t(r,e,C,v.value,n),null!==v&&(a&&null!==v.alternate&&r.delete(null===v.key?C:v.key),g=f(v,g,C),null===l?m=v:l.sibling=v,l=v);a&&r.forEach(function(a){return b(e,a)});return m}return function(a,d,f,h){var m="object"===typeof f&&null!==f&&f.type===Ma&&null===f.key;m&&(f=f.props.children);var n="object"===typeof f&&null!==f;if(n)switch(f.$$typeof){case Pc:a:{n=
f.key;for(m=d;null!==m;){if(m.key===n){switch(m.tag){case 7:if(f.type===Ma){c(a,m.sibling);d=e(m,f.props.children);d.return=a;a=d;break a}break;default:if(m.elementType===f.type){c(a,m.sibling);d=e(m,f.props);d.ref=Rb(a,m,f);d.return=a;a=d;break a}}c(a,m);break}else b(a,m);m=m.sibling}f.type===Ma?(d=Ha(f.props.children,a.mode,h,f.key),d.return=a,a=d):(h=Oc(f.type,f.key,f.props,null,a.mode,h),h.ref=Rb(a,d,f),h.return=a,a=h)}return g(a);case gb:a:{for(m=f.key;null!==d;){if(d.key===m)if(4===d.tag&&d.stateNode.containerInfo===
f.containerInfo&&d.stateNode.implementation===f.implementation){c(a,d.sibling);d=e(d,f.children||[]);d.return=a;a=d;break a}else{c(a,d);break}else b(a,d);d=d.sibling}d=re(f,a.mode,h);d.return=a;a=d}return g(a)}if("string"===typeof f||"number"===typeof f)return f=""+f,null!==d&&6===d.tag?(c(a,d.sibling),d=e(d,f),d.return=a,a=d):(c(a,d),d=qe(f,a.mode,h),d.return=a,a=d),g(a);if(Qc(f))return q(a,d,f,h);if(zb(f))return w(a,d,f,h);n&&Nc(a,f);if("undefined"===typeof f&&!m)switch(a.tag){case 1:case 0:throw a=
a.type,Error(k(152,a.displayName||a.name||"Component"));}return c(a,d)}}function Ta(a){if(a===Sb)throw Error(k(174));return a}function se(a,b){y(Tb,b);y(Ub,a);y(ja,Sb);a=b.nodeType;switch(a){case 9:case 11:b=(b=b.documentElement)?b.namespaceURI:Hd(null,"");break;default:a=8===a?b.parentNode:b,b=a.namespaceURI||null,a=a.tagName,b=Hd(b,a)}q(ja);y(ja,b)}function tb(a){q(ja);q(Ub);q(Tb)}function bh(a){Ta(Tb.current);var b=Ta(ja.current);var c=Hd(b,a.type);b!==c&&(y(Ub,a),y(ja,c))}function te(a){Ub.current===
a&&(q(ja),q(Ub))}function Rc(a){for(var b=a;null!==b;){if(13===b.tag){var c=b.memoizedState;if(null!==c&&(c=c.dehydrated,null===c||c.data===$d||c.data===Zd))return b}else if(19===b.tag&&void 0!==b.memoizedProps.revealOrder){if(0!==(b.effectTag&64))return b}else if(null!==b.child){b.child.return=b;b=b.child;continue}if(b===a)break;for(;null===b.sibling;){if(null===b.return||b.return===a)return null;b=b.return}b.sibling.return=b.return;b=b.sibling}return null}function ue(a,b){return{responder:a,props:b}}
function S(){throw Error(k(321));}function ve(a,b){if(null===b)return!1;for(var c=0;c<b.length&&c<a.length;c++)if(!Qa(a[c],b[c]))return!1;return!0}function we(a,b,c,d,e,f){Ia=f;z=b;b.memoizedState=null;b.updateQueue=null;b.expirationTime=0;Sc.current=null===a||null===a.memoizedState?dj:ej;a=c(d,e);if(b.expirationTime===Ia){f=0;do{b.expirationTime=0;if(!(25>f))throw Error(k(301));f+=1;J=K=null;b.updateQueue=null;Sc.current=fj;a=c(d,e)}while(b.expirationTime===Ia)}Sc.current=Tc;b=null!==K&&null!==K.next;
Ia=0;J=K=z=null;Uc=!1;if(b)throw Error(k(300));return a}function ub(){var a={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};null===J?z.memoizedState=J=a:J=J.next=a;return J}function vb(){if(null===K){var a=z.alternate;a=null!==a?a.memoizedState:null}else a=K.next;var b=null===J?z.memoizedState:J.next;if(null!==b)J=b,K=a;else{if(null===a)throw Error(k(310));K=a;a={memoizedState:K.memoizedState,baseState:K.baseState,baseQueue:K.baseQueue,queue:K.queue,next:null};null===J?z.memoizedState=
J=a:J=J.next=a}return J}function Ua(a,b){return"function"===typeof b?b(a):b}function Vc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=K,e=d.baseQueue,f=c.pending;if(null!==f){if(null!==e){var g=e.next;e.next=f.next;f.next=g}d.baseQueue=e=f;c.pending=null}if(null!==e){e=e.next;d=d.baseState;var h=g=f=null,m=e;do{var n=m.expirationTime;if(n<Ia){var l={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,
next:null};null===h?(g=h=l,f=d):h=h.next=l;n>z.expirationTime&&(z.expirationTime=n,Kc(n))}else null!==h&&(h=h.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,action:m.action,eagerReducer:m.eagerReducer,eagerState:m.eagerState,next:null}),Vg(n,m.suspenseConfig),d=m.eagerReducer===a?m.eagerState:a(d,m.action);m=m.next}while(null!==m&&m!==e);null===h?f=d:h.next=g;Qa(d,b.memoizedState)||(ia=!0);b.memoizedState=d;b.baseState=f;b.baseQueue=h;c.lastRenderedState=d}return[b.memoizedState,
c.dispatch]}function Wc(a,b,c){b=vb();c=b.queue;if(null===c)throw Error(k(311));c.lastRenderedReducer=a;var d=c.dispatch,e=c.pending,f=b.memoizedState;if(null!==e){c.pending=null;var g=e=e.next;do f=a(f,g.action),g=g.next;while(g!==e);Qa(f,b.memoizedState)||(ia=!0);b.memoizedState=f;null===b.baseQueue&&(b.baseState=f);c.lastRenderedState=f}return[f,d]}function xe(a){var b=ub();"function"===typeof a&&(a=a());b.memoizedState=b.baseState=a;a=b.queue={pending:null,dispatch:null,lastRenderedReducer:Ua,
lastRenderedState:a};a=a.dispatch=ch.bind(null,z,a);return[b.memoizedState,a]}function ye(a,b,c,d){a={tag:a,create:b,destroy:c,deps:d,next:null};b=z.updateQueue;null===b?(b={lastEffect:null},z.updateQueue=b,b.lastEffect=a.next=a):(c=b.lastEffect,null===c?b.lastEffect=a.next=a:(d=c.next,c.next=a,a.next=d,b.lastEffect=a));return a}function dh(a){return vb().memoizedState}function ze(a,b,c,d){var e=ub();z.effectTag|=a;e.memoizedState=ye(1|b,c,void 0,void 0===d?null:d)}function Ae(a,b,c,d){var e=vb();
d=void 0===d?null:d;var f=void 0;if(null!==K){var g=K.memoizedState;f=g.destroy;if(null!==d&&ve(d,g.deps)){ye(b,c,f,d);return}}z.effectTag|=a;e.memoizedState=ye(1|b,c,f,d)}function eh(a,b){return ze(516,4,a,b)}function Xc(a,b){return Ae(516,4,a,b)}function fh(a,b){return Ae(4,2,a,b)}function gh(a,b){if("function"===typeof b)return a=a(),b(a),function(){b(null)};if(null!==b&&void 0!==b)return a=a(),b.current=a,function(){b.current=null}}function hh(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;
return Ae(4,2,gh.bind(null,b,a),c)}function Be(a,b){}function ih(a,b){ub().memoizedState=[a,void 0===b?null:b];return a}function Yc(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];c.memoizedState=[a,b];return a}function jh(a,b){var c=vb();b=void 0===b?null:b;var d=c.memoizedState;if(null!==d&&null!==b&&ve(b,d[1]))return d[0];a=a();c.memoizedState=[a,b];return a}function Ce(a,b,c){var d=Cc();Da(98>d?98:d,function(){a(!0)});Da(97<d?97:d,function(){var d=
X.suspense;X.suspense=void 0===b?null:b;try{a(!1),c()}finally{X.suspense=d}})}function ch(a,b,c){var d=ka(),e=Vb.suspense;d=Va(d,a,e);e={expirationTime:d,suspenseConfig:e,action:c,eagerReducer:null,eagerState:null,next:null};var f=b.pending;null===f?e.next=e:(e.next=f.next,f.next=e);b.pending=e;f=a.alternate;if(a===z||null!==f&&f===z)Uc=!0,e.expirationTime=Ia,z.expirationTime=Ia;else{if(0===a.expirationTime&&(null===f||0===f.expirationTime)&&(f=b.lastRenderedReducer,null!==f))try{var g=b.lastRenderedState,
h=f(g,c);e.eagerReducer=f;e.eagerState=h;if(Qa(h,g))return}catch(m){}finally{}Ja(a,d)}}function kh(a,b){var c=la(5,null,null,0);c.elementType="DELETED";c.type="DELETED";c.stateNode=b;c.return=a;c.effectTag=8;null!==a.lastEffect?(a.lastEffect.nextEffect=c,a.lastEffect=c):a.firstEffect=a.lastEffect=c}function lh(a,b){switch(a.tag){case 5:var c=a.type;b=1!==b.nodeType||c.toLowerCase()!==b.nodeName.toLowerCase()?null:b;return null!==b?(a.stateNode=b,!0):!1;case 6:return b=""===a.pendingProps||3!==b.nodeType?
null:b,null!==b?(a.stateNode=b,!0):!1;case 13:return!1;default:return!1}}function De(a){if(Wa){var b=Ka;if(b){var c=b;if(!lh(a,b)){b=kb(c.nextSibling);if(!b||!lh(a,b)){a.effectTag=a.effectTag&-1025|2;Wa=!1;ra=a;return}kh(ra,c)}ra=a;Ka=kb(b.firstChild)}else a.effectTag=a.effectTag&-1025|2,Wa=!1,ra=a}}function mh(a){for(a=a.return;null!==a&&5!==a.tag&&3!==a.tag&&13!==a.tag;)a=a.return;ra=a}function Zc(a){if(a!==ra)return!1;if(!Wa)return mh(a),Wa=!0,!1;var b=a.type;if(5!==a.tag||"head"!==b&&"body"!==
b&&!Yd(b,a.memoizedProps))for(b=Ka;b;)kh(a,b),b=kb(b.nextSibling);mh(a);if(13===a.tag){a=a.memoizedState;a=null!==a?a.dehydrated:null;if(!a)throw Error(k(317));a:{a=a.nextSibling;for(b=0;a;){if(8===a.nodeType){var c=a.data;if(c===og){if(0===b){Ka=kb(a.nextSibling);break a}b--}else c!==ng&&c!==Zd&&c!==$d||b++}a=a.nextSibling}Ka=null}}else Ka=ra?kb(a.stateNode.nextSibling):null;return!0}function Ee(){Ka=ra=null;Wa=!1}function T(a,b,c,d){b.child=null===a?Fe(b,null,c,d):wb(b,a.child,c,d)}function nh(a,
b,c,d,e){c=c.render;var f=b.ref;rb(b,e);d=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,d,e);return b.child}function oh(a,b,c,d,e,f){if(null===a){var g=c.type;if("function"===typeof g&&!Ge(g)&&void 0===g.defaultProps&&null===c.compare&&void 0===c.defaultProps)return b.tag=15,b.type=g,ph(a,b,g,d,e,f);a=Oc(c.type,null,d,null,b.mode,f);a.ref=b.ref;a.return=b;return b.child=a}g=a.child;if(e<
f&&(e=g.memoizedProps,c=c.compare,c=null!==c?c:Ob,c(e,d)&&a.ref===b.ref))return sa(a,b,f);b.effectTag|=1;a=Sa(g,d);a.ref=b.ref;a.return=b;return b.child=a}function ph(a,b,c,d,e,f){return null!==a&&Ob(a.memoizedProps,d)&&a.ref===b.ref&&(ia=!1,e<f)?(b.expirationTime=a.expirationTime,sa(a,b,f)):He(a,b,c,d,f)}function qh(a,b){var c=b.ref;if(null===a&&null!==c||null!==a&&a.ref!==c)b.effectTag|=128}function He(a,b,c,d,e){var f=N(c)?Ra:B.current;f=pb(b,f);rb(b,e);c=we(a,b,c,d,f,e);if(null!==a&&!ia)return b.updateQueue=
a.updateQueue,b.effectTag&=-517,a.expirationTime<=e&&(a.expirationTime=0),sa(a,b,e);b.effectTag|=1;T(a,b,c,e);return b.child}function rh(a,b,c,d,e){if(N(c)){var f=!0;Bc(b)}else f=!1;rb(b,e);if(null===b.stateNode)null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),Yg(b,c,d),pe(b,c,d,e),d=!0;else if(null===a){var g=b.stateNode,h=b.memoizedProps;g.props=h;var m=g.context,n=c.contextType;"object"===typeof n&&null!==n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n));var l=c.getDerivedStateFromProps,k="function"===
typeof l||"function"===typeof g.getSnapshotBeforeUpdate;k||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n);Ga=!1;var p=b.memoizedState;g.state=p;Qb(b,d,g,e);m=b.memoizedState;h!==d||p!==m||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),m=b.memoizedState),(h=Ga||Xg(b,c,h,d,p,m,n))?(k||"function"!==typeof g.UNSAFE_componentWillMount&&"function"!==typeof g.componentWillMount||("function"===typeof g.componentWillMount&&
g.componentWillMount(),"function"===typeof g.UNSAFE_componentWillMount&&g.UNSAFE_componentWillMount()),"function"===typeof g.componentDidMount&&(b.effectTag|=4)):("function"===typeof g.componentDidMount&&(b.effectTag|=4),b.memoizedProps=d,b.memoizedState=m),g.props=d,g.state=m,g.context=n,d=h):("function"===typeof g.componentDidMount&&(b.effectTag|=4),d=!1)}else g=b.stateNode,oe(a,b),h=b.memoizedProps,g.props=b.type===b.elementType?h:aa(b.type,h),m=g.context,n=c.contextType,"object"===typeof n&&null!==
n?n=W(n):(n=N(c)?Ra:B.current,n=pb(b,n)),l=c.getDerivedStateFromProps,(k="function"===typeof l||"function"===typeof g.getSnapshotBeforeUpdate)||"function"!==typeof g.UNSAFE_componentWillReceiveProps&&"function"!==typeof g.componentWillReceiveProps||(h!==d||m!==n)&&Zg(b,g,d,n),Ga=!1,m=b.memoizedState,g.state=m,Qb(b,d,g,e),p=b.memoizedState,h!==d||m!==p||G.current||Ga?("function"===typeof l&&(Lc(b,c,l,d),p=b.memoizedState),(l=Ga||Xg(b,c,h,d,m,p,n))?(k||"function"!==typeof g.UNSAFE_componentWillUpdate&&
"function"!==typeof g.componentWillUpdate||("function"===typeof g.componentWillUpdate&&g.componentWillUpdate(d,p,n),"function"===typeof g.UNSAFE_componentWillUpdate&&g.UNSAFE_componentWillUpdate(d,p,n)),"function"===typeof g.componentDidUpdate&&(b.effectTag|=4),"function"===typeof g.getSnapshotBeforeUpdate&&(b.effectTag|=256)):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===
a.memoizedState||(b.effectTag|=256),b.memoizedProps=d,b.memoizedState=p),g.props=d,g.state=p,g.context=n,d=l):("function"!==typeof g.componentDidUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=4),"function"!==typeof g.getSnapshotBeforeUpdate||h===a.memoizedProps&&m===a.memoizedState||(b.effectTag|=256),d=!1);return Ie(a,b,c,d,f,e)}function Ie(a,b,c,d,e,f){qh(a,b);var g=0!==(b.effectTag&64);if(!d&&!g)return e&&Hg(b,c,!1),sa(a,b,f);d=b.stateNode;gj.current=b;var h=g&&"function"!==typeof c.getDerivedStateFromError?
null:d.render();b.effectTag|=1;null!==a&&g?(b.child=wb(b,a.child,null,f),b.child=wb(b,null,h,f)):T(a,b,h,f);b.memoizedState=d.state;e&&Hg(b,c,!0);return b.child}function sh(a){var b=a.stateNode;b.pendingContext?Fg(a,b.pendingContext,b.pendingContext!==b.context):b.context&&Fg(a,b.context,!1);se(a,b.containerInfo)}function th(a,b,c){var d=b.mode,e=b.pendingProps,f=D.current,g=!1,h;(h=0!==(b.effectTag&64))||(h=0!==(f&2)&&(null===a||null!==a.memoizedState));h?(g=!0,b.effectTag&=-65):null!==a&&null===
a.memoizedState||void 0===e.fallback||!0===e.unstable_avoidThisFallback||(f|=1);y(D,f&1);if(null===a){void 0!==e.fallback&&De(b);if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;b.memoizedState=Je;b.child=e;return c}d=e.children;b.memoizedState=null;return b.child=Fe(b,null,d,c)}if(null!==a.memoizedState){a=a.child;d=a.sibling;if(g){e=e.fallback;
c=Sa(a,a.pendingProps);c.return=b;if(0===(b.mode&2)&&(g=null!==b.memoizedState?b.child.child:b.child,g!==a.child))for(c.child=g;null!==g;)g.return=c,g=g.sibling;d=Sa(d,e);d.return=b;c.sibling=d;c.childExpirationTime=0;b.memoizedState=Je;b.child=c;return d}c=wb(b,a.child,e.children,c);b.memoizedState=null;return b.child=c}a=a.child;if(g){g=e.fallback;e=Ha(null,d,0,null);e.return=b;e.child=a;null!==a&&(a.return=e);if(0===(b.mode&2))for(a=null!==b.memoizedState?b.child.child:b.child,e.child=a;null!==
a;)a.return=e,a=a.sibling;c=Ha(g,d,c,null);c.return=b;e.sibling=c;c.effectTag|=2;e.childExpirationTime=0;b.memoizedState=Je;b.child=e;return c}b.memoizedState=null;return b.child=wb(b,a,e.children,c)}function uh(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);Sg(a.return,b)}function Ke(a,b,c,d,e,f){var g=a.memoizedState;null===g?a.memoizedState={isBackwards:b,rendering:null,renderingStartTime:0,last:d,tail:c,tailExpiration:0,tailMode:e,
lastEffect:f}:(g.isBackwards=b,g.rendering=null,g.renderingStartTime=0,g.last=d,g.tail=c,g.tailExpiration=0,g.tailMode=e,g.lastEffect=f)}function vh(a,b,c){var d=b.pendingProps,e=d.revealOrder,f=d.tail;T(a,b,d.children,c);d=D.current;if(0!==(d&2))d=d&1|2,b.effectTag|=64;else{if(null!==a&&0!==(a.effectTag&64))a:for(a=b.child;null!==a;){if(13===a.tag)null!==a.memoizedState&&uh(a,c);else if(19===a.tag)uh(a,c);else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===b)break a;for(;null===a.sibling;){if(null===
a.return||a.return===b)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}d&=1}y(D,d);if(0===(b.mode&2))b.memoizedState=null;else switch(e){case "forwards":c=b.child;for(e=null;null!==c;)a=c.alternate,null!==a&&null===Rc(a)&&(e=c),c=c.sibling;c=e;null===c?(e=b.child,b.child=null):(e=c.sibling,c.sibling=null);Ke(b,!1,e,c,f,b.lastEffect);break;case "backwards":c=null;e=b.child;for(b.child=null;null!==e;){a=e.alternate;if(null!==a&&null===Rc(a)){b.child=e;break}a=e.sibling;e.sibling=c;c=e;e=a}Ke(b,
!0,c,null,f,b.lastEffect);break;case "together":Ke(b,!1,null,null,void 0,b.lastEffect);break;default:b.memoizedState=null}return b.child}function sa(a,b,c){null!==a&&(b.dependencies=a.dependencies);var d=b.expirationTime;0!==d&&Kc(d);if(b.childExpirationTime<c)return null;if(null!==a&&b.child!==a.child)throw Error(k(153));if(null!==b.child){a=b.child;c=Sa(a,a.pendingProps);b.child=c;for(c.return=b;null!==a.sibling;)a=a.sibling,c=c.sibling=Sa(a,a.pendingProps),c.return=b;c.sibling=null}return b.child}
function $c(a,b){switch(a.tailMode){case "hidden":b=a.tail;for(var c=null;null!==b;)null!==b.alternate&&(c=b),b=b.sibling;null===c?a.tail=null:c.sibling=null;break;case "collapsed":c=a.tail;for(var d=null;null!==c;)null!==c.alternate&&(d=c),c=c.sibling;null===d?b||null===a.tail?a.tail=null:a.tail.sibling=null:d.sibling=null}}function hj(a,b,c){var d=b.pendingProps;switch(b.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return N(b.type)&&(q(G),q(B)),
null;case 3:return tb(),q(G),q(B),c=b.stateNode,c.pendingContext&&(c.context=c.pendingContext,c.pendingContext=null),null!==a&&null!==a.child||!Zc(b)||(b.effectTag|=4),wh(b),null;case 5:te(b);c=Ta(Tb.current);var e=b.type;if(null!==a&&null!=b.stateNode)ij(a,b,e,d,c),a.ref!==b.ref&&(b.effectTag|=128);else{if(!d){if(null===b.stateNode)throw Error(k(166));return null}a=Ta(ja.current);if(Zc(b)){d=b.stateNode;e=b.type;var f=b.memoizedProps;d[Aa]=b;d[vc]=f;switch(e){case "iframe":case "object":case "embed":w("load",
d);break;case "video":case "audio":for(a=0;a<Db.length;a++)w(Db[a],d);break;case "source":w("error",d);break;case "img":case "image":case "link":w("error",d);w("load",d);break;case "form":w("reset",d);w("submit",d);break;case "details":w("toggle",d);break;case "input":Hf(d,f);w("invalid",d);oa(c,"onChange");break;case "select":d._wrapperState={wasMultiple:!!f.multiple};w("invalid",d);oa(c,"onChange");break;case "textarea":Kf(d,f),w("invalid",d),oa(c,"onChange")}Ud(e,f);a=null;for(var g in f)if(f.hasOwnProperty(g)){var h=
f[g];"children"===g?"string"===typeof h?d.textContent!==h&&(a=["children",h]):"number"===typeof h&&d.textContent!==""+h&&(a=["children",""+h]):db.hasOwnProperty(g)&&null!=h&&oa(c,g)}switch(e){case "input":mc(d);Jf(d,f,!0);break;case "textarea":mc(d);Mf(d);break;case "select":case "option":break;default:"function"===typeof f.onClick&&(d.onclick=uc)}c=a;b.updateQueue=c;null!==c&&(b.effectTag|=4)}else{g=9===c.nodeType?c:c.ownerDocument;"http://www.w3.org/1999/xhtml"===a&&(a=Nf(e));"http://www.w3.org/1999/xhtml"===
a?"script"===e?(a=g.createElement("div"),a.innerHTML="<script>\x3c/script>",a=a.removeChild(a.firstChild)):"string"===typeof d.is?a=g.createElement(e,{is:d.is}):(a=g.createElement(e),"select"===e&&(g=a,d.multiple?g.multiple=!0:d.size&&(g.size=d.size))):a=g.createElementNS(a,e);a[Aa]=b;a[vc]=d;jj(a,b,!1,!1);b.stateNode=a;g=Vd(e,d);switch(e){case "iframe":case "object":case "embed":w("load",a);h=d;break;case "video":case "audio":for(h=0;h<Db.length;h++)w(Db[h],a);h=d;break;case "source":w("error",a);
h=d;break;case "img":case "image":case "link":w("error",a);w("load",a);h=d;break;case "form":w("reset",a);w("submit",a);h=d;break;case "details":w("toggle",a);h=d;break;case "input":Hf(a,d);h=Cd(a,d);w("invalid",a);oa(c,"onChange");break;case "option":h=Fd(a,d);break;case "select":a._wrapperState={wasMultiple:!!d.multiple};h=M({},d,{value:void 0});w("invalid",a);oa(c,"onChange");break;case "textarea":Kf(a,d);h=Gd(a,d);w("invalid",a);oa(c,"onChange");break;default:h=d}Ud(e,h);var m=h;for(f in m)if(m.hasOwnProperty(f)){var n=
m[f];"style"===f?gg(a,n):"dangerouslySetInnerHTML"===f?(n=n?n.__html:void 0,null!=n&&xh(a,n)):"children"===f?"string"===typeof n?("textarea"!==e||""!==n)&&Wb(a,n):"number"===typeof n&&Wb(a,""+n):"suppressContentEditableWarning"!==f&&"suppressHydrationWarning"!==f&&"autoFocus"!==f&&(db.hasOwnProperty(f)?null!=n&&oa(c,f):null!=n&&xd(a,f,n,g))}switch(e){case "input":mc(a);Jf(a,d,!1);break;case "textarea":mc(a);Mf(a);break;case "option":null!=d.value&&a.setAttribute("value",""+va(d.value));break;case "select":a.multiple=
!!d.multiple;c=d.value;null!=c?hb(a,!!d.multiple,c,!1):null!=d.defaultValue&&hb(a,!!d.multiple,d.defaultValue,!0);break;default:"function"===typeof h.onClick&&(a.onclick=uc)}lg(e,d)&&(b.effectTag|=4)}null!==b.ref&&(b.effectTag|=128)}return null;case 6:if(a&&null!=b.stateNode)kj(a,b,a.memoizedProps,d);else{if("string"!==typeof d&&null===b.stateNode)throw Error(k(166));c=Ta(Tb.current);Ta(ja.current);Zc(b)?(c=b.stateNode,d=b.memoizedProps,c[Aa]=b,c.nodeValue!==d&&(b.effectTag|=4)):(c=(9===c.nodeType?
c:c.ownerDocument).createTextNode(d),c[Aa]=b,b.stateNode=c)}return null;case 13:q(D);d=b.memoizedState;if(0!==(b.effectTag&64))return b.expirationTime=c,b;c=null!==d;d=!1;null===a?void 0!==b.memoizedProps.fallback&&Zc(b):(e=a.memoizedState,d=null!==e,c||null===e||(e=a.child.sibling,null!==e&&(f=b.firstEffect,null!==f?(b.firstEffect=e,e.nextEffect=f):(b.firstEffect=b.lastEffect=e,e.nextEffect=null),e.effectTag=8)));if(c&&!d&&0!==(b.mode&2))if(null===a&&!0!==b.memoizedProps.unstable_avoidThisFallback||
0!==(D.current&1))F===Xa&&(F=ad);else{if(F===Xa||F===ad)F=bd;0!==Xb&&null!==U&&(Ya(U,P),yh(U,Xb))}if(c||d)b.effectTag|=4;return null;case 4:return tb(),wh(b),null;case 10:return me(b),null;case 17:return N(b.type)&&(q(G),q(B)),null;case 19:q(D);d=b.memoizedState;if(null===d)return null;e=0!==(b.effectTag&64);f=d.rendering;if(null===f)if(e)$c(d,!1);else{if(F!==Xa||null!==a&&0!==(a.effectTag&64))for(f=b.child;null!==f;){a=Rc(f);if(null!==a){b.effectTag|=64;$c(d,!1);e=a.updateQueue;null!==e&&(b.updateQueue=
e,b.effectTag|=4);null===d.lastEffect&&(b.firstEffect=null);b.lastEffect=d.lastEffect;for(d=b.child;null!==d;)e=d,f=c,e.effectTag&=2,e.nextEffect=null,e.firstEffect=null,e.lastEffect=null,a=e.alternate,null===a?(e.childExpirationTime=0,e.expirationTime=f,e.child=null,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null):(e.childExpirationTime=a.childExpirationTime,e.expirationTime=a.expirationTime,e.child=a.child,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,
e.updateQueue=a.updateQueue,f=a.dependencies,e.dependencies=null===f?null:{expirationTime:f.expirationTime,firstContext:f.firstContext,responders:f.responders}),d=d.sibling;y(D,D.current&1|2);return b.child}f=f.sibling}}else{if(!e)if(a=Rc(f),null!==a){if(b.effectTag|=64,e=!0,c=a.updateQueue,null!==c&&(b.updateQueue=c,b.effectTag|=4),$c(d,!0),null===d.tail&&"hidden"===d.tailMode&&!f.alternate)return b=b.lastEffect=d.lastEffect,null!==b&&(b.nextEffect=null),null}else 2*Y()-d.renderingStartTime>d.tailExpiration&&
1<c&&(b.effectTag|=64,e=!0,$c(d,!1),b.expirationTime=b.childExpirationTime=c-1);d.isBackwards?(f.sibling=b.child,b.child=f):(c=d.last,null!==c?c.sibling=f:b.child=f,d.last=f)}return null!==d.tail?(0===d.tailExpiration&&(d.tailExpiration=Y()+500),c=d.tail,d.rendering=c,d.tail=c.sibling,d.lastEffect=b.lastEffect,d.renderingStartTime=Y(),c.sibling=null,b=D.current,y(D,e?b&1|2:b&1),c):null}throw Error(k(156,b.tag));}function lj(a,b){switch(a.tag){case 1:return N(a.type)&&(q(G),q(B)),b=a.effectTag,b&4096?
(a.effectTag=b&-4097|64,a):null;case 3:tb();q(G);q(B);b=a.effectTag;if(0!==(b&64))throw Error(k(285));a.effectTag=b&-4097|64;return a;case 5:return te(a),null;case 13:return q(D),b=a.effectTag,b&4096?(a.effectTag=b&-4097|64,a):null;case 19:return q(D),null;case 4:return tb(),null;case 10:return me(a),null;default:return null}}function Le(a,b){return{value:a,source:b,stack:Bd(b)}}function Me(a,b){var c=b.source,d=b.stack;null===d&&null!==c&&(d=Bd(c));null!==c&&na(c.type);b=b.value;null!==a&&1===a.tag&&
na(a.type);try{console.error(b)}catch(e){setTimeout(function(){throw e;})}}function mj(a,b){try{b.props=a.memoizedProps,b.state=a.memoizedState,b.componentWillUnmount()}catch(c){Za(a,c)}}function zh(a){var b=a.ref;if(null!==b)if("function"===typeof b)try{b(null)}catch(c){Za(a,c)}else b.current=null}function nj(a,b){switch(b.tag){case 0:case 11:case 15:case 22:return;case 1:if(b.effectTag&256&&null!==a){var c=a.memoizedProps,d=a.memoizedState;a=b.stateNode;b=a.getSnapshotBeforeUpdate(b.elementType===
b.type?c:aa(b.type,c),d);a.__reactInternalSnapshotBeforeUpdate=b}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(k(163));}function Ah(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.destroy;c.destroy=void 0;void 0!==d&&d()}c=c.next}while(c!==b)}}function Bh(a,b){b=b.updateQueue;b=null!==b?b.lastEffect:null;if(null!==b){var c=b=b.next;do{if((c.tag&a)===a){var d=c.create;c.destroy=d()}c=c.next}while(c!==b)}}function oj(a,b,c,d){switch(c.tag){case 0:case 11:case 15:case 22:Bh(3,
c);return;case 1:a=c.stateNode;c.effectTag&4&&(null===b?a.componentDidMount():(d=c.elementType===c.type?b.memoizedProps:aa(c.type,b.memoizedProps),a.componentDidUpdate(d,b.memoizedState,a.__reactInternalSnapshotBeforeUpdate)));b=c.updateQueue;null!==b&&Wg(c,b,a);return;case 3:b=c.updateQueue;if(null!==b){a=null;if(null!==c.child)switch(c.child.tag){case 5:a=c.child.stateNode;break;case 1:a=c.child.stateNode}Wg(c,b,a)}return;case 5:a=c.stateNode;null===b&&c.effectTag&4&&lg(c.type,c.memoizedProps)&&
a.focus();return;case 6:return;case 4:return;case 12:return;case 13:null===c.memoizedState&&(c=c.alternate,null!==c&&(c=c.memoizedState,null!==c&&(c=c.dehydrated,null!==c&&bg(c))));return;case 19:case 17:case 20:case 21:return}throw Error(k(163));}function Ch(a,b,c){"function"===typeof Ne&&Ne(b);switch(b.tag){case 0:case 11:case 14:case 15:case 22:a=b.updateQueue;if(null!==a&&(a=a.lastEffect,null!==a)){var d=a.next;Da(97<c?97:c,function(){var a=d;do{var c=a.destroy;if(void 0!==c){var g=b;try{c()}catch(h){Za(g,
h)}}a=a.next}while(a!==d)})}break;case 1:zh(b);c=b.stateNode;"function"===typeof c.componentWillUnmount&&mj(b,c);break;case 5:zh(b);break;case 4:Dh(a,b,c)}}function Eh(a){var b=a.alternate;a.return=null;a.child=null;a.memoizedState=null;a.updateQueue=null;a.dependencies=null;a.alternate=null;a.firstEffect=null;a.lastEffect=null;a.pendingProps=null;a.memoizedProps=null;a.stateNode=null;null!==b&&Eh(b)}function Fh(a){return 5===a.tag||3===a.tag||4===a.tag}function Gh(a){a:{for(var b=a.return;null!==
b;){if(Fh(b)){var c=b;break a}b=b.return}throw Error(k(160));}b=c.stateNode;switch(c.tag){case 5:var d=!1;break;case 3:b=b.containerInfo;d=!0;break;case 4:b=b.containerInfo;d=!0;break;default:throw Error(k(161));}c.effectTag&16&&(Wb(b,""),c.effectTag&=-17);a:b:for(c=a;;){for(;null===c.sibling;){if(null===c.return||Fh(c.return)){c=null;break a}c=c.return}c.sibling.return=c.return;for(c=c.sibling;5!==c.tag&&6!==c.tag&&18!==c.tag;){if(c.effectTag&2)continue b;if(null===c.child||4===c.tag)continue b;
else c.child.return=c,c=c.child}if(!(c.effectTag&2)){c=c.stateNode;break a}}d?Oe(a,c,b):Pe(a,c,b)}function Oe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?8===c.nodeType?c.parentNode.insertBefore(a,b):c.insertBefore(a,b):(8===c.nodeType?(b=c.parentNode,b.insertBefore(a,c)):(b=c,b.appendChild(a)),c=c._reactRootContainer,null!==c&&void 0!==c||null!==b.onclick||(b.onclick=uc));else if(4!==d&&(a=a.child,null!==a))for(Oe(a,b,c),a=a.sibling;null!==a;)Oe(a,b,c),a=a.sibling}
function Pe(a,b,c){var d=a.tag,e=5===d||6===d;if(e)a=e?a.stateNode:a.stateNode.instance,b?c.insertBefore(a,b):c.appendChild(a);else if(4!==d&&(a=a.child,null!==a))for(Pe(a,b,c),a=a.sibling;null!==a;)Pe(a,b,c),a=a.sibling}function Dh(a,b,c){for(var d=b,e=!1,f,g;;){if(!e){e=d.return;a:for(;;){if(null===e)throw Error(k(160));f=e.stateNode;switch(e.tag){case 5:g=!1;break a;case 3:f=f.containerInfo;g=!0;break a;case 4:f=f.containerInfo;g=!0;break a}e=e.return}e=!0}if(5===d.tag||6===d.tag){a:for(var h=
a,m=d,n=c,l=m;;)if(Ch(h,l,n),null!==l.child&&4!==l.tag)l.child.return=l,l=l.child;else{if(l===m)break a;for(;null===l.sibling;){if(null===l.return||l.return===m)break a;l=l.return}l.sibling.return=l.return;l=l.sibling}g?(h=f,m=d.stateNode,8===h.nodeType?h.parentNode.removeChild(m):h.removeChild(m)):f.removeChild(d.stateNode)}else if(4===d.tag){if(null!==d.child){f=d.stateNode.containerInfo;g=!0;d.child.return=d;d=d.child;continue}}else if(Ch(a,d,c),null!==d.child){d.child.return=d;d=d.child;continue}if(d===
b)break;for(;null===d.sibling;){if(null===d.return||d.return===b)return;d=d.return;4===d.tag&&(e=!1)}d.sibling.return=d.return;d=d.sibling}}function Qe(a,b){switch(b.tag){case 0:case 11:case 14:case 15:case 22:Ah(3,b);return;case 1:return;case 5:var c=b.stateNode;if(null!=c){var d=b.memoizedProps,e=null!==a?a.memoizedProps:d;a=b.type;var f=b.updateQueue;b.updateQueue=null;if(null!==f){c[vc]=d;"input"===a&&"radio"===d.type&&null!=d.name&&If(c,d);Vd(a,e);b=Vd(a,d);for(e=0;e<f.length;e+=2){var g=f[e],
h=f[e+1];"style"===g?gg(c,h):"dangerouslySetInnerHTML"===g?xh(c,h):"children"===g?Wb(c,h):xd(c,g,h,b)}switch(a){case "input":Dd(c,d);break;case "textarea":Lf(c,d);break;case "select":b=c._wrapperState.wasMultiple,c._wrapperState.wasMultiple=!!d.multiple,a=d.value,null!=a?hb(c,!!d.multiple,a,!1):b!==!!d.multiple&&(null!=d.defaultValue?hb(c,!!d.multiple,d.defaultValue,!0):hb(c,!!d.multiple,d.multiple?[]:"",!1))}}}return;case 6:if(null===b.stateNode)throw Error(k(162));b.stateNode.nodeValue=b.memoizedProps;
return;case 3:b=b.stateNode;b.hydrate&&(b.hydrate=!1,bg(b.containerInfo));return;case 12:return;case 13:c=b;null===b.memoizedState?d=!1:(d=!0,c=b.child,Re=Y());if(null!==c)a:for(a=c;;){if(5===a.tag)f=a.stateNode,d?(f=f.style,"function"===typeof f.setProperty?f.setProperty("display","none","important"):f.display="none"):(f=a.stateNode,e=a.memoizedProps.style,e=void 0!==e&&null!==e&&e.hasOwnProperty("display")?e.display:null,f.style.display=fg("display",e));else if(6===a.tag)a.stateNode.nodeValue=d?
"":a.memoizedProps;else if(13===a.tag&&null!==a.memoizedState&&null===a.memoizedState.dehydrated){f=a.child.sibling;f.return=a;a=f;continue}else if(null!==a.child){a.child.return=a;a=a.child;continue}if(a===c)break;for(;null===a.sibling;){if(null===a.return||a.return===c)break a;a=a.return}a.sibling.return=a.return;a=a.sibling}Hh(b);return;case 19:Hh(b);return;case 17:return}throw Error(k(163));}function Hh(a){var b=a.updateQueue;if(null!==b){a.updateQueue=null;var c=a.stateNode;null===c&&(c=a.stateNode=
new pj);b.forEach(function(b){var d=qj.bind(null,a,b);c.has(b)||(c.add(b),b.then(d,d))})}}function Ih(a,b,c){c=Ea(c,null);c.tag=3;c.payload={element:null};var d=b.value;c.callback=function(){cd||(cd=!0,Se=d);Me(a,b)};return c}function Jh(a,b,c){c=Ea(c,null);c.tag=3;var d=a.type.getDerivedStateFromError;if("function"===typeof d){var e=b.value;c.payload=function(){Me(a,b);return d(e)}}var f=a.stateNode;null!==f&&"function"===typeof f.componentDidCatch&&(c.callback=function(){"function"!==typeof d&&
(null===La?La=new Set([this]):La.add(this),Me(a,b));var c=b.stack;this.componentDidCatch(b.value,{componentStack:null!==c?c:""})});return c}function ka(){return(p&(ca|ma))!==H?1073741821-(Y()/10|0):0!==dd?dd:dd=1073741821-(Y()/10|0)}function Va(a,b,c){b=b.mode;if(0===(b&2))return 1073741823;var d=Cc();if(0===(b&4))return 99===d?1073741823:1073741822;if((p&ca)!==H)return P;if(null!==c)a=Fc(a,c.timeoutMs|0||5E3,250);else switch(d){case 99:a=1073741823;break;case 98:a=Fc(a,150,100);break;case 97:case 96:a=
Fc(a,5E3,250);break;case 95:a=2;break;default:throw Error(k(326));}null!==U&&a===P&&--a;return a}function ed(a,b){a.expirationTime<b&&(a.expirationTime=b);var c=a.alternate;null!==c&&c.expirationTime<b&&(c.expirationTime=b);var d=a.return,e=null;if(null===d&&3===a.tag)e=a.stateNode;else for(;null!==d;){c=d.alternate;d.childExpirationTime<b&&(d.childExpirationTime=b);null!==c&&c.childExpirationTime<b&&(c.childExpirationTime=b);if(null===d.return&&3===d.tag){e=d.stateNode;break}d=d.return}null!==e&&
(U===e&&(Kc(b),F===bd&&Ya(e,P)),yh(e,b));return e}function fd(a){var b=a.lastExpiredTime;if(0!==b)return b;b=a.firstPendingTime;if(!Kh(a,b))return b;var c=a.lastPingedTime;a=a.nextKnownPendingLevel;a=c>a?c:a;return 2>=a&&b!==a?0:a}function V(a){if(0!==a.lastExpiredTime)a.callbackExpirationTime=1073741823,a.callbackPriority=99,a.callbackNode=Og(Te.bind(null,a));else{var b=fd(a),c=a.callbackNode;if(0===b)null!==c&&(a.callbackNode=null,a.callbackExpirationTime=0,a.callbackPriority=90);else{var d=ka();
1073741823===b?d=99:1===b||2===b?d=95:(d=10*(1073741821-b)-10*(1073741821-d),d=0>=d?99:250>=d?98:5250>=d?97:95);if(null!==c){var e=a.callbackPriority;if(a.callbackExpirationTime===b&&e>=d)return;c!==Qg&&Rg(c)}a.callbackExpirationTime=b;a.callbackPriority=d;b=1073741823===b?Og(Te.bind(null,a)):Ng(d,Lh.bind(null,a),{timeout:10*(1073741821-b)-Y()});a.callbackNode=b}}}function Lh(a,b){dd=0;if(b)return b=ka(),Ue(a,b),V(a),null;var c=fd(a);if(0!==c){b=a.callbackNode;if((p&(ca|ma))!==H)throw Error(k(327));
xb();a===U&&c===P||$a(a,c);if(null!==t){var d=p;p|=ca;var e=Mh();do try{rj();break}catch(h){Nh(a,h)}while(1);le();p=d;gd.current=e;if(F===hd)throw b=id,$a(a,c),Ya(a,c),V(a),b;if(null===t)switch(e=a.finishedWork=a.current.alternate,a.finishedExpirationTime=c,d=F,U=null,d){case Xa:case hd:throw Error(k(345));case Oh:Ue(a,2<c?2:c);break;case ad:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(1073741823===ta&&(e=Re+Ph-Y(),10<e)){if(jd){var f=a.lastPingedTime;if(0===f||f>=c){a.lastPingedTime=
c;$a(a,c);break}}f=fd(a);if(0!==f&&f!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}a.timeoutHandle=We(ab.bind(null,a),e);break}ab(a);break;case bd:Ya(a,c);d=a.lastSuspendedTime;c===d&&(a.nextKnownPendingLevel=Ve(e));if(jd&&(e=a.lastPingedTime,0===e||e>=c)){a.lastPingedTime=c;$a(a,c);break}e=fd(a);if(0!==e&&e!==c)break;if(0!==d&&d!==c){a.lastPingedTime=d;break}1073741823!==Yb?d=10*(1073741821-Yb)-Y():1073741823===ta?d=0:(d=10*(1073741821-ta)-5E3,e=Y(),c=10*(1073741821-c)-e,d=e-d,0>d&&(d=0),d=
(120>d?120:480>d?480:1080>d?1080:1920>d?1920:3E3>d?3E3:4320>d?4320:1960*sj(d/1960))-d,c<d&&(d=c));if(10<d){a.timeoutHandle=We(ab.bind(null,a),d);break}ab(a);break;case Xe:if(1073741823!==ta&&null!==kd){f=ta;var g=kd;d=g.busyMinDurationMs|0;0>=d?d=0:(e=g.busyDelayMs|0,f=Y()-(10*(1073741821-f)-(g.timeoutMs|0||5E3)),d=f<=e?0:e+d-f);if(10<d){Ya(a,c);a.timeoutHandle=We(ab.bind(null,a),d);break}}ab(a);break;default:throw Error(k(329));}V(a);if(a.callbackNode===b)return Lh.bind(null,a)}}return null}function Te(a){var b=
a.lastExpiredTime;b=0!==b?b:1073741823;if((p&(ca|ma))!==H)throw Error(k(327));xb();a===U&&b===P||$a(a,b);if(null!==t){var c=p;p|=ca;var d=Mh();do try{tj();break}catch(e){Nh(a,e)}while(1);le();p=c;gd.current=d;if(F===hd)throw c=id,$a(a,b),Ya(a,b),V(a),c;if(null!==t)throw Error(k(261));a.finishedWork=a.current.alternate;a.finishedExpirationTime=b;U=null;ab(a);V(a)}return null}function uj(){if(null!==bb){var a=bb;bb=null;a.forEach(function(a,c){Ue(c,a);V(c)});ha()}}function Qh(a,b){var c=p;p|=1;try{return a(b)}finally{p=
c,p===H&&ha()}}function Rh(a,b){var c=p;p&=-2;p|=Ye;try{return a(b)}finally{p=c,p===H&&ha()}}function $a(a,b){a.finishedWork=null;a.finishedExpirationTime=0;var c=a.timeoutHandle;-1!==c&&(a.timeoutHandle=-1,vj(c));if(null!==t)for(c=t.return;null!==c;){var d=c;switch(d.tag){case 1:d=d.type.childContextTypes;null!==d&&void 0!==d&&(q(G),q(B));break;case 3:tb();q(G);q(B);break;case 5:te(d);break;case 4:tb();break;case 13:q(D);break;case 19:q(D);break;case 10:me(d)}c=c.return}U=a;t=Sa(a.current,null);
P=b;F=Xa;id=null;Yb=ta=1073741823;kd=null;Xb=0;jd=!1}function Nh(a,b){do{try{le();Sc.current=Tc;if(Uc)for(var c=z.memoizedState;null!==c;){var d=c.queue;null!==d&&(d.pending=null);c=c.next}Ia=0;J=K=z=null;Uc=!1;if(null===t||null===t.return)return F=hd,id=b,t=null;a:{var e=a,f=t.return,g=t,h=b;b=P;g.effectTag|=2048;g.firstEffect=g.lastEffect=null;if(null!==h&&"object"===typeof h&&"function"===typeof h.then){var m=h;if(0===(g.mode&2)){var n=g.alternate;n?(g.updateQueue=n.updateQueue,g.memoizedState=
n.memoizedState,g.expirationTime=n.expirationTime):(g.updateQueue=null,g.memoizedState=null)}var l=0!==(D.current&1),k=f;do{var p;if(p=13===k.tag){var q=k.memoizedState;if(null!==q)p=null!==q.dehydrated?!0:!1;else{var w=k.memoizedProps;p=void 0===w.fallback?!1:!0!==w.unstable_avoidThisFallback?!0:l?!1:!0}}if(p){var y=k.updateQueue;if(null===y){var r=new Set;r.add(m);k.updateQueue=r}else y.add(m);if(0===(k.mode&2)){k.effectTag|=64;g.effectTag&=-2981;if(1===g.tag)if(null===g.alternate)g.tag=17;else{var O=
Ea(1073741823,null);O.tag=Jc;Fa(g,O)}g.expirationTime=1073741823;break a}h=void 0;g=b;var v=e.pingCache;null===v?(v=e.pingCache=new wj,h=new Set,v.set(m,h)):(h=v.get(m),void 0===h&&(h=new Set,v.set(m,h)));if(!h.has(g)){h.add(g);var x=xj.bind(null,e,m,g);m.then(x,x)}k.effectTag|=4096;k.expirationTime=b;break a}k=k.return}while(null!==k);h=Error((na(g.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+
Bd(g))}F!==Xe&&(F=Oh);h=Le(h,g);k=f;do{switch(k.tag){case 3:m=h;k.effectTag|=4096;k.expirationTime=b;var A=Ih(k,m,b);Ug(k,A);break a;case 1:m=h;var u=k.type,B=k.stateNode;if(0===(k.effectTag&64)&&("function"===typeof u.getDerivedStateFromError||null!==B&&"function"===typeof B.componentDidCatch&&(null===La||!La.has(B)))){k.effectTag|=4096;k.expirationTime=b;var H=Jh(k,m,b);Ug(k,H);break a}}k=k.return}while(null!==k)}t=Sh(t)}catch(cj){b=cj;continue}break}while(1)}function Mh(a){a=gd.current;gd.current=
Tc;return null===a?Tc:a}function Vg(a,b){a<ta&&2<a&&(ta=a);null!==b&&a<Yb&&2<a&&(Yb=a,kd=b)}function Kc(a){a>Xb&&(Xb=a)}function tj(){for(;null!==t;)t=Th(t)}function rj(){for(;null!==t&&!yj();)t=Th(t)}function Th(a){var b=zj(a.alternate,a,P);a.memoizedProps=a.pendingProps;null===b&&(b=Sh(a));Uh.current=null;return b}function Sh(a){t=a;do{var b=t.alternate;a=t.return;if(0===(t.effectTag&2048)){b=hj(b,t,P);if(1===P||1!==t.childExpirationTime){for(var c=0,d=t.child;null!==d;){var e=d.expirationTime,
f=d.childExpirationTime;e>c&&(c=e);f>c&&(c=f);d=d.sibling}t.childExpirationTime=c}if(null!==b)return b;null!==a&&0===(a.effectTag&2048)&&(null===a.firstEffect&&(a.firstEffect=t.firstEffect),null!==t.lastEffect&&(null!==a.lastEffect&&(a.lastEffect.nextEffect=t.firstEffect),a.lastEffect=t.lastEffect),1<t.effectTag&&(null!==a.lastEffect?a.lastEffect.nextEffect=t:a.firstEffect=t,a.lastEffect=t))}else{b=lj(t);if(null!==b)return b.effectTag&=2047,b;null!==a&&(a.firstEffect=a.lastEffect=null,a.effectTag|=
2048)}b=t.sibling;if(null!==b)return b;t=a}while(null!==t);F===Xa&&(F=Xe);return null}function Ve(a){var b=a.expirationTime;a=a.childExpirationTime;return b>a?b:a}function ab(a){var b=Cc();Da(99,Aj.bind(null,a,b));return null}function Aj(a,b){do xb();while(null!==Zb);if((p&(ca|ma))!==H)throw Error(k(327));var c=a.finishedWork,d=a.finishedExpirationTime;if(null===c)return null;a.finishedWork=null;a.finishedExpirationTime=0;if(c===a.current)throw Error(k(177));a.callbackNode=null;a.callbackExpirationTime=
0;a.callbackPriority=90;a.nextKnownPendingLevel=0;var e=Ve(c);a.firstPendingTime=e;d<=a.lastSuspendedTime?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:d<=a.firstSuspendedTime&&(a.firstSuspendedTime=d-1);d<=a.lastPingedTime&&(a.lastPingedTime=0);d<=a.lastExpiredTime&&(a.lastExpiredTime=0);a===U&&(t=U=null,P=0);1<c.effectTag?null!==c.lastEffect?(c.lastEffect.nextEffect=c,e=c.firstEffect):e=c:e=c.firstEffect;if(null!==e){var f=p;p|=ma;Uh.current=null;Ze=tc;var g=kg();if(Xd(g)){if("selectionStart"in
g)var h={start:g.selectionStart,end:g.selectionEnd};else a:{h=(h=g.ownerDocument)&&h.defaultView||window;var m=h.getSelection&&h.getSelection();if(m&&0!==m.rangeCount){h=m.anchorNode;var n=m.anchorOffset,q=m.focusNode;m=m.focusOffset;try{h.nodeType,q.nodeType}catch(sb){h=null;break a}var ba=0,w=-1,y=-1,B=0,D=0,r=g,z=null;b:for(;;){for(var v;;){r!==h||0!==n&&3!==r.nodeType||(w=ba+n);r!==q||0!==m&&3!==r.nodeType||(y=ba+m);3===r.nodeType&&(ba+=r.nodeValue.length);if(null===(v=r.firstChild))break;z=r;
r=v}for(;;){if(r===g)break b;z===h&&++B===n&&(w=ba);z===q&&++D===m&&(y=ba);if(null!==(v=r.nextSibling))break;r=z;z=r.parentNode}r=v}h=-1===w||-1===y?null:{start:w,end:y}}else h=null}h=h||{start:0,end:0}}else h=null;$e={activeElementDetached:null,focusedElem:g,selectionRange:h};tc=!1;l=e;do try{Bj()}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=e;do try{for(g=a,h=b;null!==l;){var x=l.effectTag;x&16&&Wb(l.stateNode,"");if(x&128){var A=l.alternate;if(null!==A){var u=
A.ref;null!==u&&("function"===typeof u?u(null):u.current=null)}}switch(x&1038){case 2:Gh(l);l.effectTag&=-3;break;case 6:Gh(l);l.effectTag&=-3;Qe(l.alternate,l);break;case 1024:l.effectTag&=-1025;break;case 1028:l.effectTag&=-1025;Qe(l.alternate,l);break;case 4:Qe(l.alternate,l);break;case 8:n=l,Dh(g,n,h),Eh(n)}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);u=$e;A=kg();x=u.focusedElem;h=u.selectionRange;if(A!==x&&x&&x.ownerDocument&&jg(x.ownerDocument.documentElement,
x)){null!==h&&Xd(x)&&(A=h.start,u=h.end,void 0===u&&(u=A),"selectionStart"in x?(x.selectionStart=A,x.selectionEnd=Math.min(u,x.value.length)):(u=(A=x.ownerDocument||document)&&A.defaultView||window,u.getSelection&&(u=u.getSelection(),n=x.textContent.length,g=Math.min(h.start,n),h=void 0===h.end?g:Math.min(h.end,n),!u.extend&&g>h&&(n=h,h=g,g=n),n=ig(x,g),q=ig(x,h),n&&q&&(1!==u.rangeCount||u.anchorNode!==n.node||u.anchorOffset!==n.offset||u.focusNode!==q.node||u.focusOffset!==q.offset)&&(A=A.createRange(),
A.setStart(n.node,n.offset),u.removeAllRanges(),g>h?(u.addRange(A),u.extend(q.node,q.offset)):(A.setEnd(q.node,q.offset),u.addRange(A))))));A=[];for(u=x;u=u.parentNode;)1===u.nodeType&&A.push({element:u,left:u.scrollLeft,top:u.scrollTop});"function"===typeof x.focus&&x.focus();for(x=0;x<A.length;x++)u=A[x],u.element.scrollLeft=u.left,u.element.scrollTop=u.top}tc=!!Ze;$e=Ze=null;a.current=c;l=e;do try{for(x=a;null!==l;){var F=l.effectTag;F&36&&oj(x,l.alternate,l);if(F&128){A=void 0;var E=l.ref;if(null!==
E){var G=l.stateNode;switch(l.tag){case 5:A=G;break;default:A=G}"function"===typeof E?E(A):E.current=A}}l=l.nextEffect}}catch(sb){if(null===l)throw Error(k(330));Za(l,sb);l=l.nextEffect}while(null!==l);l=null;Cj();p=f}else a.current=c;if(ld)ld=!1,Zb=a,$b=b;else for(l=e;null!==l;)b=l.nextEffect,l.nextEffect=null,l=b;b=a.firstPendingTime;0===b&&(La=null);1073741823===b?a===af?ac++:(ac=0,af=a):ac=0;"function"===typeof bf&&bf(c.stateNode,d);V(a);if(cd)throw cd=!1,a=Se,Se=null,a;if((p&Ye)!==H)return null;
ha();return null}function Bj(){for(;null!==l;){var a=l.effectTag;0!==(a&256)&&nj(l.alternate,l);0===(a&512)||ld||(ld=!0,Ng(97,function(){xb();return null}));l=l.nextEffect}}function xb(){if(90!==$b){var a=97<$b?97:$b;$b=90;return Da(a,Dj)}}function Dj(){if(null===Zb)return!1;var a=Zb;Zb=null;if((p&(ca|ma))!==H)throw Error(k(331));var b=p;p|=ma;for(a=a.current.firstEffect;null!==a;){try{var c=a;if(0!==(c.effectTag&512))switch(c.tag){case 0:case 11:case 15:case 22:Ah(5,c),Bh(5,c)}}catch(d){if(null===
a)throw Error(k(330));Za(a,d)}c=a.nextEffect;a.nextEffect=null;a=c}p=b;ha();return!0}function Vh(a,b,c){b=Le(c,b);b=Ih(a,b,1073741823);Fa(a,b);a=ed(a,1073741823);null!==a&&V(a)}function Za(a,b){if(3===a.tag)Vh(a,a,b);else for(var c=a.return;null!==c;){if(3===c.tag){Vh(c,a,b);break}else if(1===c.tag){var d=c.stateNode;if("function"===typeof c.type.getDerivedStateFromError||"function"===typeof d.componentDidCatch&&(null===La||!La.has(d))){a=Le(b,a);a=Jh(c,a,1073741823);Fa(c,a);c=ed(c,1073741823);null!==
c&&V(c);break}}c=c.return}}function xj(a,b,c){var d=a.pingCache;null!==d&&d.delete(b);U===a&&P===c?F===bd||F===ad&&1073741823===ta&&Y()-Re<Ph?$a(a,P):jd=!0:Kh(a,c)&&(b=a.lastPingedTime,0!==b&&b<c||(a.lastPingedTime=c,V(a)))}function qj(a,b){var c=a.stateNode;null!==c&&c.delete(b);b=0;0===b&&(b=ka(),b=Va(b,a,null));a=ed(a,b);null!==a&&V(a)}function Ej(a){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var b=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(b.isDisabled||!b.supportsFiber)return!0;try{var c=
b.inject(a);bf=function(a,e){try{b.onCommitFiberRoot(c,a,void 0,64===(a.current.effectTag&64))}catch(f){}};Ne=function(a){try{b.onCommitFiberUnmount(c,a)}catch(e){}}}catch(d){}return!0}function Fj(a,b,c,d){this.tag=a;this.key=c;this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null;this.index=0;this.ref=null;this.pendingProps=b;this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null;this.mode=d;this.effectTag=0;this.lastEffect=this.firstEffect=this.nextEffect=
null;this.childExpirationTime=this.expirationTime=0;this.alternate=null}function Ge(a){a=a.prototype;return!(!a||!a.isReactComponent)}function Gj(a){if("function"===typeof a)return Ge(a)?1:0;if(void 0!==a&&null!==a){a=a.$$typeof;if(a===zd)return 11;if(a===Ad)return 14}return 2}function Sa(a,b){var c=a.alternate;null===c?(c=la(a.tag,b,a.key,a.mode),c.elementType=a.elementType,c.type=a.type,c.stateNode=a.stateNode,c.alternate=a,a.alternate=c):(c.pendingProps=b,c.effectTag=0,c.nextEffect=null,c.firstEffect=
null,c.lastEffect=null);c.childExpirationTime=a.childExpirationTime;c.expirationTime=a.expirationTime;c.child=a.child;c.memoizedProps=a.memoizedProps;c.memoizedState=a.memoizedState;c.updateQueue=a.updateQueue;b=a.dependencies;c.dependencies=null===b?null:{expirationTime:b.expirationTime,firstContext:b.firstContext,responders:b.responders};c.sibling=a.sibling;c.index=a.index;c.ref=a.ref;return c}function Oc(a,b,c,d,e,f){var g=2;d=a;if("function"===typeof a)Ge(a)&&(g=1);else if("string"===typeof a)g=
5;else a:switch(a){case Ma:return Ha(c.children,e,f,b);case Hj:g=8;e|=7;break;case Af:g=8;e|=1;break;case kc:return a=la(12,c,b,e|8),a.elementType=kc,a.type=kc,a.expirationTime=f,a;case lc:return a=la(13,c,b,e),a.type=lc,a.elementType=lc,a.expirationTime=f,a;case yd:return a=la(19,c,b,e),a.elementType=yd,a.expirationTime=f,a;default:if("object"===typeof a&&null!==a)switch(a.$$typeof){case Cf:g=10;break a;case Bf:g=9;break a;case zd:g=11;break a;case Ad:g=14;break a;case Ef:g=16;d=null;break a;case Df:g=
22;break a}throw Error(k(130,null==a?a:typeof a,""));}b=la(g,c,b,e);b.elementType=a;b.type=d;b.expirationTime=f;return b}function Ha(a,b,c,d){a=la(7,a,d,b);a.expirationTime=c;return a}function qe(a,b,c){a=la(6,a,null,b);a.expirationTime=c;return a}function re(a,b,c){b=la(4,null!==a.children?a.children:[],a.key,b);b.expirationTime=c;b.stateNode={containerInfo:a.containerInfo,pendingChildren:null,implementation:a.implementation};return b}function Ij(a,b,c){this.tag=b;this.current=null;this.containerInfo=
a;this.pingCache=this.pendingChildren=null;this.finishedExpirationTime=0;this.finishedWork=null;this.timeoutHandle=-1;this.pendingContext=this.context=null;this.hydrate=c;this.callbackNode=null;this.callbackPriority=90;this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Kh(a,b){var c=a.firstSuspendedTime;a=a.lastSuspendedTime;return 0!==c&&c>=b&&a<=b}function Ya(a,b){var c=a.firstSuspendedTime,d=a.lastSuspendedTime;
c<b&&(a.firstSuspendedTime=b);if(d>b||0===c)a.lastSuspendedTime=b;b<=a.lastPingedTime&&(a.lastPingedTime=0);b<=a.lastExpiredTime&&(a.lastExpiredTime=0)}function yh(a,b){b>a.firstPendingTime&&(a.firstPendingTime=b);var c=a.firstSuspendedTime;0!==c&&(b>=c?a.firstSuspendedTime=a.lastSuspendedTime=a.nextKnownPendingLevel=0:b>=a.lastSuspendedTime&&(a.lastSuspendedTime=b+1),b>a.nextKnownPendingLevel&&(a.nextKnownPendingLevel=b))}function Ue(a,b){var c=a.lastExpiredTime;if(0===c||c>b)a.lastExpiredTime=b}
function md(a,b,c,d){var e=b.current,f=ka(),g=Vb.suspense;f=Va(f,e,g);a:if(c){c=c._reactInternalFiber;b:{if(Na(c)!==c||1!==c.tag)throw Error(k(170));var h=c;do{switch(h.tag){case 3:h=h.stateNode.context;break b;case 1:if(N(h.type)){h=h.stateNode.__reactInternalMemoizedMergedChildContext;break b}}h=h.return}while(null!==h);throw Error(k(171));}if(1===c.tag){var m=c.type;if(N(m)){c=Gg(c,m,h);break a}}c=h}else c=Ca;null===b.context?b.context=c:b.pendingContext=c;b=Ea(f,g);b.payload={element:a};d=void 0===
d?null:d;null!==d&&(b.callback=d);Fa(e,b);Ja(e,f);return f}function cf(a){a=a.current;if(!a.child)return null;switch(a.child.tag){case 5:return a.child.stateNode;default:return a.child.stateNode}}function Wh(a,b){a=a.memoizedState;null!==a&&null!==a.dehydrated&&a.retryTime<b&&(a.retryTime=b)}function df(a,b){Wh(a,b);(a=a.alternate)&&Wh(a,b)}function ef(a,b,c){c=null!=c&&!0===c.hydrate;var d=new Ij(a,b,c),e=la(3,null,null,2===b?7:1===b?3:0);d.current=e;e.stateNode=d;ne(e);a[Lb]=d.current;c&&0!==b&&
xi(a,9===a.nodeType?a:a.ownerDocument);this._internalRoot=d}function bc(a){return!(!a||1!==a.nodeType&&9!==a.nodeType&&11!==a.nodeType&&(8!==a.nodeType||" react-mount-point-unstable "!==a.nodeValue))}function Jj(a,b){b||(b=a?9===a.nodeType?a.documentElement:a.firstChild:null,b=!(!b||1!==b.nodeType||!b.hasAttribute("data-reactroot")));if(!b)for(var c;c=a.lastChild;)a.removeChild(c);return new ef(a,0,b?{hydrate:!0}:void 0)}function nd(a,b,c,d,e){var f=c._reactRootContainer;if(f){var g=f._internalRoot;
if("function"===typeof e){var h=e;e=function(){var a=cf(g);h.call(a)}}md(b,g,a,e)}else{f=c._reactRootContainer=Jj(c,d);g=f._internalRoot;if("function"===typeof e){var m=e;e=function(){var a=cf(g);m.call(a)}}Rh(function(){md(b,g,a,e)})}return cf(g)}function Kj(a,b,c){var d=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:gb,key:null==d?null:""+d,children:a,containerInfo:b,implementation:c}}function Xh(a,b){var c=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;
if(!bc(b))throw Error(k(200));return Kj(a,b,null,c)}if(!ea)throw Error(k(227));var ki=function(a,b,c,d,e,f,g,h,m){var n=Array.prototype.slice.call(arguments,3);try{b.apply(c,n)}catch(C){this.onError(C)}},yb=!1,gc=null,hc=!1,pd=null,li={onError:function(a){yb=!0;gc=a}},td=null,rf=null,mf=null,ic=null,cb={},jc=[],qd={},db={},rd={},wa=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),M=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.assign,
sd=null,eb=null,fb=null,ee=function(a,b){return a(b)},eg=function(a,b,c,d,e){return a(b,c,d,e)},vd=function(){},vf=ee,Oa=!1,wd=!1,Z=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.Scheduler,Lj=Z.unstable_cancelCallback,ff=Z.unstable_now,$f=Z.unstable_scheduleCallback,Mj=Z.unstable_shouldYield,Yh=Z.unstable_requestPaint,Pd=Z.unstable_runWithPriority,Nj=Z.unstable_getCurrentPriorityLevel,Oj=Z.unstable_ImmediatePriority,Zh=Z.unstable_UserBlockingPriority,ag=Z.unstable_NormalPriority,Pj=Z.unstable_LowPriority,
Qj=Z.unstable_IdlePriority,oi=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,wf=Object.prototype.hasOwnProperty,yf={},xf={},E={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(a){E[a]=
new L(a,0,!1,a,null,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(a){var b=a[0];E[b]=new L(b,1,!1,a[1],null,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(a){E[a]=new L(a,2,!1,a.toLowerCase(),null,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(a){E[a]=new L(a,2,!1,a,null,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(a){E[a]=
new L(a,3,!1,a.toLowerCase(),null,!1)});["checked","multiple","muted","selected"].forEach(function(a){E[a]=new L(a,3,!0,a,null,!1)});["capture","download"].forEach(function(a){E[a]=new L(a,4,!1,a,null,!1)});["cols","rows","size","span"].forEach(function(a){E[a]=new L(a,6,!1,a,null,!1)});["rowSpan","start"].forEach(function(a){E[a]=new L(a,5,!1,a.toLowerCase(),null,!1)});var gf=/[\-:]([a-z])/g,hf=function(a){return a[1].toUpperCase()};"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(a){var b=
a.replace(gf,hf);E[b]=new L(b,1,!1,a,null,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/1999/xlink",!1)});["xml:base","xml:lang","xml:space"].forEach(function(a){var b=a.replace(gf,hf);E[b]=new L(b,1,!1,a,"http://www.w3.org/XML/1998/namespace",!1)});["tabIndex","crossOrigin"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!1)});E.xlinkHref=new L("xlinkHref",1,
!1,"xlink:href","http://www.w3.org/1999/xlink",!0);["src","href","action","formAction"].forEach(function(a){E[a]=new L(a,1,!1,a.toLowerCase(),null,!0)});var da=ea.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;da.hasOwnProperty("ReactCurrentDispatcher")||(da.ReactCurrentDispatcher={current:null});da.hasOwnProperty("ReactCurrentBatchConfig")||(da.ReactCurrentBatchConfig={suspense:null});var si=/^(.*)[\\\/]/,Q="function"===typeof Symbol&&Symbol.for,Pc=Q?Symbol.for("react.element"):60103,gb=Q?Symbol.for("react.portal"):
60106,Ma=Q?Symbol.for("react.fragment"):60107,Af=Q?Symbol.for("react.strict_mode"):60108,kc=Q?Symbol.for("react.profiler"):60114,Cf=Q?Symbol.for("react.provider"):60109,Bf=Q?Symbol.for("react.context"):60110,Hj=Q?Symbol.for("react.concurrent_mode"):60111,zd=Q?Symbol.for("react.forward_ref"):60112,lc=Q?Symbol.for("react.suspense"):60113,yd=Q?Symbol.for("react.suspense_list"):60120,Ad=Q?Symbol.for("react.memo"):60115,Ef=Q?Symbol.for("react.lazy"):60116,Df=Q?Symbol.for("react.block"):60121,zf="function"===
typeof Symbol&&Symbol.iterator,od,xh=function(a){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(b,c,d,e){MSApp.execUnsafeLocalFunction(function(){return a(b,c,d,e)})}:a}(function(a,b){if("http://www.w3.org/2000/svg"!==a.namespaceURI||"innerHTML"in a)a.innerHTML=b;else{od=od||document.createElement("div");od.innerHTML="<svg>"+b.valueOf().toString()+"</svg>";for(b=od.firstChild;a.firstChild;)a.removeChild(a.firstChild);for(;b.firstChild;)a.appendChild(b.firstChild)}}),Wb=function(a,
b){if(b){var c=a.firstChild;if(c&&c===a.lastChild&&3===c.nodeType){c.nodeValue=b;return}}a.textContent=b},ib={animationend:nc("Animation","AnimationEnd"),animationiteration:nc("Animation","AnimationIteration"),animationstart:nc("Animation","AnimationStart"),transitionend:nc("Transition","TransitionEnd")},Id={},Of={};wa&&(Of=document.createElement("div").style,"AnimationEvent"in window||(delete ib.animationend.animation,delete ib.animationiteration.animation,delete ib.animationstart.animation),"TransitionEvent"in
window||delete ib.transitionend.transition);var $h=oc("animationend"),ai=oc("animationiteration"),bi=oc("animationstart"),ci=oc("transitionend"),Db="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Pf=new ("function"===typeof WeakMap?WeakMap:Map),Ab=null,wi=function(a){if(a){var b=a._dispatchListeners,c=a._dispatchInstances;
if(Array.isArray(b))for(var d=0;d<b.length&&!a.isPropagationStopped();d++)lf(a,b[d],c[d]);else b&&lf(a,b,c);a._dispatchListeners=null;a._dispatchInstances=null;a.isPersistent()||a.constructor.release(a)}},qc=[],Rd=!1,fa=[],xa=null,ya=null,za=null,Eb=new Map,Fb=new Map,Jb=[],Nd="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),
yi="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" "),dg={},cg=new Map,Td=new Map,Rj=["abort","abort",$h,"animationEnd",ai,"animationIteration",bi,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata",
"loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",ci,"transitionEnd","waiting","waiting"];Sd("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),
0);Sd("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1);Sd(Rj,2);(function(a,b){for(var c=0;c<a.length;c++)Td.set(a[c],b)})("change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),0);var Hi=Zh,Gi=Pd,tc=!0,Kb={animationIterationCount:!0,
borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,
strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Sj=["Webkit","ms","Moz","O"];Object.keys(Kb).forEach(function(a){Sj.forEach(function(b){b=b+a.charAt(0).toUpperCase()+a.substring(1);Kb[b]=Kb[a]})});var Ii=M({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0}),ng="$",og="/$",$d="$?",Zd="$!",Ze=null,$e=null,We="function"===typeof setTimeout?setTimeout:void 0,vj="function"===
typeof clearTimeout?clearTimeout:void 0,jf=Math.random().toString(36).slice(2),Aa="__reactInternalInstance$"+jf,vc="__reactEventHandlers$"+jf,Lb="__reactContainere$"+jf,Ba=null,ce=null,wc=null;M(R.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():"unknown"!==typeof a.returnValue&&(a.returnValue=!1),this.isDefaultPrevented=xc)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():"unknown"!==
typeof a.cancelBubble&&(a.cancelBubble=!0),this.isPropagationStopped=xc)},persist:function(){this.isPersistent=xc},isPersistent:yc,destructor:function(){var a=this.constructor.Interface,b;for(b in a)this[b]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null;this.isPropagationStopped=this.isDefaultPrevented=yc;this._dispatchInstances=this._dispatchListeners=null}});R.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(a){return a.timeStamp||
Date.now()},defaultPrevented:null,isTrusted:null};R.extend=function(a){function b(){return c.apply(this,arguments)}var c=this,d=function(){};d.prototype=c.prototype;d=new d;M(d,b.prototype);b.prototype=d;b.prototype.constructor=b;b.Interface=M({},c.Interface,a);b.extend=c.extend;sg(b);return b};sg(R);var Tj=R.extend({data:null}),Uj=R.extend({data:null}),Ni=[9,13,27,32],de=wa&&"CompositionEvent"in window,cc=null;wa&&"documentMode"in document&&(cc=document.documentMode);var Vj=wa&&"TextEvent"in window&&
!cc,xg=wa&&(!de||cc&&8<cc&&11>=cc),wg=String.fromCharCode(32),ua={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},
dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},vg=!1,mb=!1,Wj={eventTypes:ua,extractEvents:function(a,b,c,d,e){var f;if(de)b:{switch(a){case "compositionstart":var g=ua.compositionStart;break b;case "compositionend":g=ua.compositionEnd;break b;case "compositionupdate":g=
ua.compositionUpdate;break b}g=void 0}else mb?tg(a,c)&&(g=ua.compositionEnd):"keydown"===a&&229===c.keyCode&&(g=ua.compositionStart);g?(xg&&"ko"!==c.locale&&(mb||g!==ua.compositionStart?g===ua.compositionEnd&&mb&&(f=rg()):(Ba=d,ce="value"in Ba?Ba.value:Ba.textContent,mb=!0)),e=Tj.getPooled(g,b,c,d),f?e.data=f:(f=ug(c),null!==f&&(e.data=f)),lb(e),f=e):f=null;(a=Vj?Oi(a,c):Pi(a,c))?(b=Uj.getPooled(ua.beforeInput,b,c,d),b.data=a,lb(b)):b=null;return null===f?b:null===b?f:[f,b]}},Qi={color:!0,date:!0,
datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0},Ag={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}},Mb=null,Nb=null,kf=!1;wa&&(kf=Tf("input")&&(!document.documentMode||9<document.documentMode));var Xj={eventTypes:Ag,_isInputEventSupported:kf,extractEvents:function(a,b,c,d,e){e=b?Pa(b):window;var f=
e.nodeName&&e.nodeName.toLowerCase();if("select"===f||"input"===f&&"file"===e.type)var g=Si;else if(yg(e))if(kf)g=Wi;else{g=Ui;var h=Ti}else(f=e.nodeName)&&"input"===f.toLowerCase()&&("checkbox"===e.type||"radio"===e.type)&&(g=Vi);if(g&&(g=g(a,b)))return zg(g,c,d);h&&h(a,e,b);"blur"===a&&(a=e._wrapperState)&&a.controlled&&"number"===e.type&&Ed(e,"number",e.value)}},dc=R.extend({view:null,detail:null}),Yi={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"},di=0,ei=0,fi=!1,gi=!1,ec=dc.extend({screenX:null,
screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:fe,button:null,buttons:null,relatedTarget:function(a){return a.relatedTarget||(a.fromElement===a.srcElement?a.toElement:a.fromElement)},movementX:function(a){if("movementX"in a)return a.movementX;var b=di;di=a.screenX;return fi?"mousemove"===a.type?a.screenX-b:0:(fi=!0,0)},movementY:function(a){if("movementY"in a)return a.movementY;var b=ei;ei=a.screenY;return gi?"mousemove"===
a.type?a.screenY-b:0:(gi=!0,0)}}),hi=ec.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),fc={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout",
"pointerover"]}},Yj={eventTypes:fc,extractEvents:function(a,b,c,d,e){var f="mouseover"===a||"pointerover"===a,g="mouseout"===a||"pointerout"===a;if(f&&0===(e&32)&&(c.relatedTarget||c.fromElement)||!g&&!f)return null;f=d.window===d?d:(f=d.ownerDocument)?f.defaultView||f.parentWindow:window;if(g){if(g=b,b=(b=c.relatedTarget||c.toElement)?Bb(b):null,null!==b){var h=Na(b);if(b!==h||5!==b.tag&&6!==b.tag)b=null}}else g=null;if(g===b)return null;if("mouseout"===a||"mouseover"===a){var m=ec;var n=fc.mouseLeave;
var l=fc.mouseEnter;var k="mouse"}else if("pointerout"===a||"pointerover"===a)m=hi,n=fc.pointerLeave,l=fc.pointerEnter,k="pointer";a=null==g?f:Pa(g);f=null==b?f:Pa(b);n=m.getPooled(n,g,c,d);n.type=k+"leave";n.target=a;n.relatedTarget=f;c=m.getPooled(l,b,c,d);c.type=k+"enter";c.target=f;c.relatedTarget=a;d=g;k=b;if(d&&k)a:{m=d;l=k;g=0;for(a=m;a;a=pa(a))g++;a=0;for(b=l;b;b=pa(b))a++;for(;0<g-a;)m=pa(m),g--;for(;0<a-g;)l=pa(l),a--;for(;g--;){if(m===l||m===l.alternate)break a;m=pa(m);l=pa(l)}m=null}else m=
null;l=m;for(m=[];d&&d!==l;){g=d.alternate;if(null!==g&&g===l)break;m.push(d);d=pa(d)}for(d=[];k&&k!==l;){g=k.alternate;if(null!==g&&g===l)break;d.push(k);k=pa(k)}for(k=0;k<m.length;k++)be(m[k],"bubbled",n);for(k=d.length;0<k--;)be(d[k],"captured",c);return 0===(e&64)?[n]:[n,c]}},Qa="function"===typeof Object.is?Object.is:Zi,$i=Object.prototype.hasOwnProperty,Zj=wa&&"documentMode"in document&&11>=document.documentMode,Eg={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},
dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},nb=null,he=null,Pb=null,ge=!1,ak={eventTypes:Eg,extractEvents:function(a,b,c,d,e,f){e=f||(d.window===d?d.document:9===d.nodeType?d:d.ownerDocument);if(!(f=!e)){a:{e=Jd(e);f=rd.onSelect;for(var g=0;g<f.length;g++)if(!e.has(f[g])){e=!1;break a}e=!0}f=!e}if(f)return null;e=b?Pa(b):window;switch(a){case "focus":if(yg(e)||"true"===e.contentEditable)nb=e,he=b,Pb=null;break;case "blur":Pb=he=nb=null;
break;case "mousedown":ge=!0;break;case "contextmenu":case "mouseup":case "dragend":return ge=!1,Dg(c,d);case "selectionchange":if(Zj)break;case "keydown":case "keyup":return Dg(c,d)}return null}},bk=R.extend({animationName:null,elapsedTime:null,pseudoElement:null}),ck=R.extend({clipboardData:function(a){return"clipboardData"in a?a.clipboardData:window.clipboardData}}),dk=dc.extend({relatedTarget:null}),ek={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",
Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},fk={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",
224:"Meta"},gk=dc.extend({key:function(a){if(a.key){var b=ek[a.key]||a.key;if("Unidentified"!==b)return b}return"keypress"===a.type?(a=Ac(a),13===a?"Enter":String.fromCharCode(a)):"keydown"===a.type||"keyup"===a.type?fk[a.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:fe,charCode:function(a){return"keypress"===a.type?Ac(a):0},keyCode:function(a){return"keydown"===a.type||"keyup"===a.type?a.keyCode:0},which:function(a){return"keypress"===
a.type?Ac(a):"keydown"===a.type||"keyup"===a.type?a.keyCode:0}}),hk=ec.extend({dataTransfer:null}),ik=dc.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:fe}),jk=R.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),kk=ec.extend({deltaX:function(a){return"deltaX"in a?a.deltaX:"wheelDeltaX"in a?-a.wheelDeltaX:0},deltaY:function(a){return"deltaY"in a?a.deltaY:"wheelDeltaY"in a?-a.wheelDeltaY:"wheelDelta"in a?
-a.wheelDelta:0},deltaZ:null,deltaMode:null}),lk={eventTypes:dg,extractEvents:function(a,b,c,d,e){e=cg.get(a);if(!e)return null;switch(a){case "keypress":if(0===Ac(c))return null;case "keydown":case "keyup":a=gk;break;case "blur":case "focus":a=dk;break;case "click":if(2===c.button)return null;case "auxclick":case "dblclick":case "mousedown":case "mousemove":case "mouseup":case "mouseout":case "mouseover":case "contextmenu":a=ec;break;case "drag":case "dragend":case "dragenter":case "dragexit":case "dragleave":case "dragover":case "dragstart":case "drop":a=
hk;break;case "touchcancel":case "touchend":case "touchmove":case "touchstart":a=ik;break;case $h:case ai:case bi:a=bk;break;case ci:a=jk;break;case "scroll":a=dc;break;case "wheel":a=kk;break;case "copy":case "cut":case "paste":a=ck;break;case "gotpointercapture":case "lostpointercapture":case "pointercancel":case "pointerdown":case "pointermove":case "pointerout":case "pointerover":case "pointerup":a=hi;break;default:a=R}b=a.getPooled(e,b,c,d);lb(b);return b}};(function(a){if(ic)throw Error(k(101));
ic=Array.prototype.slice.call(a);nf()})("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" "));(function(a,b,c){td=a;rf=b;mf=c})(ae,Hb,Pa);pf({SimpleEventPlugin:lk,EnterLeaveEventPlugin:Yj,ChangeEventPlugin:Xj,SelectEventPlugin:ak,BeforeInputEventPlugin:Wj});var ie=[],ob=-1,Ca={},B={current:Ca},G={current:!1},Ra=Ca,bj=Pd,je=$f,Rg=Lj,aj=Nj,Dc=Oj,Ig=Zh,Jg=ag,Kg=Pj,Lg=Qj,Qg={},yj=Mj,Cj=void 0!==Yh?Yh:function(){},qa=null,
Ec=null,ke=!1,ii=ff(),Y=1E4>ii?ff:function(){return ff()-ii},Ic={current:null},Hc=null,qb=null,Gc=null,Tg=0,Jc=2,Ga=!1,Vb=da.ReactCurrentBatchConfig,$g=(new ea.Component).refs,Mc={isMounted:function(a){return(a=a._reactInternalFiber)?Na(a)===a:!1},enqueueSetState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;d=Va(d,a,e);e=Ea(d,e);e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueReplaceState:function(a,b,c){a=a._reactInternalFiber;var d=ka(),e=Vb.suspense;
d=Va(d,a,e);e=Ea(d,e);e.tag=1;e.payload=b;void 0!==c&&null!==c&&(e.callback=c);Fa(a,e);Ja(a,d)},enqueueForceUpdate:function(a,b){a=a._reactInternalFiber;var c=ka(),d=Vb.suspense;c=Va(c,a,d);d=Ea(c,d);d.tag=Jc;void 0!==b&&null!==b&&(d.callback=b);Fa(a,d);Ja(a,c)}},Qc=Array.isArray,wb=ah(!0),Fe=ah(!1),Sb={},ja={current:Sb},Ub={current:Sb},Tb={current:Sb},D={current:0},Sc=da.ReactCurrentDispatcher,X=da.ReactCurrentBatchConfig,Ia=0,z=null,K=null,J=null,Uc=!1,Tc={readContext:W,useCallback:S,useContext:S,
useEffect:S,useImperativeHandle:S,useLayoutEffect:S,useMemo:S,useReducer:S,useRef:S,useState:S,useDebugValue:S,useResponder:S,useDeferredValue:S,useTransition:S},dj={readContext:W,useCallback:ih,useContext:W,useEffect:eh,useImperativeHandle:function(a,b,c){c=null!==c&&void 0!==c?c.concat([a]):null;return ze(4,2,gh.bind(null,b,a),c)},useLayoutEffect:function(a,b){return ze(4,2,a,b)},useMemo:function(a,b){var c=ub();b=void 0===b?null:b;a=a();c.memoizedState=[a,b];return a},useReducer:function(a,b,c){var d=
ub();b=void 0!==c?c(b):b;d.memoizedState=d.baseState=b;a=d.queue={pending:null,dispatch:null,lastRenderedReducer:a,lastRenderedState:b};a=a.dispatch=ch.bind(null,z,a);return[d.memoizedState,a]},useRef:function(a){var b=ub();a={current:a};return b.memoizedState=a},useState:xe,useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=xe(a),d=c[0],e=c[1];eh(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=
xe(!1),c=b[0];b=b[1];return[ih(Ce.bind(null,b,a),[b,a]),c]}},ej={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Vc,useRef:dh,useState:function(a){return Vc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Vc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Vc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,
b,a),[b,a]),c]}},fj={readContext:W,useCallback:Yc,useContext:W,useEffect:Xc,useImperativeHandle:hh,useLayoutEffect:fh,useMemo:jh,useReducer:Wc,useRef:dh,useState:function(a){return Wc(Ua)},useDebugValue:Be,useResponder:ue,useDeferredValue:function(a,b){var c=Wc(Ua),d=c[0],e=c[1];Xc(function(){var c=X.suspense;X.suspense=void 0===b?null:b;try{e(a)}finally{X.suspense=c}},[a,b]);return d},useTransition:function(a){var b=Wc(Ua),c=b[0];b=b[1];return[Yc(Ce.bind(null,b,a),[b,a]),c]}},ra=null,Ka=null,Wa=
!1,gj=da.ReactCurrentOwner,ia=!1,Je={dehydrated:null,retryTime:0};var jj=function(a,b,c,d){for(c=b.child;null!==c;){if(5===c.tag||6===c.tag)a.appendChild(c.stateNode);else if(4!==c.tag&&null!==c.child){c.child.return=c;c=c.child;continue}if(c===b)break;for(;null===c.sibling;){if(null===c.return||c.return===b)return;c=c.return}c.sibling.return=c.return;c=c.sibling}};var wh=function(a){};var ij=function(a,b,c,d,e){var f=a.memoizedProps;if(f!==d){var g=b.stateNode;Ta(ja.current);a=null;switch(c){case "input":f=
Cd(g,f);d=Cd(g,d);a=[];break;case "option":f=Fd(g,f);d=Fd(g,d);a=[];break;case "select":f=M({},f,{value:void 0});d=M({},d,{value:void 0});a=[];break;case "textarea":f=Gd(g,f);d=Gd(g,d);a=[];break;default:"function"!==typeof f.onClick&&"function"===typeof d.onClick&&(g.onclick=uc)}Ud(c,d);var h,m;c=null;for(h in f)if(!d.hasOwnProperty(h)&&f.hasOwnProperty(h)&&null!=f[h])if("style"===h)for(m in g=f[h],g)g.hasOwnProperty(m)&&(c||(c={}),c[m]="");else"dangerouslySetInnerHTML"!==h&&"children"!==h&&"suppressContentEditableWarning"!==
h&&"suppressHydrationWarning"!==h&&"autoFocus"!==h&&(db.hasOwnProperty(h)?a||(a=[]):(a=a||[]).push(h,null));for(h in d){var k=d[h];g=null!=f?f[h]:void 0;if(d.hasOwnProperty(h)&&k!==g&&(null!=k||null!=g))if("style"===h)if(g){for(m in g)!g.hasOwnProperty(m)||k&&k.hasOwnProperty(m)||(c||(c={}),c[m]="");for(m in k)k.hasOwnProperty(m)&&g[m]!==k[m]&&(c||(c={}),c[m]=k[m])}else c||(a||(a=[]),a.push(h,c)),c=k;else"dangerouslySetInnerHTML"===h?(k=k?k.__html:void 0,g=g?g.__html:void 0,null!=k&&g!==k&&(a=a||
[]).push(h,k)):"children"===h?g===k||"string"!==typeof k&&"number"!==typeof k||(a=a||[]).push(h,""+k):"suppressContentEditableWarning"!==h&&"suppressHydrationWarning"!==h&&(db.hasOwnProperty(h)?(null!=k&&oa(e,h),a||g===k||(a=[])):(a=a||[]).push(h,k))}c&&(a=a||[]).push("style",c);e=a;if(b.updateQueue=e)b.effectTag|=4}};var kj=function(a,b,c,d){c!==d&&(b.effectTag|=4)};var pj="function"===typeof WeakSet?WeakSet:Set,wj="function"===typeof WeakMap?WeakMap:Map,sj=Math.ceil,gd=da.ReactCurrentDispatcher,
Uh=da.ReactCurrentOwner,H=0,Ye=8,ca=16,ma=32,Xa=0,hd=1,Oh=2,ad=3,bd=4,Xe=5,p=H,U=null,t=null,P=0,F=Xa,id=null,ta=1073741823,Yb=1073741823,kd=null,Xb=0,jd=!1,Re=0,Ph=500,l=null,cd=!1,Se=null,La=null,ld=!1,Zb=null,$b=90,bb=null,ac=0,af=null,dd=0,Ja=function(a,b){if(50<ac)throw ac=0,af=null,Error(k(185));a=ed(a,b);if(null!==a){var c=Cc();1073741823===b?(p&Ye)!==H&&(p&(ca|ma))===H?Te(a):(V(a),p===H&&ha()):V(a);(p&4)===H||98!==c&&99!==c||(null===bb?bb=new Map([[a,b]]):(c=bb.get(a),(void 0===c||c>b)&&bb.set(a,
b)))}};var zj=function(a,b,c){var d=b.expirationTime;if(null!==a){var e=b.pendingProps;if(a.memoizedProps!==e||G.current)ia=!0;else{if(d<c){ia=!1;switch(b.tag){case 3:sh(b);Ee();break;case 5:bh(b);if(b.mode&4&&1!==c&&e.hidden)return b.expirationTime=b.childExpirationTime=1,null;break;case 1:N(b.type)&&Bc(b);break;case 4:se(b,b.stateNode.containerInfo);break;case 10:d=b.memoizedProps.value;e=b.type._context;y(Ic,e._currentValue);e._currentValue=d;break;case 13:if(null!==b.memoizedState){d=b.child.childExpirationTime;
if(0!==d&&d>=c)return th(a,b,c);y(D,D.current&1);b=sa(a,b,c);return null!==b?b.sibling:null}y(D,D.current&1);break;case 19:d=b.childExpirationTime>=c;if(0!==(a.effectTag&64)){if(d)return vh(a,b,c);b.effectTag|=64}e=b.memoizedState;null!==e&&(e.rendering=null,e.tail=null);y(D,D.current);if(!d)return null}return sa(a,b,c)}ia=!1}}else ia=!1;b.expirationTime=0;switch(b.tag){case 2:d=b.type;null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;e=pb(b,B.current);rb(b,c);e=we(null,
b,d,a,e,c);b.effectTag|=1;if("object"===typeof e&&null!==e&&"function"===typeof e.render&&void 0===e.$$typeof){b.tag=1;b.memoizedState=null;b.updateQueue=null;if(N(d)){var f=!0;Bc(b)}else f=!1;b.memoizedState=null!==e.state&&void 0!==e.state?e.state:null;ne(b);var g=d.getDerivedStateFromProps;"function"===typeof g&&Lc(b,d,g,a);e.updater=Mc;b.stateNode=e;e._reactInternalFiber=b;pe(b,d,a,c);b=Ie(null,b,d,!0,f,c)}else b.tag=0,T(null,b,e,c),b=b.child;return b;case 16:a:{e=b.elementType;null!==a&&(a.alternate=
null,b.alternate=null,b.effectTag|=2);a=b.pendingProps;ri(e);if(1!==e._status)throw e._result;e=e._result;b.type=e;f=b.tag=Gj(e);a=aa(e,a);switch(f){case 0:b=He(null,b,e,a,c);break a;case 1:b=rh(null,b,e,a,c);break a;case 11:b=nh(null,b,e,a,c);break a;case 14:b=oh(null,b,e,aa(e.type,a),d,c);break a}throw Error(k(306,e,""));}return b;case 0:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),He(a,b,d,e,c);case 1:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),rh(a,b,d,e,c);
case 3:sh(b);d=b.updateQueue;if(null===a||null===d)throw Error(k(282));d=b.pendingProps;e=b.memoizedState;e=null!==e?e.element:null;oe(a,b);Qb(b,d,null,c);d=b.memoizedState.element;if(d===e)Ee(),b=sa(a,b,c);else{if(e=b.stateNode.hydrate)Ka=kb(b.stateNode.containerInfo.firstChild),ra=b,e=Wa=!0;if(e)for(c=Fe(b,null,d,c),b.child=c;c;)c.effectTag=c.effectTag&-3|1024,c=c.sibling;else T(a,b,d,c),Ee();b=b.child}return b;case 5:return bh(b),null===a&&De(b),d=b.type,e=b.pendingProps,f=null!==a?a.memoizedProps:
null,g=e.children,Yd(d,e)?g=null:null!==f&&Yd(d,f)&&(b.effectTag|=16),qh(a,b),b.mode&4&&1!==c&&e.hidden?(b.expirationTime=b.childExpirationTime=1,b=null):(T(a,b,g,c),b=b.child),b;case 6:return null===a&&De(b),null;case 13:return th(a,b,c);case 4:return se(b,b.stateNode.containerInfo),d=b.pendingProps,null===a?b.child=wb(b,null,d,c):T(a,b,d,c),b.child;case 11:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),nh(a,b,d,e,c);case 7:return T(a,b,b.pendingProps,c),b.child;case 8:return T(a,
b,b.pendingProps.children,c),b.child;case 12:return T(a,b,b.pendingProps.children,c),b.child;case 10:a:{d=b.type._context;e=b.pendingProps;g=b.memoizedProps;f=e.value;var h=b.type._context;y(Ic,h._currentValue);h._currentValue=f;if(null!==g)if(h=g.value,f=Qa(h,f)?0:("function"===typeof d._calculateChangedBits?d._calculateChangedBits(h,f):1073741823)|0,0===f){if(g.children===e.children&&!G.current){b=sa(a,b,c);break a}}else for(h=b.child,null!==h&&(h.return=b);null!==h;){var m=h.dependencies;if(null!==
m){g=h.child;for(var l=m.firstContext;null!==l;){if(l.context===d&&0!==(l.observedBits&f)){1===h.tag&&(l=Ea(c,null),l.tag=Jc,Fa(h,l));h.expirationTime<c&&(h.expirationTime=c);l=h.alternate;null!==l&&l.expirationTime<c&&(l.expirationTime=c);Sg(h.return,c);m.expirationTime<c&&(m.expirationTime=c);break}l=l.next}}else g=10===h.tag?h.type===b.type?null:h.child:h.child;if(null!==g)g.return=h;else for(g=h;null!==g;){if(g===b){g=null;break}h=g.sibling;if(null!==h){h.return=g.return;g=h;break}g=g.return}h=
g}T(a,b,e.children,c);b=b.child}return b;case 9:return e=b.type,f=b.pendingProps,d=f.children,rb(b,c),e=W(e,f.unstable_observedBits),d=d(e),b.effectTag|=1,T(a,b,d,c),b.child;case 14:return e=b.type,f=aa(e,b.pendingProps),f=aa(e.type,f),oh(a,b,e,f,d,c);case 15:return ph(a,b,b.type,b.pendingProps,d,c);case 17:return d=b.type,e=b.pendingProps,e=b.elementType===d?e:aa(d,e),null!==a&&(a.alternate=null,b.alternate=null,b.effectTag|=2),b.tag=1,N(d)?(a=!0,Bc(b)):a=!1,rb(b,c),Yg(b,d,e),pe(b,d,e,c),Ie(null,
b,d,!0,a,c);case 19:return vh(a,b,c)}throw Error(k(156,b.tag));};var bf=null,Ne=null,la=function(a,b,c,d){return new Fj(a,b,c,d)};ef.prototype.render=function(a){md(a,this._internalRoot,null,null)};ef.prototype.unmount=function(){var a=this._internalRoot,b=a.containerInfo;md(null,a,null,function(){b[Lb]=null})};var Di=function(a){if(13===a.tag){var b=Fc(ka(),150,100);Ja(a,b);df(a,b)}};var Yf=function(a){13===a.tag&&(Ja(a,3),df(a,3))};var Bi=function(a){if(13===a.tag){var b=ka();b=Va(b,a,null);Ja(a,
b);df(a,b)}};sd=function(a,b,c){switch(b){case "input":Dd(a,c);b=c.name;if("radio"===c.type&&null!=b){for(c=a;c.parentNode;)c=c.parentNode;c=c.querySelectorAll("input[name="+JSON.stringify(""+b)+'][type="radio"]');for(b=0;b<c.length;b++){var d=c[b];if(d!==a&&d.form===a.form){var e=ae(d);if(!e)throw Error(k(90));Gf(d);Dd(d,e)}}}break;case "textarea":Lf(a,c);break;case "select":b=c.value,null!=b&&hb(a,!!c.multiple,b,!1)}};(function(a,b,c,d){ee=a;eg=b;vd=c;vf=d})(Qh,function(a,b,c,d,e){var f=p;p|=4;
try{return Da(98,a.bind(null,b,c,d,e))}finally{p=f,p===H&&ha()}},function(){(p&(1|ca|ma))===H&&(uj(),xb())},function(a,b){var c=p;p|=2;try{return a(b)}finally{p=c,p===H&&ha()}});var mk={Events:[Hb,Pa,ae,pf,qd,lb,function(a){Kd(a,Ki)},sf,tf,sc,pc,xb,{current:!1}]};(function(a){var b=a.findFiberByHostInstance;return Ej(M({},a,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:da.ReactCurrentDispatcher,findHostInstanceByFiber:function(a){a=Sf(a);
return null===a?null:a.stateNode},findFiberByHostInstance:function(a){return b?b(a):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))})({findFiberByHostInstance:Bb,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"});I.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=mk;I.createPortal=Xh;I.findDOMNode=function(a){if(null==a)return null;if(1===a.nodeType)return a;var b=a._reactInternalFiber;if(void 0===
b){if("function"===typeof a.render)throw Error(k(188));throw Error(k(268,Object.keys(a)));}a=Sf(b);a=null===a?null:a.stateNode;return a};I.flushSync=function(a,b){if((p&(ca|ma))!==H)throw Error(k(187));var c=p;p|=1;try{return Da(99,a.bind(null,b))}finally{p=c,ha()}};I.hydrate=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!0,c)};I.render=function(a,b,c){if(!bc(b))throw Error(k(200));return nd(null,a,b,!1,c)};I.unmountComponentAtNode=function(a){if(!bc(a))throw Error(k(40));return a._reactRootContainer?
(Rh(function(){nd(null,null,a,!1,function(){a._reactRootContainer=null;a[Lb]=null})}),!0):!1};I.unstable_batchedUpdates=Qh;I.unstable_createPortal=function(a,b){return Xh(a,b,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)};I.unstable_renderSubtreeIntoContainer=function(a,b,c,d){if(!bc(c))throw Error(k(200));if(null==a||void 0===a._reactInternalFiber)throw Error(k(38));return nd(a,b,c,!1,d)};I.version="16.13.1"});
</script>
    <script>const e = React.createElement;

function pathToString(path) {
  if (path[0] === '/') {
    return '/' + path.slice(1).join('/');
  } else {
    return path.join('/');
  }
}

function findCommonPath(files) {
  if (!files || !files.length) {
    return [];
  }

  function isPrefix(arr, prefix) {
    if (arr.length < prefix.length) {
      return false;
    }
    for (let i = prefix.length - 1; i >= 0; --i) {
      if (arr[i] !== prefix[i]) {
        return false;
      }
    }
    return true;
  }

  let commonPath = files[0].path.slice(0, -1);
  while (commonPath.length) {
    if (files.every(file => isPrefix(file.path, commonPath))) {
      break;
    }
    commonPath.pop();
  }
  return commonPath;
}

function findFolders(files) {
  if (!files || !files.length) {
    return [];
  }

  let folders = files.filter(file => file.path.length > 1).map(file => file.path[0]);
  folders = [...new Set(folders)]; // unique
  folders.sort();

  folders = folders.map(folder => {
    let filesInFolder = files
      .filter(file => file.path[0] === folder)
      .map(file => ({
        ...file,
        path: file.path.slice(1),
        parent: [...file.parent, file.path[0]],
      }));

    const children = findFolders(filesInFolder); // recursion

    return {
      is_folder: true,
      path: [folder],
      parent: files[0].parent,
      children,
      covered: children.reduce((sum, file) => sum + file.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.coverable, 0),
      prevRun: {
        covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
        coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
      },
    };
  });

  return [...folders, ...files.filter(file => file.path.length === 1)];
}

class App extends React.Component {
  constructor(...args) {
    super(...args);

    this.state = {
      current: [],
    };
  }

  componentDidMount() {
    this.updateStateFromLocation();
    window.addEventListener('hashchange', () => this.updateStateFromLocation(), false);
  }

  updateStateFromLocation() {
    if (window.location.hash.length > 1) {
      const current = window.location.hash.slice(1).split('/');
      this.setState({current});
    } else {
      this.setState({current: []});
    }
  }

  getCurrentPath() {
    let file = this.props.root;
    let path = [file];
    for (let p of this.state.current) {
      file = file.children.find(file => file.path[0] === p);
      if (!file) {
        return path;
      }
      path.push(file);
    }
    return path;
  }

  render() {
    const path = this.getCurrentPath();
    const file = path[path.length - 1];

    let w = null;
    if (file.is_folder) {
      w = e(FilesList, {
        folder: file,
        onSelectFile: this.selectFile.bind(this),
        onBack: path.length > 1 ? this.back.bind(this) : null,
      });
    } else {
      w = e(DisplayFile, {
        file,
        onBack: this.back.bind(this),
      });
    }

    return e('div', {className: 'app'}, w);
  }

  selectFile(file) {
    this.setState(
      ({current}) => {
        return {current: [...current, file.path[0]]};
      },
      () => this.updateHash(),
    );
  }

  back(file) {
    this.setState(
      ({current}) => {
        return {current: current.slice(0, current.length - 1)};
      },
      () => this.updateHash(),
    );
  }

  updateHash() {
    if (!this.state.current || !this.state.current.length) {
      window.location = '#';
    } else {
      window.location = '#' + this.state.current.join('/');
    }
  }
}

function FilesList({folder, onSelectFile, onBack}) {
  let files = folder.children;
  return e(
    'div',
    {className: 'display-folder'},
    e(FileHeader, {file: folder, onBack}),
    e(
      'table',
      {className: 'files-list'},
      e('thead', {className: 'files-list__head'}, e('tr', null, e('th', null, 'Path'), e('th', null, 'Coverage'))),
      e(
        'tbody',
        {className: 'files-list__body'},
        files.map(file => e(File, {file, onClick: onSelectFile})),
      ),
    ),
  );
}

function File({file, onClick}) {
  const coverage = file.coverable ? (file.covered / file.coverable) * 100 : -1;
  const coverageDelta =
    file.prevRun && (file.covered / file.coverable) * 100 - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'tr',
    {
      className:
        'files-list__file' +
        (coverage >= 0 && coverage < 50 ? ' files-list__file_low' : '') +
        (coverage >= 50 && coverage < 80 ? ' files-list__file_medium' : '') +
        (coverage >= 80 ? ' files-list__file_high' : '') +
        (file.is_folder ? ' files-list__file_folder' : ''),
      onClick: () => onClick(file),
    },
    e('td', null, e('a', null, pathToString(file.path))),
    e(
      'td',
      null,
      file.covered + ' / ' + file.coverable + (coverage >= 0 ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
    ),
  );
}

function DisplayFile({file, onBack}) {
  return e('div', {className: 'display-file'}, e(FileHeader, {file, onBack}), e(FileContent, {file}));
}

function FileHeader({file, onBack}) {
  const coverage = (file.covered / file.coverable) * 100;
  const coverageDelta = file.prevRun && coverage - (file.prevRun.covered / file.prevRun.coverable) * 100;

  return e(
    'div',
    {className: 'file-header'},
    onBack ? e('a', {className: 'file-header__back', onClick: onBack}, 'Back') : null,
    e('div', {className: 'file-header__name'}, pathToString([...file.parent, ...file.path])),
    e(
      'div',
      {className: 'file-header__stat'},
      'Covered: ' + file.covered + ' of ' + file.coverable + (file.coverable ? ' (' + coverage.toFixed(2) + '%)' : ''),
      e(
        'span',
        {title: 'Change from the previous run'},
        coverageDelta ? ` (${coverageDelta > 0 ? '+' : ''}${coverageDelta.toFixed(2)}%)` : '',
      ),
      e('input', {id: 'theme-toggle', type: 'checkbox', hidden: true}),
      e('label', {for: 'theme-toggle', id: 'theme-toggle-label'}, ''),
    ),
  );
}

function FileContent({file}) {
  return e(
    'pre',
    {className: 'file-content'},
    file.content.split(/\r?\n/).map((line, index) => {
      const trace = file.traces.find(trace => trace.line === index + 1);
      const covered = trace && trace.stats.Line;
      const uncovered = trace && !trace.stats.Line;
      return e(
        'code',
        {
          className: 'code-line' + (covered ? ' code-line_covered' : '') + (uncovered ? ' code-line_uncovered' : ''),
          title: trace ? JSON.stringify(trace.stats, null, 2) : null,
        },
        line,
      );
    }),
  );
}

(function () {
  const commonPath = findCommonPath(data.files);
  const prevFilesMap = new Map();

  previousData &&
    previousData.files.forEach(file => {
      const path = file.path.slice(commonPath.length).join('/');
      prevFilesMap.set(path, file);
    });

  const files = data.files.map(file => {
    const path = file.path.slice(commonPath.length);
    const {covered = 0, coverable = 0} = prevFilesMap.get(path.join('/')) || {};
    return {
      ...file,
      path,
      parent: commonPath,
      prevRun: {covered, coverable},
    };
  });

  const children = findFolders(files);

  const root = {
    is_folder: true,
    children,
    path: commonPath,
    parent: [],
    covered: children.reduce((sum, file) => sum + file.covered, 0),
    coverable: children.reduce((sum, file) => sum + file.coverable, 0),
    prevRun: {
      covered: children.reduce((sum, file) => sum + file.prevRun.covered, 0),
      coverable: children.reduce((sum, file) => sum + file.prevRun.coverable, 0),
    },
  };

  ReactDOM.render(e(App, {root, prevFilesMap}), document.getElementById('root'));

  const toggle = document.getElementById('theme-toggle');
  const label = document.getElementById('theme-toggle-label');
  label.textContent = '';

  toggle.addEventListener('change', () => {
    if (toggle.checked) {
      document.documentElement.setAttribute('data-theme', 'dark');
      label.textContent = '';
    } else {
      document.documentElement.removeAttribute('data-theme');
      label.textContent = '';
    }
  });
})();
</script>
</body>
</html>